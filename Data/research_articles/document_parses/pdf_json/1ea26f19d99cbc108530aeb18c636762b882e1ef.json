{"paper_id": "1ea26f19d99cbc108530aeb18c636762b882e1ef", "metadata": {"title": "A multi-component framework for the analysis and design of explainable artificial intelligence", "authors": [{"first": "S", "middle": [], "last": "Atakishiyev", "suffix": "", "affiliation": {}, "email": ""}, {"first": "H", "middle": [], "last": "Babiker", "suffix": "", "affiliation": {}, "email": ""}, {"first": "N", "middle": [], "last": "Farruque", "suffix": "", "affiliation": {}, "email": ""}, {"first": "R", "middle": [], "last": "Goebel", "suffix": "", "affiliation": {}, "email": ""}, {"first": "M-Y", "middle": [], "last": "Kim", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Alberta", "location": {"postCode": "T4V 2R3", "settlement": "Camrose", "region": "Alberta", "country": "Canada"}}, "email": ""}, {"first": "M", "middle": ["H"], "last": "Motallebi", "suffix": "", "affiliation": {}, "email": ""}, {"first": "J", "middle": [], "last": "Rabelo", "suffix": "", "affiliation": {}, "email": ""}, {"first": "T", "middle": [], "last": "Syed", "suffix": "", "affiliation": {}, "email": ""}, {"first": "O", "middle": ["R"], "last": "Za\u00efane", "suffix": "", "affiliation": {}, "email": ""}]}, "abstract": [{"text": "The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, which have created high expectations for industrial, commercial and social value.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "Second, the emergence of concern for creating trusted AI systems, including the creation of regulatory principles to ensure transparency and trust of AI systems.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "These two threads have created a kind of \"perfect storm\" of research activity, all eager to create and deliver any set of tools and techniques to address the XAI demand.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science, and which provides a basis for the development of a framework for transparent XAI. Here we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a history of XAI ideas, and synthesize those ideas into a simple framework to calibrate five successive levels of XAI.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Fueled by a growing need for trust and ethical artificial intelligence (AI) by design, the wake of the last decade of machine learning is crowded with a broad spectrum of research on explainable AI (XAI).", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "It is therefore not surprising that the rapid and eclectic flurry of activities in XAI have exposed confusion and controversy about foundational concepts like interpretability, explanation, and causality.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Perhaps confusion and disagreement is not surprising, given some of the complexity of modern learning methods. For example, when some deep learning methods can build predictive models based on more than 50 million distinct parameters, it is not a surprise that humans will debate what has been captured (e.g., see [1] 2 ). Note also the confusion regarding misconceptions on a specious trade off between predictive accuracy and explainability (cf. [2] ), which have precipitated scientific workshops to address such misconceptions 3 . Another example of yet-to-be-resolved issues includes the strange anomaly where syntactic reduction of the parameter space of some deep learning created models actually results in improved predictive accuracy (e.g., [3, 4] ). The reality is that the foundations for scientific understanding of general machine learning, and thus XAI, is not yet sufficiently developed.", "cite_spans": [{"start": 314, "end": 317, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 448, "end": 451, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 531, "end": 532, "text": "3", "ref_id": "BIBREF2"}, {"start": 751, "end": 754, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 755, "end": 757, "text": "4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Introduction"}, {"text": "Even though the long history of science and more recent history of scientific explanation and causality have considerable contributions to make (e.g., [5, 6] ), it seems like the demand created by potential industrial value has induced brittleness in identifying and confirming a robust trajectory from the history and Simon's Turing Award paper ( [7] ) identified as the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means for general intelligent action.\" (p. 116).", "cite_spans": [{"start": 151, "end": 154, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 155, "end": 157, "text": "6]", "ref_id": "BIBREF5"}, {"start": 348, "end": 351, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Introduction"}, {"text": "The challenge is to clarify connections between the recent vocabulary of XAI and their historical roots, in order to distinguish between scientifically valuable history and potentially new extensions. In what follows, we hope to articulate and connect a broader historical fabric of concepts essential to XAI, including interpretation, explanation, causality, evaluation, system debugging, expressivity, semantics, inference, abstraction, and prediction.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "To do so, we need to articulate a general, if incomplete XAI framework, which is immediately challenged to be comprehensive enough to acknowledge a broad spectrum of components, yet but still avoid becoming a na\u00efve long and unstructured survey of overlapping -sometimes competing -concepts and methods. Our approach is to start with a simple structural idea first illustrated Figure 1 .", "cite_spans": [], "ref_spans": [{"start": 376, "end": 384, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "One can conceptualize the content of Figure 1 by comparing an accepted framework around the articulation levels of autonomous driving ( [8] ). The simple distinction of degree of human control creates the basis for discussion about how one could achieve Level 0 (no automation) to Level 5 (full automation), then discuss the details of those levels and the components necessary to achieve them.", "cite_spans": [{"start": 136, "end": 139, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [{"start": 37, "end": 45, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "In our case for explainability, we intend the x-axis of Figure 1 to distinguish what we consider a kind of quality of explanatory system, with the intuition that explanations at a higher level can be confirmed as \"better\" by some evaluation measure, e.g., we expect a causal explanation to provide the basis for recreating a causal inference chain to a prediction. We would distinguish, for example \"explanation by authority,\" towards the left end of the scale, to be something like an explanation to the question \"why can't I go out tonight?\" to be something like \"Because I said so,\" from a parent to a teenager, which we might just say is explanation by authority. Just these simple distinctions help frame and motivate a more detailed analysis of distinctions across our informal scale of levels of explanation, which will be further articulated in what follows.", "cite_spans": [], "ref_spans": [{"start": 56, "end": 64, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "Note that an alternative to Figure 1 , might be a simple abstract comparison of levels of explanation as a check list of possible components, like in Table 1 .", "cite_spans": [], "ref_spans": [{"start": 28, "end": 36, "text": "Figure 1", "ref_id": "FIGREF0"}, {"start": 150, "end": 157, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Introduction"}, {"text": "Please note that both representations are intended only to begin to consider how such components may obtain in any particular XAI system. For example, our figure and table do not intend the reader to draw the inference that an XAI system is somehow better with a dialogue component than without, nor that any anticipated evaluation of performance is higher with an integration of more of the components. The idea is only to suggest that there will emerge a foundation of what XAI components are essential, orthogonal, and have distinct and value-contributing roles in future XAI systems.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Similarly, our desire for some kind of sensible measures to clarify a level n explanation from a level n + 1 explanation creates our speculation on a kind of y-axis, which is not intended to imply the existence of any measure. Rather our y-axis is a kind of independent set of plausible orthogonal explanatory system attributes, which should be distinguished clearly enough to be able to use each attribute as a check list of attributes that any explanatory system may or may not have (cf. Table 1 ).", "cite_spans": [], "ref_spans": [{"start": 490, "end": 497, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Introduction"}, {"text": "For example, most surveys of XAI note the requirement for a system to produce alternative explanations; simply put, producing a single explanation may be completely insufficient for multiple explainees [9] . Similarly, many have noted the value of interactive XAI systems and dialogue systems [10] , which provide a basis for an explainee to submit and receive responses to questions about a model prediction, and thus build deeper trust of the system.", "cite_spans": [{"start": 202, "end": 205, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 293, "end": 297, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Introduction"}, {"text": "In what follows, we will provide increasing detail and precision about how we believe existing XAI concepts align with this simple framework, in order to consider how to articulate choices in the design of an XAI system.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The rest of our paper is organized as follows. Section 2 presents what we consider the principal components of XAI, including that explanations need to be explainee-specific, that there can always be multiple explanations, and that the evaluation of the quality of explanation has more to do with the explainee than the explanation system. This will help provide sufficient detail to articulate the relationship between current explanatory concepts and their relationship to historical roots, e.g., to consider the emerging demands on the properties of a formal definition of interpretability by assessing the classical formal systems view of interpretability. Section 3 considers the more general history of explanation, as an attempt to connect the formal philosophy and scientific explanation foundations. This concludes with the articulation of the explanatory 5 role of recent theories of causal representation. Section 4 summarizes important emerging trends and components in proposed XAI architectures, including those that apply to both machine-learned predictive models and general AI systems. Section 5 provides a synopsis of current XAI research threads, and how they might integrate into an emerging XAI architecture. The opinions include the description of important XAI ideas like pre-hoc versus post-hoc explanation creation, and the evaluation of explanations, in order to sketch an architecture of how necessary components for XAI are connected. Finally Section 6 provides a brief summary, and what we believe the future architectures of XAI systems will require to ensure the trust of future AI systems.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "There is a confusion in the literature regarding the definitions of interpretability and explainability of models. Many recent papers use those terms interchangeably [11] , [12] . Some papers do make a distinction between those terms, but we do not agree with those definitions as well. For example, Gilpin et al. [13] define interpretability as a rudimentary form of explainability. Rudin [14] finds that there is no single definition on interpretability. However, the author defines a spectrum which extends from fully interpretable models such as rule-based models (that provide explanations by definition) to deep models that cannot provide explanations out of the box.", "cite_spans": [{"start": 166, "end": 170, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 173, "end": 177, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 314, "end": 318, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 390, "end": 394, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Explainability and Interpretability"}, {"text": "We note that there is no confusion about interpretation, explainability and semantics in the case of the history of mathematical logic (e.g., [15] information to produce explanations. Explainability pertains to the mechanism of justification provided for an inference or prediction using a learned model, whether the used model is clearly interpretable or loosely interpretable. Figure 2 illustrates the distinction between interpretability, which concerns the rendition or comprehension of a predictive model learned from data during training, and explainability, which pertains to the elucidation and justification of a prediction or decision made in the presence of a new observation or case. Both may revert to and rely on the original training data for analogy or grounds for justification. Based on the above definitions, models such as decision trees and rule-based systems that are considered transparent (i.e., they are generally considered toward the fully-interpretable end of the transparency spectrum) are also explainable, while deep models are not. For example, once we add an explanation module to the deep neural model (e.g., [16] ), they become explainable systems as well but, interpretation of their meaning can be either in terms of how models are learned or what predictions mean (i.e., their interpretation in their application domain).", "cite_spans": [{"start": 142, "end": 146, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1143, "end": 1147, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [{"start": 379, "end": 387, "text": "Figure 2", "ref_id": "FIGREF2"}], "section": "Explainability and Interpretability"}, {"text": "Notice that these distinctions between explainability and interpretability do not comply with a reasonable assumption that is true in a common sense usage of the terms: outside of the AI arena, it is reasonable to expect explainability requires interpretability; one can only explain something they fully understand and can interpret. That stems from our definition of explainability: in AI, an explanation carries a completely different meaning from the one of its usual usage. Even a saliency map which highlights areas of an image is considered to be (to some extent) an explanation of an image classification system. That \"explanation\" does not consider image semantics, it is just an objective identification of which pixels contribute more to the final activations in a neural network. Still, they can be especially useful as debugging tools for machine learning practitioners (see Subsection 2.3).", "cite_spans": [], "ref_spans": [], "section": "Explainability and Interpretability"}, {"text": "According to [17] , a person's background knowledge, often called prior knowledge, is a collection of \"abstracted residue\" that has been formed from all of life's experiences, and is brought by everyone, at any age, to every subsequent life experience, as well as used to connect new information to old. ", "cite_spans": [{"start": 13, "end": 17, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Alternative explanations: who are explanations for?"}, {"text": "As mentioned above, some approaches to explainability provide information related to how the model works internally. However, not all information provided by those approaches can really be considered domain explanatory information. Of course, the information provided by, e.g., rule-based systems can be understood as a detailed explanation on how the system operates and could be applicable in scenarios where an end user needs to understand how the system generated a prediction. However, other approaches (especially those applicable to the so called opaque or black-box systems) are way less informative, and can be considered superficial \"hints\" rather than actual explanations, e.g., saliency maps on convolutional neural networks. This is really an observation about understanding internal components so as to debug those mechanisms, not as explanation. Although explanation-wise constrained, these approaches are still useful on helping to understand how a model behaves, especially if the consumer of the information has the necessary background. Hence, they may be considered debugging techniques for opaque AI systems rather than production of explanations based on a user's understanding of the semantics of an application domain.", "cite_spans": [], "ref_spans": [], "section": "Debugging versus explanation"}, {"text": "One example of a debugging tool to augment a model is the work of [19] , in which the authors used a ResNet [20] to recognize objects in a scene. Applying ", "cite_spans": [{"start": 66, "end": 70, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 108, "end": 112, "text": "[20]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Debugging versus explanation"}, {"text": "Deep learning-based systems became prevalent in AI especially after its successful applications in image classification problems. Deep learning-based systems achieve impressive accuracy rates on standard datasets (e.g., ImageNet [22] ) without requiring much effort on designing and implementing handcrafted rules or feature extractors. In fact, by leveraging transfer learning techniques and well known architectures based on convolutional neural networks, a deep learning practitioner can quickly build an image classifier outperforming image classification methods which were state of the art before the \"deep learning revolution.\"", "cite_spans": [{"start": 229, "end": 233, "text": "[22]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Is there a trade off between explanatory models and classification accuracy?"}, {"text": "Nevertheless, despite their excellent overall accuracy, deep learning systems are considered black-boxes unable to provide explanations as to why they make a given prediction. In some applications, that limitation does not translate into a serious practical problem: a mobile phone picture classification application which misclassifies two animals will not bring consequences to a user other than a few giggles and a funny discussion topic at friends gatherings. If those errors are seldom, nobody would really care or lose confidence on the application.", "cite_spans": [], "ref_spans": [], "section": "Is there a trade off between explanatory models and classification accuracy?"}, {"text": "Errors may come up in random images and could be induced. Figure 3 shows an example of a panda picture being classified as a gibbon after some adversarial noise is added [23] .", "cite_spans": [{"start": 170, "end": 174, "text": "[23]", "ref_id": "BIBREF23"}], "ref_spans": [{"start": 58, "end": 66, "text": "Figure 3", "ref_id": "FIGREF4"}], "section": "Is there a trade off between explanatory models and classification accuracy?"}, {"text": "The above example illustrates it is possible to intentionally fool a classifier through addition of appropriate noise. Depending on the image classification application, that kind of error may produce more serious consequences than the hypothetical phone application mentioned above. For example, recently hackers were able to fool Tesla's autopilot by tampering speed limit signs with adhesive tape (see Figure 4 ), making the car to accelerate to 85 mph. This is a simple example which illustrates predictions from AI models cannot be blindly accepted in many practical applications. Moreover, techniques unable to explain how they arrive at a prediction make them even more sensitive to random errors or deliberate attacks. That observation raises an important question around a potential trade off between model accuracy and explanatory capabilities: it is true that a deep learning-based model can achieve accuracy in many practical applications. That allows practitioners to quickly build accurate models with not so much effort. However, some preconditions do exist, the main one being the availability of potentially large labelled datasets (a problem potentially alleviated by transfer learning, but still common in machine learning in general and in deep learning techniques in particular). In some cases, training large state of the art deep learning networks requires thousands of even millions of dollars (the estimated cost of training just one of models developed in [25] was estimated in US$1.4 million [26] ). All considered, it is not appropriate to claim there is necessarily a trade off between accuracy and explainability (or more generally, model performance). In some cases, deep learning methods will not be able to provide state of the art results (e.g., when there is not enough labelled data, when the model is so large it will be impractical to deploy on the target platforms, or even train due to prohibitive costs, etc.) so more explanation capable techniques might even provide better results. But as previously noted, there is no reason in principle that induced models like decision trees should in principle be less accurate than deep learned models.", "cite_spans": [{"start": 1482, "end": 1486, "text": "[25]", "ref_id": "BIBREF25"}, {"start": 1519, "end": 1523, "text": "[26]", "ref_id": "BIBREF26"}], "ref_spans": [{"start": 405, "end": 413, "text": "Figure 4", "ref_id": "FIGREF5"}], "section": "Is there a trade off between explanatory models and classification accuracy?"}, {"text": "Whereas a factually wrong explanation is obviously inappropriate, determining if an explanation is good transcends its correctness. The quality of an explanation is a little like beauty; it is in the eye of the beholder. It is very clear (and quite common) that two factually correct, but different explanations could be considered good or bad depending on to whom they were provided. This means that, to assess quality of explanations, one (again) needs to consider the explainee, the person who receives the explanation (see Subsection 2.2). The explainee's background, expectations, goals, context, etc., will play a determinant role in the evaluation process.", "cite_spans": [], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "From the above paragraph, it is clear that assessing the quality of explanations is subjective, and a quite complicated task, even if done manually. Thus, coming up with an effective technique to evaluate explanation capabilities is beyond the reach of currently available methods. In fact, automatic evaluation of any generative model is a difficult task. Metrics commonly used for translation systems such as BLEU [27] or for automatic summarization such as ROUGE 13 [28] are not appropriate for more sophisticated tasks such as explainability or even dialogue systems, since they assume that valid responses have significant word overlap with the ground truth responses [29] .", "cite_spans": [{"start": 416, "end": 420, "text": "[27]", "ref_id": "BIBREF27"}, {"start": 469, "end": 473, "text": "[28]", "ref_id": "BIBREF28"}, {"start": 673, "end": 677, "text": "[29]", "ref_id": "BIBREF29"}], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "For that reason, most evaluation methods for explainability systems require human intervention. For example, the organizers of a fake news detection competition 4 which requires an explanation of why a given statement is considered fake news or not, split the competition in two phases and limited the explanations assessment to the second phase to which only 10 teams would be qualified, thus making it manually tractable.", "cite_spans": [], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "The history of evaluation in the field of data visualization is also relevant to the question of how to evaluate explanations. The initial focus on alternative visual renderings of data have, over a decade, transformed from whether a visualization was \"interesting\" to consideration for what human inferences are enabled by alternative visualization techniques (e.g., [30] ).", "cite_spans": [{"start": 368, "end": 372, "text": "[30]", "ref_id": "BIBREF30"}], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "The simplest conceptual alignment is that a visualization is a visual explanation. The compression of a large volume of data to a visualization picture is lossy and inductive, so the choice of how to create that lossy inductive picture or explanation is about what inferences to imply for the human visual system.", "cite_spans": [], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "The evaluation of alternative visualizations has evolved to a framework where evaluation is about what inferences are easily observed (e.g., [31] ). Furthermore, interactive visual explanation is easily considered as our suggestion of explanation being interactive and driven by the semantics of the application domain (e.g., [32] ).", "cite_spans": [{"start": 141, "end": 145, "text": "[31]", "ref_id": "BIBREF31"}, {"start": 326, "end": 330, "text": "[32]", "ref_id": "BIBREF32"}], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "Evaluation of what we can consider as a more general explanatory framework, which produces alternative explanations in terms of text, rules, pictures, and various media, can similarly be aligned with the evolution of how to evaluate visual explanations.", "cite_spans": [], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "But of course there is yet no clear explanation evaluation framework, but only a broad scope of important components (e.g., [9] ). Even specific instances In fact, that thread of exploiting abduction in Artificial Intelligence is aligned with perspectives from other disciplines. For example, Eriksson and Lindstr\u00f6m describe abductive reasoning as an initial step of inquiry to develop hypotheses where the corresponding outcomes are explained logically through deductive reasoning and experimentally through inductive reasoning [37] . Their application to \"care science\" is just another example that confirms the generality of abductive reasoning.", "cite_spans": [{"start": 124, "end": 127, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 529, "end": 533, "text": "[37]", "ref_id": "BIBREF37"}], "ref_spans": [], "section": "Assessing the quality of explanations"}, {"text": "The block diagram of Figure 5 , partially inspired by a figure in [38] , is intended only to confirm the connection between abductive, deductive, and in-ductive reasoning. We see that abductive reasoning entails justification of ideas that support the articulation of new knowledge by integrating deductive and inductive reasoning. In Artificial Intelligence studies, the process involving these reasoning steps are as follows: 1) identify observations that require explanation as they cannot be confirmed with already accepted hypotheses; 2) identify a new covering hypothesis using abductive reasoning; 3) empirical consequences of the hypothesis, including consistency with already known knowledge, is established through deduction; 4) after an accepted level of verification, the hypothesis is accepted as the most scientifically plausible. ", "cite_spans": [{"start": 66, "end": 70, "text": "[38]", "ref_id": "BIBREF38"}], "ref_spans": [{"start": 21, "end": 29, "text": "Figure 5", "ref_id": "FIGREF7"}], "section": "Assessing the quality of explanations"}, {"text": "The connection between Artificial Intelligence frameworks for abductive explanation have suggested a direct connection between \"scientific explanation,\"", "cite_spans": [], "ref_spans": [], "section": "Scientific explanation"}, {"text": "and is the subject of many debatable issues in the community of science and philosophy [5] . Some of the discussions imply that there is only one form of explanation that may be considered scientific. There are also some proponents of the idea that a theory of explanation should include both scientific and other simpler forms of explanation. Consequently, it has been a common goal to formulate principles that can confirm an explanation a scientific explanation. As far back in history as Aristotle, generally is considered to be the first philosopher to articulate an opinion that knowledge becomes scientific when it tries to explain the causes of \"why\". His view urges that science should not only keep facts, but also describe them in an appropriate explanatory framework [39] .", "cite_spans": [{"start": 87, "end": 90, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 779, "end": 783, "text": "[39]", "ref_id": "BIBREF39"}], "ref_spans": [], "section": "Scientific explanation"}, {"text": "In addition to this theoretical view, empiricists also maintain a belief that the components of ideas should be acquired from perceptions with which humans become familiar through sensory experience. The development of the principles of scientific explanation from this perspective prospered with the so-called", "cite_spans": [], "ref_spans": [], "section": "Scientific explanation"}, {"text": "Deductive-Nomological (DN) model that was described by Hempel in [40], [41] , [42] , and by Hempel and Oppenheim in [43] .", "cite_spans": [{"start": 71, "end": 75, "text": "[41]", "ref_id": "BIBREF41"}, {"start": 78, "end": 82, "text": "[42]", "ref_id": "BIBREF42"}, {"start": 116, "end": 120, "text": "[43]", "ref_id": "BIBREF43"}], "ref_spans": [], "section": "Scientific explanation"}, {"text": "The DN model is based on the idea that two main elements form a scientific explanation: An explanandum, a sentence that outlines a phenomenon to be explained, and an explanan, a sentence that is specified as explanations of that phenomenon. For instance, one might constitute an explanandum by asking \"Why did the dish washer stop working?\" and another person may provide an explanan by answering \"Because the electricity went off.\" We may infer that the explanan is rationally or even causally connected to the explanandum, or at least that the explanandum is the reasonable consequence of the explanans, otherwise speaking [44] . In this way, the explanation delivered as an explanans becomes a form of deductive argument and constitutes the \"deductive\" part of the model. Note that a series of statements comprises an explanan should comply with \"laws of nature.\" This is a vital property, because derivation of the explanandum from the explanan loses its validity if this property is violated [5] .", "cite_spans": [{"start": 625, "end": 629, "text": "[44]", "ref_id": "BIBREF44"}, {"start": 997, "end": 1000, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Scientific explanation"}, {"text": "This is the nomological component of the model, where the term \"nomological\" means \"lawful.\" Hempel and Oppenheims DN model formulation states that a scientific explanation is an answer to a so-called \"why\" question, and there may be multiple such answers. There may also be several types of questions (e.g., \"How does an airplane fly?\") that cannot be converted into why-questions.", "cite_spans": [], "ref_spans": [], "section": "Scientific explanation"}, {"text": "Consequently, answers to such questions are not considered to be scientific explanations. This does not mean that such answers are not part of a scientific discipline; these answers just become descriptive rather than being explanatory [45] , and is related to our next section on causality.", "cite_spans": [{"start": 236, "end": 240, "text": "[45]", "ref_id": "BIBREF45"}], "ref_spans": [], "section": "Scientific explanation"}, {"text": "Another aspect of the DN model is that the elements of an explanation are statements or sentences describing phenomenon, not the phenomenon itself.", "cite_spans": [], "ref_spans": [], "section": "Scientific explanation"}, {"text": "Finally, the sentences in the explanans must be accurate and verified, urging the arguments of the scientific explanation to be valid and sound. Thus, the DN model can be summarized as a model of a scientific explanation outlining a conception of explanation and a connection in the flow of an explanation.", "cite_spans": [], "ref_spans": [], "section": "Scientific explanation"}, {"text": "As hinted in the summary of scientific explanation, perhaps the most strict form of explanation is causal explanation. Informally, a casual explanation is one that arises from the construction of causal models, which require that explanations for arising predictions are in face \"recipes\" for reconstruction that prediction.", "cite_spans": [], "ref_spans": [], "section": "Causality"}, {"text": "Causal models typically facilitate the creation of explanation for a phenomena or an answer to a query by constructing a formal expression. That formal expression is derived from some causal representation, which typically captures directed causal relationships in a graphical model of cause and effect (or causality). This representation encodes an incomplete set of assumptions built upon prior knowledge. The causal explanation expression is, as with abduction and scientific explanation, revised continuously until a suitable explanation is obtained, and can answer the particular query.", "cite_spans": [], "ref_spans": [], "section": "Causality"}, {"text": "The most relevant and recent framework of causal representation and reasoning is given by the culmination of Pearl's research in [6] . In that work, an abductive explanation is called an \"estimand.\" This idea of a formal expression that best explains a particular phenomena (or a query) has its root in formal philosophy, as noted about, and especially in abductive reasoning. As noted above, abductive reasoning is given a set of incomplete observations (or assumptions as described above) and seeks to construct an explanation which best describes it (or the estimand).", "cite_spans": [{"start": 129, "end": 132, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Causality"}, {"text": "An important point here is that the overall information architectures of abduction, scientific reasoning, and causal reasoning are similar, but their mechanism and the evaluation of an explanation are successfully refined.", "cite_spans": [], "ref_spans": [], "section": "Causality"}, {"text": "A lingering unaddressed distinction is about the content or meaning of an explanation, especially in the context of what counts as an explanation to a user. Again a principled distinction exists in the realm of mathematical logic (cf. [15] ; any logic textbook will suffice). In the context of predictions from domain models (whether learned or fabricated by hand), a prediction has at least two kinds of explanation. For example, consider the simple familiar syllogism", "cite_spans": [{"start": 235, "end": 239, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "\u2022 All men are mortal.", "cite_spans": [], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "\u2022 Socrates is a man.", "cite_spans": [], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "\u2022 Socrates is mortal.", "cite_spans": [], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "Consider \"Socrates is mortal\" as a prediction of the very simple model. From the perspective of formal logic, there are (at least) two explanations. One is the explanation of the deductive mechanism that produced \"Socrates is moral\" from the first two expressions. This a so-called proof-theoretic explanation as it amounts to a description of how two premises are combined by deductive inference to derive the prediction. In an analogy with programming language debuggers, this kind of explanation is about the mechanism that produced the prediction, and is akin to how current work in explaining image classification (e.g., [16] ). This kind of explanation is appropriate when the explainee has interest in understanding and debugging the mechanism.", "cite_spans": [{"start": 626, "end": 630, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "But note an alternative explanation is not about mechanism but about the meaning of the expressions. Logically, the proof theory or deductive chain explanation is about mechanism. But the semantic explanation is about what it means to be mortal and what it means to be a man. That kind of explanation is semantic, and is intended to be appropriate for an explainee who is not interested in mechanism but in meaning. If a prediction was \"Socrates is a duck\" obtained from the same system, it can immediately be viewed with suspicion because of its meaning, not because of the mechanism that produced it from a presumably faulty model. So distinguishing syntax from semantics or meaning has more to do with the internal rules that a system has to follow to compute something. We all know that symbolic debuggers for programming languages create labels and traces which become the basis for producing mechanism explanations. The computation rules themselves might not be sufficient to provide a clear picture on nation for a ResNet model [19] described in Section 2 is one good example of the use of semantics to explain an opaque system.", "cite_spans": [{"start": 1035, "end": 1039, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Explaining mechanism/syntax versus semantics"}, {"text": "In the last five years, there has been a surge in the papers attempting to introduce new explanation methods. This intensity of work in XAI is, in fact, a side effect of widespread use of AI in sensitive domains such as legal reasoning and the medical field. In this section, we review some of the various explanation approaches popular in the literature, and classify in our framework based on how the explanations are built, and compare that with the levels of explanation 21 introduced in Section 1.", "cite_spans": [], "ref_spans": [], "section": "Classification of Current Research Trends"}, {"text": "Some have focused on creating models that try to build explanations concurrently together with the main task (e.g., learning a classifier). As an example, consider the work of [47] who seek to identify segments of text that support an explanation of text review classification. Their approach proposes a neural architecture that is made up of a generator followed by an encoder component.", "cite_spans": [{"start": 176, "end": 180, "text": "[47]", "ref_id": "BIBREF47"}], "ref_spans": [], "section": "Concurrently constructed explanations"}, {"text": "The generator extracts portions of the input text as salient words, then forwards them to the encoder to predict the target class. The final output of the system comprises the class label, together with the extracted \"justification\" from the input text. Other similar work, applied beyond text to images and text, has relied on learning attention weights from the input. In the related work, some authors referred to this category as learning interpretable representations.", "cite_spans": [], "ref_spans": [], "section": "Concurrently constructed explanations"}, {"text": "In natural language processing (NLP) text classifications for instance, attention layers attempt to learn the weight of each latent representation produced by the recurrent layer. The attention weights are then used to explain the prediction made by the classifier [48] . There is a debate in the literature on whether attention weights could be used as an explanation or not [49, 50] . An interesting connection is to our discussion above regarding the difference between debugging explanations and semantic explanations; much of this research is motivated to equate a mechanism behaviour to semantic interpretability.", "cite_spans": [{"start": 265, "end": 269, "text": "[48]", "ref_id": "BIBREF48"}, {"start": 376, "end": 380, "text": "[49,", "ref_id": "BIBREF49"}, {"start": 381, "end": 384, "text": "50]", "ref_id": "BIBREF50"}], "ref_spans": [], "section": "Concurrently constructed explanations"}, {"text": "Another approach is to use a post-hoc technique. The basic idea is to approximate explanations from a trained model. As mentioned earlier, concurrently constructed explanations need to be computed within the model, which means they need to have access to the internals of the model, or what many refer to as model-dependent (this further creates confusion about whether a model is syntactic or semantic). However, some post-hoc approaches can create approximate explanations without having access to the internals of the model, thus could be classified either as model-dependent or model-independent 5 . In the next subsection, we will briefly discuss the difference between model-dependent and model-independent.", "cite_spans": [], "ref_spans": [], "section": "Post-hoc explanations"}, {"text": "To describe model-dependent explanations, consider the case of non-linear deep networks. One can use a back-propagation algorithm to learn feature importance (e.g., which pixels contributed most in classifying the image as a cat rather than a dog) then use that learned feature ranking as the basis for explaining predictions. The simplest general approach is to compute a gradient with respect to the predicted class and use the back-propagation to propagate the gradient to the input. Finally, one can combine the input with the gradient to capture the salient pixels which can be used to explain the predicted class (e.g., Grad-CAM [51] ).", "cite_spans": [{"start": 635, "end": 639, "text": "[51]", "ref_id": "BIBREF51"}], "ref_spans": [], "section": "Model-dependent explanations"}, {"text": "The goal of this group of methods is to focus more on explaining individual instances without the target model being exposed. In fact, the target model is This labelled dataset is then used to frame a near enough justification. While LIME is the most cited model-independent method, there are other approaches which can be classified as model-independent [53, 54] .", "cite_spans": [{"start": 355, "end": 359, "text": "[53,", "ref_id": "BIBREF53"}, {"start": 360, "end": 363, "text": "54]", "ref_id": "BIBREF54"}], "ref_spans": [], "section": "Model-independent explanations"}, {"text": "Another way to classify explanation methods is to consider how an explanation mechanism is related to the application domain of the task. An applicationdependent method implicitly assumes the explainee is knowledgeable about the 5 Sometimes, this is called model-agnostic post-hoc explanations.", "cite_spans": [], "ref_spans": [], "section": "Application-dependent vs. generic explanations"}, {"text": "application and thus it employs the domain's vocabulary. In a medical application, for instance, a system can explain the prediction using medical terms. A generic explanation, on the other hand, can only provide explanations based on the mechanism of model building, combined with information available in the training set (e.g., correlation between features). Note that a model-dependent method is not necessarily taking into account the knowledge of the explainee (i.e., it will provide the same explanation irrespective of the customers' knowledge), but it must take advantage of the application's vocabulary (see Subsec-", "cite_spans": [], "ref_spans": [], "section": "Application-dependent vs. generic explanations"}, {"text": "It is also noteworthy that the system needs to go beyond correlative features -which is how most current machine learning methods work -to be capable of providing such application-dependent explanations. Many explainees (e.g., physicians, lawyers) would prefer having application-dependent explana-", "cite_spans": [], "ref_spans": [], "section": "Application-dependent vs. generic explanations"}, {"text": "tions. This will not be achieved without moving the machine learning research on explanation toward scientific and casual explanation.", "cite_spans": [], "ref_spans": [], "section": "Application-dependent vs. generic explanations"}, {"text": "As described briefly in Section 1, different levels of explanation could be introduced as shown in Figure 1 . Here we want to further elaborate those abstract levels and classify the related work accordingly. Table 2 To the best of our knowledge there is as yet no existing work on Levels 3 and 4.", "cite_spans": [], "ref_spans": [{"start": 99, "end": 107, "text": "Figure 1", "ref_id": "FIGREF0"}, {"start": 209, "end": 216, "text": "Table 2", "ref_id": "TABREF4"}], "section": "Classification based on levels of explanation"}, {"text": "However, there are some conceptual approaches that aim to achieve such levels [55] . In the subsections below, we provide details.", "cite_spans": [{"start": 78, "end": 82, "text": "[55]", "ref_id": "BIBREF55"}], "ref_spans": [], "section": "Classification based on levels of explanation"}, {"text": "Models classified as Level 0, provide no explanation at all. They are, in essence, black-box models that cannot provide any explanatory information to a user. In other words, the explainee is expected to accept or reject a system's decision without any further information. Most off-the-shelf methods for learning classifiers (e.g., deep learned models, support vector machines, or random forests) belong to this level.", "cite_spans": [], "ref_spans": [], "section": "Level 0"}, {"text": "The explainee is provided with a single type of explanation in models falling into this category. For example, a framework that provides heat-maps to explain image classification belongs to this level. Most of this approach focuses on providing a post-hoc explanation, which transitions a black-box model -that originally belonged to Level 0-to a Level 1 model. Recently, however, a few methods have been proposed to look at building concurrently constructed explanation algorithms [47, 57] to make models that by definition belong to Level 1.", "cite_spans": [{"start": 482, "end": 486, "text": "[47,", "ref_id": "BIBREF47"}, {"start": 487, "end": 490, "text": "57]", "ref_id": "BIBREF57"}], "ref_spans": [], "section": "Level 1"}, {"text": "Level 2 adds another type of explanation to enrich the knowledge communicated with the explainees. At this level, the system not only provides a heat-map to explain a classified animal image as a cat, but it also contains another type of explanation such as a textual explanation as an alternative description of the predicted classification. In this way, the alternative explanations allows the user to grasp more insights about the reasoning process employed by the system to make the prediction. If one explanation is not well understood by the explainee, then they have the opportunity to understand from an alternative explanation.", "cite_spans": [], "ref_spans": [], "section": "Level 2"}, {"text": "Note that in the case of the abductive systems described above, there can be a large number of alternative explanations.", "cite_spans": [], "ref_spans": [], "section": "Level 2"}, {"text": "An explainee and their familiarity with the domain plays a vital role in this level. The explanatory system includes some model of the explainee's domain model, and is capable of deciding the right type of explanation according to the knowledge of the explainee. For instance, a patient is diagnosed with some disease and an AI system is used to provide a potential treatment therapy.", "cite_spans": [], "ref_spans": [], "section": "Level 3"}, {"text": "While the therapist requires a detailed medical explanation by the AI system, the patient would strongly prefer to have a lay person's explanation for any alternative treatment recommendations.", "cite_spans": [], "ref_spans": [], "section": "Level 3"}, {"text": "In the current context of the COVID-19 pandemic as an example, Hydroxychloroquine is alleged to be a potential cure and has attracted many ordinary people's attention around the globe. People are interested to understand why this drug is a potential treatment. As a result, many medical researchers provide interviews to the media explaining how this drug works, typically with very shallow detail. As we can expect, however, the same experts would use a different level of granularity to explain the drug to other experts. Please note that, none of the existing explanation methods take into account the knowledge of the explainee.", "cite_spans": [], "ref_spans": [], "section": "Level 3"}, {"text": "While previous levels (e.g., Level 0, 1, and 2) do not include the capability of interaction between the explainee and the system except perhaps for at most one interaction (Level 3), methods classified as Level 4 can interact with the user.", "cite_spans": [], "ref_spans": [], "section": "Level 4"}, {"text": "They are expected to support a conversation sort of capability which allows the explainee to refine their questions and concerns regarding the decision. In other words, each interaction in the conversation allows the explainee to get clarifications. Here, the system is capable of adapting its explanation to the vocabulary of the explainee. Take the Richard Feynman's interview [18] with the BBC as an example. He could provide the reporter with what he thought the reporter would understand most. Once the reporter understood that explanation, if he had further questions, or wanted more in-depth explanation, the reporter could ask, and an appropriate explanation could be provided by Richard Feynman.", "cite_spans": [{"start": 379, "end": 383, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Level 4"}, {"text": "To the best of our knowledge, existing systems lack this interaction capability.", "cite_spans": [], "ref_spans": [], "section": "Level 4"}, {"text": "As noted, much of the work on the explainability has focused on deep supervised learning, which describe methods that answer the following two questions:", "cite_spans": [], "ref_spans": [], "section": "XAI architecture"}, {"text": "(1) which input features are used to create an output prediction, and (2) So merely responding to questions (1) and (2) do not satisfy the multiple purposes that XAI researchers aim to achieve: to increase societal acceptance of algorithmic decision outcomes, to generate human-level transparency about why a decision outcome is achieved, and to have a fruitful conversation among different stakeholders concerning the justification of using these algorithms for decision-making [58] .", "cite_spans": [{"start": 479, "end": 483, "text": "[58]", "ref_id": "BIBREF58"}], "ref_spans": [], "section": "XAI architecture"}, {"text": "To incorporate an interactive \"explainer\" in XAI, an emerging XAI architecture needs to embed both an explainable model and an explanation interface.", "cite_spans": [], "ref_spans": [], "section": "XAI architecture"}, {"text": "The explainable model includes all types of the pre-hoc, post-hoc and concurrent explanation models. As examples of the explainable model, there can be a causal model, an explainable deep adaptive program, an explainable reinforcement learning model, etc. An explanation interface can be also a variety of types, such as a visualization system, or a dialogue manager with a query manager and a natural language generator that corresponds to Level-3 and Level-4 of Figure 1 .", "cite_spans": [], "ref_spans": [{"start": 464, "end": 472, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "XAI architecture"}, {"text": "As Miller [9] notes, the process of explanation involves two processes: (a) a cognitive process, namely the process of determining an explanation for a given event, called, as with Hempel, the explanandum. This identifies causes for the event, and a subset of these causes are selected as the explanation (or explanans); and (b) a social process of transferring knowledge between explainer and explainee, generally an interaction between a group of people, in which the goal is that the explainee has enough information to understand the causes of the event. This is one kind of blueprint for the Level 4 interactive explanation process noted above.", "cite_spans": [{"start": 10, "end": 13, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "User-guided explanation"}, {"text": "Miller provided an in-depth survey on explanation research in philosophy, psychology, and cognitive science. He noted that the latter could be a valuable resource for the progress of the field of XAI, and highlighted three major findings: (i) Explanations are contrastive: people do not ask why event E happened, but rather why event E happened instead of some other event F ; (ii)", "cite_spans": [], "ref_spans": [], "section": "User-guided explanation"}, {"text": "Explanations are selective and focus on one or two possible causes and not all causes for the recommendation; and (iii) Explanations are social conversation and interaction for transfer of knowledge, implying that the explainer must be able to leverage the mental model of the explainee while engaging in the explanation process. He asserted that it is imperative to take into account these three points if the goal is to build a useful XAI.", "cite_spans": [], "ref_spans": [], "section": "User-guided explanation"}, {"text": "One should note that it is plausible, given the study of explanation based on cognitive norms, that an explanation may not be required to be factual, but rather only to be judged to be satisfactory to the explainee (cf. Subsections 2.5, and 4.3).", "cite_spans": [], "ref_spans": [], "section": "User-guided explanation"}, {"text": "As we described in Figure 1 , a dialogue system that can process a question of \"what if another condition\" from an explainee and produce a new prediction output based on the new condition will achieve another higher level of explanation.", "cite_spans": [], "ref_spans": [{"start": 19, "end": 27, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "User-guided explanation"}, {"text": "The explanation that can deal with \"What would the outcome be if the data looked like this instead?\" or \"How could I alter the data to get outcome X?\"", "cite_spans": [], "ref_spans": [], "section": "User-guided explanation"}, {"text": "is called contrastive explanation. Contrastive explanation is a human-friendly explanation as it mimics human explanations that are contrastive, selective, and social.", "cite_spans": [], "ref_spans": [], "section": "User-guided explanation"}, {"text": "To accommodate the communication aspects of explanations, several dialogue models have been proposed. Bex and Walton [10] introduce a dialogue system for argumentation and explanation that consists of a communication language that defines the speech acts and protocols that allow transitions in the dialogue. This allows the explainee to challenge and interrogate the given explanations to gain further understanding. Madumal et al. [55] also proposed a grounded, data-driven approach for explanation interaction protocol between explainer and explainee.", "cite_spans": [{"start": 433, "end": 437, "text": "[55]", "ref_id": "BIBREF55"}], "ref_spans": [], "section": "User-guided explanation"}, {"text": "The production of explanations about decisions made by AI systems is not the end of the AI explainability debate. The practical value of these expla- User satisfaction is measured in terms of clarity of the explanation, and utility of the explanation. Task performance is to check if the explanation improved user's decision and task performance. Trust assessment is to assess trust and measure if it can be appropriate for future use. Assessment of a mental model is related to strength/weakness assessment, and it also assesses the predictions of \"what will it do\" or what if questions, and \"how do I intervene\"", "cite_spans": [], "ref_spans": [], "section": "Measuring value of explanations"}, {"text": "to adjust or guide explanatory outputs. Finally, Correctability is to measure if interaction with the system helps to identify and correct errors. As far as we are aware, there is also no Level 4 system that has confirmed any experiments that demonstrate this kind of richly faceted evaluation.", "cite_spans": [], "ref_spans": [], "section": "Measuring value of explanations"}, {"text": "Finally, to measure contrastive explanation that is close to human explanation, we need additional evaluation metrics for contrast, selection, and social explanation. Contrast can be measured in terms of the clear justification of the output through the comparison. Contrastive explanation should be able to explain why the output has been produced between the probable output candidates. Selection can be measured in terms of the importance (salience) of the reasons (features) that were mentioned during the contrastive explanation.", "cite_spans": [], "ref_spans": [], "section": "Measuring value of explanations"}, {"text": "Lastly, social explanation can be measured in terms of the clarity, understandability and utility of the explanation to the explainee. The measure of the social explanation corresponds to the measure of user satisfaction in [60] . But as noted, we know of no existing explanation systems that have been so considered with this rich palette of evaluation parameters.", "cite_spans": [{"start": 224, "end": 228, "text": "[60]", "ref_id": null}], "ref_spans": [], "section": "Measuring value of explanations"}, {"text": "In summary, our goal has been to articulate a set of required components of an XAI architecture, and describe a high level framework to understand their connections. In two alternative graphical depictions ( Figure 1 , Table 1 ), we distinguish what we believe are mostly orthogonal components of an explanation system, and suggest an information framework related to levels of autonomous driving, where a richer set of components provides a more sophisticated expla-nation system.", "cite_spans": [], "ref_spans": [{"start": 208, "end": 216, "text": "Figure 1", "ref_id": "FIGREF0"}, {"start": 219, "end": 226, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Summary and Conclusions"}, {"text": "That framework is descriptive and informal, but it allows us to factor some components (e.g., interpretability, explanation quality) into separate analyses, which we hope creates some line of sight to historical work on explanation. No where is this more important than the history of abductive reasoning and its connection to the history of scientific reasoning, culminating in the construction and use of causal models as a basis for causal explanations.", "cite_spans": [], "ref_spans": [], "section": "Summary and Conclusions"}, {"text": "We then try and consider more recent research in the context of these components and their relationship to the analysis of a deeper background literature, and provide some description of how those early ideas fit, and what they lack.", "cite_spans": [], "ref_spans": [], "section": "Summary and Conclusions"}, {"text": "This culminates with a considering of how to evaluate explanatory systems, and connects recent work that addresses the cognitive properties of explanations.", "cite_spans": [], "ref_spans": [], "section": "Summary and Conclusions"}, {"text": "Overall, we hope that our framework and analysis provides some connective tissue between historical threads of explanation mechanisms and modern reinterpretation of those mechanisms in the context of cognitive evaluation.", "cite_spans": [], "ref_spans": [], "section": "Summary and Conclusions"}, {"text": "We conclude that there is much still to do to inform a principled design of a high level explanation system, but that there are many components and integrating them with the appropriate knowledge representations within machine learning models, and respecting the cognitive aspects of evaluation, are a minimal requirement for progress.", "cite_spans": [], "ref_spans": [], "section": "Summary and Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "The limitations of deep learning", "authors": [{"first": "F", "middle": [], "last": "Chollet", "suffix": ""}], "year": 2017, "venue": "Deep Learning with Python", "volume": "2", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Please stop explaining black box models for high stakes decisions, 32nd Conference on Neural Information Processing Systems (NIPS 2018), Workshop on Critiquing and Correcting Trends in Machine Learning", "authors": [{"first": "C", "middle": [], "last": "Rudin", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Importance estimation for neural network pruning", "authors": [{"first": "P", "middle": [], "last": "Molchanov", "suffix": ""}, {"first": "A", "middle": [], "last": "Mallya", "suffix": ""}, {"first": "S", "middle": [], "last": "Tyree", "suffix": ""}, {"first": "I", "middle": [], "last": "Frosio", "suffix": ""}, {"first": "J", "middle": [], "last": "Kautz", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Can increasing depth serve to accelerate optimization?", "authors": [{"first": "N", "middle": [], "last": "Cohen", "suffix": ""}], "year": 2018, "venue": "", "volume": "BLOG", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Scientific explanation", "authors": [{"first": "J", "middle": [], "last": "Woodward", "suffix": ""}], "year": null, "venue": "Stanford Encyclopedia of Philosophy", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "The Book of Why: The New Science of Cause and Effect, Basic Books", "authors": [{"first": "J", "middle": [], "last": "Pearl", "suffix": ""}, {"first": "D", "middle": [], "last": "Mackenzie", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Computer science as empirical inquiry: symbols and search", "authors": [{"first": "A", "middle": [], "last": "Newell", "suffix": ""}, {"first": "H", "middle": ["A"], "last": "Simon", "suffix": ""}], "year": 1976, "venue": "Communications of the ACM", "volume": "19", "issn": "", "pages": "113--146", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Society of Automobile Engineers, Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles", "authors": [], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences", "authors": [{"first": "T", "middle": [], "last": "Miller", "suffix": ""}], "year": 2019, "venue": "Artificial Intelligence", "volume": "267", "issn": "", "pages": "1--38", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Combining explanation and argumentation in dialogue", "authors": [{"first": "F", "middle": [], "last": "Bex", "suffix": ""}, {"first": "D", "middle": [], "last": "Walton", "suffix": ""}], "year": 2016, "venue": "Argument and Computation", "volume": "7", "issn": "1", "pages": "55--68", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Understanding black-box predictions via influence functions", "authors": [{"first": "P", "middle": ["W"], "last": "Koh", "suffix": ""}, {"first": "P", "middle": [], "last": "Liang", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1703.04730"]}}, "BIBREF11": {"ref_id": "b11", "title": "Explainable neural networks based on additive index models", "authors": [{"first": "J", "middle": [], "last": "Vaughan", "suffix": ""}, {"first": "A", "middle": [], "last": "Sudjianto", "suffix": ""}, {"first": "E", "middle": [], "last": "Brahimi", "suffix": ""}, {"first": "J", "middle": [], "last": "Chen", "suffix": ""}, {"first": "V", "middle": ["N"], "last": "Nair", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1806.01933"]}}, "BIBREF12": {"ref_id": "b12", "title": "Explaining explanations: An approach to evaluating interpretability of machine learning", "authors": [{"first": "L", "middle": ["H"], "last": "Gilpin", "suffix": ""}, {"first": "D", "middle": [], "last": "Bau", "suffix": ""}, {"first": "B", "middle": ["Z"], "last": "Yuan", "suffix": ""}, {"first": "A", "middle": [], "last": "Bajwa", "suffix": ""}, {"first": "M", "middle": [], "last": "Specter", "suffix": ""}, {"first": "L", "middle": [], "last": "", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["CoRRabs/1806.00069.arXiv:1806.00069"]}}, "BIBREF13": {"ref_id": "b13", "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "authors": [{"first": "C", "middle": [], "last": "Rudin", "suffix": ""}], "year": 2019, "venue": "Nature Machine Intelligence", "volume": "1", "issn": "5", "pages": "206--215", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Introduction to Mathematical Logic", "authors": [{"first": "E", "middle": [], "last": "Mendelson", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "An introduction to deep visual explanation", "authors": [{"first": "H", "middle": [], "last": "Babiker", "suffix": ""}, {"first": "R", "middle": [], "last": "Goebel", "suffix": ""}], "year": 2017, "venue": "31st Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Overcoming Textbook Fatigue: 21st Century Tools to Revitalize Teaching and Learning, Association for Supervision and Curriculum Development", "authors": [{"first": "R", "middle": ["C"], "last": "Lent", "suffix": ""}], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "British Broadcasting Corporation, Richard Feynman: Magnets and Why Questions", "authors": [], "year": 1983, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Feeding machine learning with knowledge graphs for explainable object detection", "authors": [{"first": "F", "middle": [], "last": "Lcu", "suffix": ""}, {"first": "T", "middle": [], "last": "Pommellet", "suffix": ""}], "year": null, "venue": "Proceedings of the ISWC 2019 Satellite Tracks (Posters & Demonstrations, Industry, and Outrageous Ideas", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "International Semantic Web Conference (ISWC 2019", "authors": [], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "277--280", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "2015 IEEE International Conference on Computer Vision (ICCV)", "authors": [{"first": "R", "middle": [], "last": "Girshick", "suffix": ""}, {"first": "R-Cnn", "middle": [], "last": "Fast", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "1440--1448", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Learning explanatory rules from noisy data", "authors": [{"first": "R", "middle": [], "last": "Evans", "suffix": ""}, {"first": "E", "middle": [], "last": "Greffenstette", "suffix": ""}], "year": 2018, "venue": "Journal of Artificial Intelligence Research", "volume": "61", "issn": "", "pages": "1--64", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "2009 IEEE Conference on Computer Vision and Pattern Recognition", "authors": [{"first": "J", "middle": [], "last": "Deng", "suffix": ""}, {"first": "W", "middle": [], "last": "Dong", "suffix": ""}, {"first": "R", "middle": [], "last": "Socher", "suffix": ""}, {"first": "L", "middle": [], "last": "Li", "suffix": ""}, {"first": "Kai", "middle": [], "last": "Li", "suffix": ""}, {"first": "Li", "middle": [], "last": "Fei-Fei", "suffix": ""}], "year": 2009, "venue": "", "volume": "", "issn": "", "pages": "248--255", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Explaining and harnessing adversarial examples", "authors": [{"first": "I", "middle": ["J"], "last": "Goodfellow", "suffix": ""}, {"first": "J", "middle": [], "last": "Shlens", "suffix": ""}, {"first": "C", "middle": [], "last": "Szegedy", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6572"]}}, "BIBREF24": {"ref_id": "b24", "title": "Hackers can trick a Tesla into accelerating by 50 miles per hour", "authors": [{"first": "Patrick", "middle": [], "last": "Howell", "suffix": ""}, {"first": "O&apos;", "middle": [], "last": "Neil", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "Towards a humanlike open-domain chatbot", "authors": [{"first": "D", "middle": [], "last": "Adiwardana", "suffix": ""}, {"first": "M.-T", "middle": [], "last": "Luong", "suffix": ""}, {"first": "D", "middle": ["R"], "last": "So", "suffix": ""}, {"first": "J", "middle": [], "last": "Hall", "suffix": ""}, {"first": "N", "middle": [], "last": "Fiedel", "suffix": ""}, {"first": "R", "middle": [], "last": "Thoppilan", "suffix": ""}, {"first": "Z", "middle": [], "last": "Yang", "suffix": ""}, {"first": "A", "middle": [], "last": "Kulshreshtha", "suffix": ""}, {"first": "G", "middle": [], "last": "Nemade", "suffix": ""}, {"first": "Y", "middle": [], "last": "Lu", "suffix": ""}, {"first": "Q", "middle": ["V"], "last": "Le", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2001.09977"]}}, "BIBREF26": {"ref_id": "b26", "title": "Artificial intelligence: Does another huge language model prove anything?", "authors": [{"first": "B", "middle": [], "last": "Dickson", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Bleu: a method for automatic evaluation of machine translation", "authors": [{"first": "T", "middle": ["W K"], "last": "Papineni", "suffix": ""}, {"first": "S", "middle": [], "last": "Roukos", "suffix": ""}, {"first": "W", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2002, "venue": "Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL)", "volume": "", "issn": "", "pages": "311--318", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "ROUGE: A package for automatic evaluation of summaries", "authors": [{"first": "C.-Y.", "middle": [], "last": "Lin", "suffix": ""}], "year": 2004, "venue": "Text Summarization Branches Out, Association for Computational Linguistics", "volume": "", "issn": "", "pages": "74--81", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "authors": [{"first": "C.-W", "middle": [], "last": "Liu", "suffix": ""}, {"first": "R", "middle": [], "last": "Lowe", "suffix": ""}, {"first": "I", "middle": ["V"], "last": "Serban", "suffix": ""}, {"first": "M", "middle": [], "last": "Noseworthy", "suffix": ""}, {"first": "L", "middle": [], "last": "Charlin", "suffix": ""}, {"first": "J", "middle": [], "last": "Pineau", "suffix": ""}], "year": 2016, "venue": "Proceedings 35 of the 2016 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "2122--2132", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "Information Visualization", "authors": [{"first": "R", "middle": [], "last": "Spence", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "Empirical studies in information visualization: Seven scenarios", "authors": [{"first": "H", "middle": [], "last": "Lam", "suffix": ""}, {"first": "E", "middle": [], "last": "Bertini", "suffix": ""}, {"first": "P", "middle": [], "last": "Isenberg", "suffix": ""}, {"first": "C", "middle": [], "last": "Plaisant", "suffix": ""}, {"first": "S", "middle": [], "last": "Carpendale", "suffix": ""}], "year": 2012, "venue": "IEEE Transactions on Graphics and Visual Computing", "volume": "18", "issn": "9", "pages": "1520--1536", "other_ids": {}}, "BIBREF32": {"ref_id": "b32", "title": "The role of direct manipulation of visualizations in the development and use of multi-level knowledge models", "authors": [{"first": "R", "middle": [], "last": "Goebel", "suffix": ""}, {"first": "W", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Y", "middle": [], "last": "Tanaka", "suffix": ""}], "year": 2013, "venue": "Proceedings of the 17th International Conference on Information Visualisation, IV 13", "volume": "", "issn": "", "pages": "325--332", "other_ids": {}}, "BIBREF33": {"ref_id": "b33", "title": "The architecture of theories", "authors": [{"first": "C", "middle": ["S"], "last": "Peirce", "suffix": ""}], "year": null, "venue": "The Monist", "volume": "1", "issn": "2", "pages": "161--176", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "On the mechanization of abductive logic", "authors": [{"first": "H", "middle": [], "last": "Pople", "suffix": ""}], "year": 1973, "venue": "IJCAI73: Proceedings of the 3rd International Joint Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "147--152", "other_ids": {}}, "BIBREF35": {"ref_id": "b35", "title": "Theorist: A logical reasoning system for defaults and diagnosis", "authors": [{"first": "D", "middle": [], "last": "Poole", "suffix": ""}, {"first": "R", "middle": [], "last": "Goebel", "suffix": ""}, {"first": "R", "middle": [], "last": "Aleliunas", "suffix": ""}], "year": 1987, "venue": "", "volume": "", "issn": "", "pages": "331--352", "other_ids": {}}, "BIBREF36": {"ref_id": "b36", "title": "Inductive logic programming", "authors": [{"first": "S", "middle": [], "last": "Muggleton", "suffix": ""}], "year": 1991, "venue": "New Generation Computing", "volume": "", "issn": "", "pages": "295--318", "other_ids": {}}, "BIBREF37": {"ref_id": "b37", "title": "Abductiona way to deeper understanding of the world of caring", "authors": [{"first": "K", "middle": [], "last": "Eriksson", "suffix": ""}, {"first": "U", "middle": ["\u00c5"], "last": "Lindstr\u00f6m", "suffix": ""}], "year": 1997, "venue": "Scandinavian journal of caring sciences", "volume": "11", "issn": "4", "pages": "195--198", "other_ids": {}}, "BIBREF38": {"ref_id": "b38", "title": "Using your logical powers: Abductive reasoning for business success", "authors": [{"first": "I", "middle": [], "last": "Jokhio", "suffix": ""}, {"first": "I", "middle": [], "last": "Chalmers", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF39": {"ref_id": "b39", "title": "Aristotle on causality", "authors": [{"first": "A", "middle": [], "last": "Falcon", "suffix": ""}], "year": null, "venue": "Stanford Encyclopedia of Philosophy", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF40": {"ref_id": "b40", "title": "The function of general laws in history", "authors": [{"first": "C", "middle": ["G"], "last": "Hempel", "suffix": ""}], "year": 1942, "venue": "The Journal of Philosophy", "volume": "39", "issn": "2", "pages": "35--48", "other_ids": {}}, "BIBREF41": {"ref_id": "b41", "title": "The theoretician's dilemma: A study in the logic of theory construction, Minnesota Studies in the", "authors": [{"first": "C", "middle": ["G"], "last": "Hempel", "suffix": ""}], "year": 1958, "venue": "Philosophy of Science", "volume": "2", "issn": "", "pages": "173--226", "other_ids": {}}, "BIBREF42": {"ref_id": "b42", "title": "Aspects of scientific explanation", "authors": [{"first": "C", "middle": ["G"], "last": "Hempel", "suffix": ""}], "year": 1965, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF43": {"ref_id": "b43", "title": "Studies in the logic of explanation, Philosophy of science", "authors": [{"first": "C", "middle": ["G"], "last": "Hempel", "suffix": ""}, {"first": "P", "middle": [], "last": "Oppenheim", "suffix": ""}], "year": 1948, "venue": "", "volume": "15", "issn": "", "pages": "135--175", "other_ids": {}}, "BIBREF44": {"ref_id": "b44", "title": "Philosophy of science: An historical anthology", "authors": [{"first": "T", "middle": [], "last": "Mcgrew", "suffix": ""}, {"first": "M", "middle": [], "last": "Alspector-Kelly", "suffix": ""}, {"first": "F", "middle": [], "last": "Allhoff", "suffix": ""}], "year": 2009, "venue": "", "volume": "14", "issn": "", "pages": "", "other_ids": {}}, "BIBREF45": {"ref_id": "b45", "title": "Boston Studies in the Philosophy and History of Science", "authors": [{"first": "M", "middle": [], "last": "Bunzl", "suffix": ""}], "year": 1993, "venue": "", "volume": "149", "issn": "", "pages": "", "other_ids": {}}, "BIBREF47": {"ref_id": "b47", "title": "Rationalizing neural predictions", "authors": [{"first": "T", "middle": [], "last": "Lei", "suffix": ""}, {"first": "R", "middle": [], "last": "Barzilay", "suffix": ""}, {"first": "T", "middle": [], "last": "Jaakkola", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "107--117", "other_ids": {"DOI": ["10.18653/v1/D16-1011"]}}, "BIBREF48": {"ref_id": "b48", "title": "Hierarchical attention networks for document classification", "authors": [{"first": "Z", "middle": [], "last": "Yang", "suffix": ""}, {"first": "D", "middle": [], "last": "Yang", "suffix": ""}, {"first": "C", "middle": [], "last": "Dyer", "suffix": ""}, {"first": "X", "middle": [], "last": "He", "suffix": ""}, {"first": "A", "middle": [], "last": "Smola", "suffix": ""}, {"first": "E", "middle": [], "last": "Hovy", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies", "volume": "", "issn": "", "pages": "1480--1489", "other_ids": {}}, "BIBREF49": {"ref_id": "b49", "title": "Is attention interpretable?", "authors": [{"first": "S", "middle": [], "last": "Serrano", "suffix": ""}, {"first": "N", "middle": ["A"], "last": "Smith", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "2931--2951", "other_ids": {"DOI": ["10.18653/v1/P19-1282"]}}, "BIBREF50": {"ref_id": "b50", "title": "Attention is not explanation", "authors": [{"first": "S", "middle": [], "last": "Jain", "suffix": ""}, {"first": "B", "middle": ["C"], "last": "Wallace", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1902.10186"]}}, "BIBREF51": {"ref_id": "b51", "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "authors": [{"first": "R", "middle": ["R"], "last": "Selvaraju", "suffix": ""}, {"first": "M", "middle": [], "last": "Cogswell", "suffix": ""}, {"first": "A", "middle": [], "last": "Das", "suffix": ""}, {"first": "R", "middle": [], "last": "Vedantam", "suffix": ""}, {"first": "D", "middle": [], "last": "Parikh", "suffix": ""}, {"first": "D", "middle": [], "last": "Batra", "suffix": ""}], "year": 2017, "venue": "Proceedings of the IEEE international", "volume": "", "issn": "", "pages": "618--626", "other_ids": {}}, "BIBREF52": {"ref_id": "b52", "title": "Why should i trust you?: Explaining the predictions of any classifier", "authors": [{"first": "M", "middle": ["T"], "last": "Ribeiro", "suffix": ""}, {"first": "S", "middle": [], "last": "Singh", "suffix": ""}, {"first": "C", "middle": [], "last": "Guestrin", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining", "volume": "", "issn": "", "pages": "1135--1144", "other_ids": {}}, "BIBREF53": {"ref_id": "b53", "title": "A unified approach to interpreting model predictions", "authors": [{"first": "S", "middle": ["M"], "last": "Lundberg", "suffix": ""}, {"first": "S.-I", "middle": [], "last": "Lee", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "4765--4774", "other_ids": {}}, "BIBREF54": {"ref_id": "b54", "title": "Local rule-based explanations of black box decision systems", "authors": [{"first": "R", "middle": [], "last": "Guidotti", "suffix": ""}, {"first": "A", "middle": [], "last": "Monreale", "suffix": ""}, {"first": "S", "middle": [], "last": "Ruggieri", "suffix": ""}, {"first": "D", "middle": [], "last": "Pedreschi", "suffix": ""}, {"first": "F", "middle": [], "last": "Turini", "suffix": ""}, {"first": "F", "middle": [], "last": "Giannotti", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1805.10820"]}}, "BIBREF55": {"ref_id": "b55", "title": "Towards a grounded dialog model for explainable artificial intelligence", "authors": [{"first": "P", "middle": [], "last": "Madumal", "suffix": ""}, {"first": "T", "middle": [], "last": "Miller", "suffix": ""}, {"first": "F", "middle": [], "last": "Vetere", "suffix": ""}, {"first": "L", "middle": [], "last": "Sonenberg", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF56": {"ref_id": "b56", "title": "Grounding visual explanations", "authors": [{"first": "L", "middle": ["A"], "last": "Hendricks", "suffix": ""}, {"first": "R", "middle": [], "last": "Hu", "suffix": ""}, {"first": "T", "middle": [], "last": "Darrell", "suffix": ""}, {"first": "Z", "middle": [], "last": "Akata", "suffix": ""}], "year": 2018, "venue": "European Conference on Computer Vision", "volume": "", "issn": "", "pages": "269--286", "other_ids": {}}, "BIBREF57": {"ref_id": "b57", "title": "Interpretable neural predictions with differentiable binary variables", "authors": [{"first": "J", "middle": [], "last": "Bastings", "suffix": ""}, {"first": "W", "middle": [], "last": "Aziz", "suffix": ""}, {"first": "I", "middle": [], "last": "Titov", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting 38 of the Association for Computational Linguistics, Association for Computational Linguistics", "volume": "", "issn": "", "pages": "19--1284", "other_ids": {"DOI": ["10.18653/v1/P19-1284"]}}, "BIBREF58": {"ref_id": "b58", "title": "Mathematical Decisions & Non-causal Elements of Explainable AI", "authors": [{"first": "A", "middle": [], "last": "Kasirzadeh", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF59": {"ref_id": "b59", "title": "Normality Part descriptive, part prescriptive", "authors": [{"first": "A", "middle": [], "last": "Bear", "suffix": ""}, {"first": "J", "middle": [], "last": "Knobe", "suffix": ""}], "year": 2017, "venue": "Cognition", "volume": "167", "issn": "", "pages": "25--37", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Major Explanatory Components and their Potential Role in a Scale of Explanation of formal systems to modern applications of AI. However, we believe that one can re-establish some important scientific momentum by exploiting what Newell", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Interpretability of a model vs. Explainability of a prediction.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "a saliency map to figure out what area in the image was contributing more to the final activations is not really helpful for a (lay) human consuming the model output to understand misclassifications, but it may help a researcher at design time to figure out alternatives to overcome the model limitations. In this case, the authors augmented the model by post-processing the final results using an external knowledge graph to add semantic context and modify the confidence score of the recognized objects. An alternative, perhaps more foundational model, is presented by Evans and Greffenstette in [21], who articulate an inductive logic programming (ILP) framework in which explanatory models are identified in the space of all possible inductive logic programs. This framework requires the development of a mea-sure space for all such ILP instances, in which a gradient can be determined. But the positive consequences of that technical maneuver is that an instance of an inductive logic program can be interpreted at the level of the semantics of an application domain, all the way down to instructions for a Turing machine. This framework does not resolve the challenge of what an appropriate level of explanation should be for a particular explainee; but it does provide a rich and mathematically elegant space in which to identify everything from descriptions of computation to arrive at a predictive model all the way to rule-based specifications at the level of an application domain.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "A panda image mistakenly classified as a gibbon after noise is added[23].", "latex": null, "type": "figure"}, "FIGREF5": {"text": "A modified speed limit sign reads as 85 mph on the Tesla's heads-up display[24].", "latex": null, "type": "figure"}, "FIGREF6": {"text": "https://leadersprize.truenorthwaterloo.com/en/ 14 of proposals for explanation evaluation beg the need for increased precision.For example,[25] suggest explanation quality is dependent on two main factors: sensibleness and specificity. A measure which takes those factors into account (Sensibleness and Specificity Average -SSA). This suggestion arose from work on the topic of dialogue systems, and has been characterized in terms of a high correlation with another measure called \"perplexity:\" a measurement of how well a probability distribution or probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample. In the context of conversational systems, perplexity measures the uncertainty of a language model, which is a probability distribution over entire sentences or texts. The lower the perplexity, the more confident the model is in generating the next token (character, subword, or word). Thus, perplexity can be understood as a representation of the number of choices the model is trying to choose from when producing the next token. This measure is commonly used to assess the quality of conversational agents and as a metric which must be optimized by machine learning based dialogue models. Thus, although not ideal and lacking specific experiments on the domain of explainability, perplexity could potentially be effectively used to evaluate text-based XAI systems as a reasonable approximation of human evaluation.While we have more to say about evaluation below, what is clear is that evaluation of explanatory systems is based on how the explainee confirms their own understanding of an explanation or the conclusion of an explanatory dialogue.3. A brief history of explanation3.1. AbductionExplanations have always been an indispensable component of decision making, learning, understanding, and communication in the human-in-the-loop environments. After the emergence and rapid growth of artificial intelligence as a science in the 1950s, an interest in interpreting underlying decisions of intelligent systems also proliferated. Especially, C.S. Peirces hypothesis of abduction[33] stimulated the AI communitys attention to exploiting this conceptual framework for the design and development of complex expert systems in a variety of domains. Abduction or abductive reasoning is a form of reasoning that starts with a set of observations and then uses them to find the most likely explanations for the observations.A compressed historical journey of Peirce's ideas can be traced in four projects, beginning with Pople [34], Poole et al. [35], Muggleton [36], to Evans et al. [21]. In 1973, Pople provided a description of an algorithm to implement abductive and showed its application to medical diagnosis. Poole et al. extended abductive ideas to a full first order implementation and showed its application to guide the creation of explanatory hypothesis for any application domain. Muggleton produced a further refined system called inductive logic programming, in which creation of hypotheses are generally identified by inductive constraints in any general logic. Finally, the adoption of this thread of mechanisms based on abductive reasoning have been generalized to the full scope of explanation generation based on inductive logic programming by Evans et al. Every instance of these contributions relies on a logical architecture in which explanations arise as rational connections between hypotheses and observations (cf. scientific explanation). The most recent work by Evans et al. extends the framework in a manner that supports modern heuristics of inductive model construction -or learning of predictive models -by providing the definition of a gradient measure to guide search over alternative inductive logic programs.", "latex": null, "type": "figure"}, "FIGREF7": {"text": "The process steps of the reasoning methods", "latex": null, "type": "figure"}, "FIGREF8": {"text": "why a system came to a conclusion (or an answer to a query). But interpretation of syntactic expressions is what creates asemantic interpretation. Returning to the idea of estimands, an estimand can be viewed as a well-constructed expression if it makes sense semantically. As with the simple syllogism above, the form of the explanation can be based on that of the causal (or any) model. In this era of deep learned models, we can consider these relationships between syntax and semantics as the internal representations of each layer and their composition at the final layer respectively. Interestingly, this notion of construction of semantics (whole) as a function of semantics of its parts and their careful combination that obeys a particular syntax is very familiar in the logics to interpret natural language, developed by a famous linguist-logician, Richard Montague [46]. At the syntactic level we might infer the correlation among different variables (in the intermediate layers) of a deep learned system but in semantic level we know what combination of those variables (in the final layer) provide an interpretation for a particular query. Ontology driven expla-", "latex": null, "type": "figure"}, "FIGREF9": {"text": "now a black-box model. Ribeiro et al. introduced LIME [52] to approach the explanation problem using a perturbation method. They perturb the original data point to create a new dataset in the vicinity of that instance. The black-box model is queried to get the labels associated with the aforementioned points.", "latex": null, "type": "figure"}, "FIGREF10": {"text": "classifies some of the most prominent existing work based on the levels of explanation. Most of recent research have focused on Level 1, and only a few have worked on Level 2.", "latex": null, "type": "figure"}, "FIGREF11": {"text": "which input features are semantically correlated with the outcome prediction. The answers for these two questions contribute to the trust in the system, but explanation additionally requires a social process of transferring knowledge to the explainee considering the background knowledge of the explainee.While the answers to questions (1) and (2) may acknowledge the importance of features that a model uses to arrive at a prediction, it may not necessarily align with a human explanation; prior knowledge, experience and other forms of approximate reasoning (e.g., metaphorical inference) may further shape an explanation, while the predictions of a machine learning model may be restricted to the dataset and the semantics around it. Generally, an explanation system (for example, a human) is not restricted to the knowledge on which they make predictions and explanations and can draw parallels with different events, semantics and knowledge.", "latex": null, "type": "figure"}, "FIGREF12": {"text": "Some disagreements with an explanation for a decision outcome in a sensitive context due to the background assumptions of the audience of explanations reveal some moral or social mismatch about algorithmic decision-making between the receiver of an explanation and its producer.If one does not have an appropriate level of knowledge about the relevant precedent assumptions, one might not have the capacity to judge and interpret an explanation of a decision. In that case, iteratively refined question-answer dialogue (cf. Fenyman's point made in Section 2.2) may lead to an improved understanding by the explainee. In general the interpretability of explanations has significant practical value for revealing the explicit and the implicit reasons about why a decision-making procedure and process is chosen.A schema for the interpretability of explanations aims to capture various precedent assumptions that become relevant in context-dependent evaluation of each kind of AI explanation for why a decision outcome is achieved.Finally, in another elaboration of how to evaluate explanations[60], there are proposed five measures of explanation effectiveness:", "latex": null, "type": "figure"}, "TABREF0": {"text": "Tabular Representation of Levels of Explanation and the Corresponding Attributes", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Attributes Level 0 </td><td>Level 1 </td><td>Level 2 </td><td>Level 3 </td><td>Level 4\n</td></tr><tr><td>Explicit Explanation Representation </td><td>X </td><td>X </td><td>X </td><td>X\n</td></tr><tr><td>Alternative Explanations </td><td>\u00a0</td><td>X </td><td>X </td><td>X\n</td></tr><tr><td>Knowledge of the explainee </td><td>\u00a0</td><td>\u00a0</td><td>X </td><td>X\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>Interactive </td><td>X\n</td></tr></table></body></html>"}, "TABREF1": {"text": "But the manner in which representations emerge in the context of empirical developments in machine learning has not typically been guided by any adaption of extension of the systems of interpretability and semantics of logic. Our", "latex": null, "type": "table"}, "TABREF2": {"text": "As such, it becomes clear that, in the context of XAI, systems should be able to effectively take background knowledge into consideration in order to connect predictions and predictive models, and to shape explanations to the appropriate level of detail, i.e., adjusting explanations to conform to the knowledge of the corresponding explainee. However, the most common current approaches to", "latex": null, "type": "table"}, "TABREF4": {"text": "Classification of recent explanation techniques based on Levels of Explanation. MD", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Method </td><td>Level </td><td>MD/MI </td><td>CC/PH\n</td></tr><tr><td>LIME [52] </td><td>1 </td><td>MI </td><td>PH\n</td></tr><tr><td>Grad-CAM [51] </td><td>1 </td><td>MD </td><td>PH\n</td></tr><tr><td>SHAP [53] </td><td>1 </td><td>MI </td><td>PH\n</td></tr><tr><td>Rationalizing-predictions [47] </td><td>1 </td><td>MD </td><td>CC\n</td></tr><tr><td>Grounding visual explanations [56] </td><td>2 </td><td>MD </td><td>PH\n</td></tr></table></body></html>"}, "TABREF5": {"text": "nations, partly, depends on the audience who consumes them: an explanation must result in an appropriate level of understanding for the receivers of explanations. In other words, explanations are required to be interpreted and judged against different points, about whether they are good or bad, satisfactory or unsatisfactory, effective or ineffective, acceptable or unacceptable.Again the previously mentioned evolution of the evaluation of visualization systems is highly relevant, as that evolution ultimately requires the design of29 cognitive experiments to confirm the quality and value of alternative explanations, visual or not (see Subsection 2.5). It is clearly the case that quality of a \"visual explanation\" is about how well it leads the reader to the intended inferences from the visualized data domain. Naturally, the background knowledge of a viewer is like the background knowledge of an explainee; their knowledge and experience determines what preferred inferences obtain.Looking forward to how to evaluate XAI systems, among those background assumptions that impact the judgements of explanations are what are returned to as cognitive \"norms.\" It has been empirically shown that norms influence causal judgements[59]. To put it simply, norms are informal rules that are held by people, and can have statistical or prescriptive content. The empirical and mathematical aspects for why a decision outcome is achieved are interpreted against some background assumptions held by the audiences of explanations.", "latex": null, "type": "table"}}, "back_matter": [{"text": "We acknowledge support from the Alberta Machine Intelligence Institute (AMII), from the Computing Science Department of the University of Alberta, and the Natural Sciences and Engineering Research Council of Canada (NSERC).", "cite_spans": [], "ref_spans": [], "section": "Acknowledgements"}]}