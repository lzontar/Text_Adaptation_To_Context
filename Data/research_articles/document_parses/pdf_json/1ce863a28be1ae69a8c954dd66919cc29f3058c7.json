{"paper_id": "1ce863a28be1ae69a8c954dd66919cc29f3058c7", "metadata": {"title": "DAKE: Document-Level Attention for Keyphrase Extraction", "authors": [{"first": "Tokala", "middle": [], "last": "Yaswanth", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Sri", "middle": ["Sai"], "last": "Santosh", "suffix": "", "affiliation": {"laboratory": "", "institution": "IIT Kharagpur", "location": {"postCode": "721302", "settlement": "Kharagpur", "country": "India"}}, "email": "santoshtyss@gmail.com"}, {"first": "Debarshi", "middle": [], "last": "Kumar Sanyal", "suffix": "", "affiliation": {"laboratory": "", "institution": "IIT Kharagpur", "location": {"postCode": "721302", "settlement": "Kharagpur", "country": "India"}}, "email": "debarshisanyal@gmail.com"}, {"first": "Plaban", "middle": ["Kumar"], "last": "Bhowmick", "suffix": "", "affiliation": {}, "email": "plaban@cet.iitkgp.ac.in"}, {"first": "Partha", "middle": ["Pratim"], "last": "Das", "suffix": "", "affiliation": {"laboratory": "", "institution": "IIT Kharagpur", "location": {"postCode": "721302", "settlement": "Kharagpur", "country": "India"}}, "email": ""}]}, "abstract": [{"text": "Keyphrases provide a concise representation of the topical content of a document and they are helpful in various downstream tasks. Previous approaches for keyphrase extraction model it as a sequence labelling task and use local contextual information to understand the semantics of the input text but they fail when the local context is ambiguous or unclear. We present a new framework to improve keyphrase extraction by utilizing additional supporting contextual information. We retrieve this additional information from other sentences within the same document. To this end, we propose Document-level Attention for Keyphrase Extraction (DAKE), which comprises Bidirectional Long Short-Term Memory networks that capture hidden semantics in text, a document-level attention mechanism to incorporate document level contextual information, gating mechanisms which help to determine the influence of additional contextual information on the fusion with local contextual information, and Conditional Random Fields which capture output label dependencies. Our experimental results on a dataset of research papers show that the proposed model outperforms previous state-of-the-art approaches for keyphrase extraction.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Keyphrase extraction is the task of automatically extracting words or phrases from a text, which concisely represent the essence of the text. Because of the succinct expression, keyphrases are widely used in many tasks like document retrieval [13, 25] , document categorization [9, 12] , opinion mining [3] and summarization [24, 31] . Figure 1 shows an example of a title and the abstract of a research paper along with the author-specified keyphrases highlighted in bold.", "cite_spans": [{"start": 243, "end": 247, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 248, "end": 251, "text": "25]", "ref_id": "BIBREF24"}, {"start": 278, "end": 281, "text": "[9,", "ref_id": "BIBREF8"}, {"start": 282, "end": 285, "text": "12]", "ref_id": "BIBREF11"}, {"start": 303, "end": 306, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 325, "end": 329, "text": "[24,", "ref_id": "BIBREF23"}, {"start": 330, "end": 333, "text": "31]", "ref_id": "BIBREF31"}], "ref_spans": [{"start": 336, "end": 344, "text": "Figure 1", "ref_id": "FIGREF1"}], "section": "Introduction"}, {"text": "Present methods for keyphrase extraction follow a two-step procedure where they select important phrases from the document as potential keyphrase candidates by heuristic rules [18, 28, 29] and then the extracted candidate phrases are ranked either by unsupervised approaches [17, 21, 27] or supervised approaches [18, 22, 29] . Unsupervised approaches score those candidate phrases based on individual words comprising the candidate phrases. They utilize various scoring measures based on the informativeness of the word with respect to the whole document [10] . Other paradigms utilize graph-based ranking algorithms wherein each word in the document is mapped to a node in the graph and the connecting edges in the graph represent the association patterns among the words in the document. Then, the scores of the individual words are estimated using various graph centrality measures [6, 21, 27] . On the other hand, supervised approaches [4, 14] use binary classification to label the extracted candidate phrases as keyphrases or non-keyphrases, based on various features such as, tf-idf, part-of-speech (POS) tags, and the position of phrases in the document. The major limitation of these supervised approaches is that they classify the labels of each candidate phrase independently without taking into account the dependencies that could potentially exist between neighbouring labels and they also ignore the semantic meaning of the text. To overcome the above stated limitation, [8] formulated keyphrase extraction as a sequence labeling task and used linear-chain Conditional Random Fields for this task. However, this approach does not explicitly take into account the long-term dependencies and semantics of the text. More recently, to capture both the semantics of the text as well as the dependencies among the labels of neighboring words [1] used a deep learning-based approach called BiLSTM-CRF which combines a bi-directional Long Short-Term Memory (BiLSTM) layer that models the sequential input text with a Conditional Random Field (CRF) layer that captures the dependencies in the output.", "cite_spans": [{"start": 176, "end": 180, "text": "[18,", "ref_id": "BIBREF17"}, {"start": 181, "end": 184, "text": "28,", "ref_id": "BIBREF27"}, {"start": 185, "end": 188, "text": "29]", "ref_id": "BIBREF29"}, {"start": 275, "end": 279, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 280, "end": 283, "text": "21,", "ref_id": "BIBREF20"}, {"start": 284, "end": 287, "text": "27]", "ref_id": "BIBREF26"}, {"start": 313, "end": 317, "text": "[18,", "ref_id": "BIBREF17"}, {"start": 318, "end": 321, "text": "22,", "ref_id": "BIBREF21"}, {"start": 322, "end": 325, "text": "29]", "ref_id": "BIBREF29"}, {"start": 556, "end": 560, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 886, "end": 889, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 890, "end": 893, "text": "21,", "ref_id": "BIBREF20"}, {"start": 894, "end": 897, "text": "27]", "ref_id": "BIBREF26"}, {"start": 941, "end": 944, "text": "[4,", "ref_id": "BIBREF3"}, {"start": 945, "end": 948, "text": "14]", "ref_id": "BIBREF13"}, {"start": 1486, "end": 1489, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 1851, "end": 1854, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Introduction"}, {"text": "Title: DCE-MRI data analysis for cancer area classification. Abstract: The paper aims at improving the support of medical researchers in the context of in-vivo cancer imaging. [..] The proposed approach is based on a three-step procedure: i) robust feature extraction from raw time-intensity curves, ii) voxel segmentation, and iii) voxel classification based on a learningby-example approach. Finally, in the third step, a support vector machine (SVM) is trained to classify voxels according to the labels obtained by the clustering phase. The above mentioned approaches treat keyphrase extraction as a sentencelevel task where sentences in the same document are viewed as independent. When labeling a word, local contextual information from the surrounding words is crucial because the context gives insight to the semantic meaning of the word. However, there are many instances in which the local context is ambiguous or lacks sufficient information. If the model has access to supporting information that provides additional context, the model may use this additional supporting information to predict the label correctly. Such additional supporting information may be found from other sentences in the same document from which the query sentence is taken. To utilize this additional supporting information, we propose a document-level attention mechanism inspired from [20, 30] ; it dynamically weights the additional supporting information emphasizing the most relevant information from each supporting sentence with respect to the local context. But leveraging this additional supporting information has a downside of introducing noise into the representations. To alleviate this problem, we use a gating mechanism [20, 30] that balances the influence of the local contextual representations and the additional supporting information from the document-level contextual representations.", "cite_spans": [{"start": 1374, "end": 1378, "text": "[20,", "ref_id": "BIBREF19"}, {"start": 1379, "end": 1382, "text": "30]", "ref_id": "BIBREF30"}, {"start": 1722, "end": 1726, "text": "[20,", "ref_id": "BIBREF19"}, {"start": 1727, "end": 1730, "text": "30]", "ref_id": "BIBREF30"}], "ref_spans": [], "section": "Introduction"}, {"text": "To this end, in this paper, we propose Document-level Attention for Keyphrase Extraction (DAKE). It initially produces representations for each word that encode the local context from the query sentence using BiLSTM, then uses a document-level attention mechanism to incorporate the most relevant information from each supporting information with respect to the local context, and employs a gating mechanism to filter out the irrelevant information. Finally, it uses a CRF layer which captures output label dependencies to decode the gated local and the document-level contextual representations to predict the label. The main contributions of this paper are as follows:", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "-We propose DAKE, a BiLSTM-CRF model augmented with document-level attention and a gating mechanism for improved keyword extraction from research papers. -Experimental results on a dataset of research papers show that DAKE outperforms previous state-of-the-art approaches.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "We formally describe the keyphrase extraction task as follows: Given a sentence, s = {w 1 , w 2 , . . . , w n } where n is the length of the sentence, predict the labels sequence y = {y 1 , y 2 , . . . , y n } where y i is the label corresponding to word w i and it can KP (keyphrase word) or Not-KP (not a keyphrase word). Every longest sequence of KP words in a sentence is a keyphrase.", "cite_spans": [], "ref_spans": [], "section": "Problem Formulation"}, {"text": "The main components in our proposed architecture, DAKE, are: Word Embedding Layer, Sentence Encoding Layer, Document-level Attention mechanism, Gating mechanism, Context Augmenting Layer and Label Sequence Prediction Layer. The first layer produces word embeddings of the sentence from which the second layer generates word representations that encode the local context from the query sentence. Then the document-level attention mechanism extracts supporting information from other sentences in the document to enrich the current word representation. Subsequently, we utilize a gating mechanism to filter out the irrelevant information from each word representation. The next layer fuses the local and the global contexts into each word representation. Finally, we feed these word representations into the CRF layer which acts as a decoder to predict the label, KP or Not-KP, associated with each word. The model is trained in an end-to-end fashion.", "cite_spans": [], "ref_spans": [], "section": "Proposed Method"}, {"text": ". . , w in } is a sequence of n words, we transform each word w ij in the sentence s i into a vector x ij using pre-trained word embeddings.", "cite_spans": [], "ref_spans": [], "section": "Word Embedding Layer"}, {"text": "We use a BiLSTM [11] to obtain the hidden representation H i of the sentence s i . A BiLSTM comprises a forward-LSTM which reads the input sequence in the original direction and a backward-LSTM which reads it in the opposite direction. We apply forward-LSTM on the sentence", "cite_spans": [{"start": 16, "end": 20, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Sentence Encoding Layer"}, {"text": "We concatenate the outputs of the forward and the backward LSTMs to obtain the local contextual representation", "cite_spans": [], "ref_spans": [], "section": "Sentence Encoding Layer"}, {"text": "here, : denotes concatenation operation. Succinctly,", "cite_spans": [], "ref_spans": [], "section": "Sentence Encoding Layer"}, {"text": "Many keyphrase mentions are tagged incorrectly in current approaches including the BiLSTM-CRF model [1] due to ambiguous contexts present in the input sentence. In cases where a sentence is short or highly ambiguous, the model may either fail to identify keyphrases due to insufficient information or make wrong predictions by using noisy context. We hypothesize that this limitation can be alleviated using additional supporting information from other sentences within the same document. To extract this global context, we need vector representations of other sentences in the same document D. We utilize BERT [5] as a sentence encoder to obtain representations for the sentences in D. Given an input sentence s l in D, we extract the final hidden state of the [CLS] token as the representation h l of the sentence, where [CLS] is the special classification embedding in BERT. Then, for each word, w ij in the input sentence s i , we apply an attention mechanism to weight the supporting sentences in D as follows", "cite_spans": [{"start": 100, "end": 103, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 611, "end": 614, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Document-Level Attention"}, {"text": "where W 1 , W 2 are trainable weight matrices and b 1 is a trainable bias vector. We compute the final representation of supporting information ash ij = m l=1 \u03b1 l ij h l . For each word w ij ,h ij captures the document-level supporting evidence with regard to w ij .", "cite_spans": [], "ref_spans": [], "section": "Document-Level Attention"}, {"text": "Though the above supporting information from the entire document is valuable to the prediction, we must mitigate the influence of the distant supporting information as the prediction should be made primarily based on the local context. Therefore, we apply a gating mechanism to constrain this influence and enable the model to decide the amount of the supporting information that should be incorporated in the model, which is given as follows:", "cite_spans": [], "ref_spans": [], "section": "Gating Mechanism"}, {"text": "where denotes Hadamard product and W 3 ,W 4 ,W 5 ,W 6 ,W 7 ,W 8 are trainable weight matrices and b 2 ,b 3 ,b 4 are trainable bias vectors. d ij is the representation for the gated supporting evidence for w ij .", "cite_spans": [], "ref_spans": [], "section": "Gating Mechanism"}, {"text": "For each word w ij of sentence s i , we concatenate its local contextual representation h ij and gated document-level supporting contextual representation d ij to obtain its final representation a ij = [h ij : d ij ], where : denotes concatenation operation. These final representations A i = {a i1 , a i2 , . . . , a in } of sentence s i are fed to another BiLSTM to further encode the local contextual features along with supporting contextual information into unified representations", "cite_spans": [], "ref_spans": [], "section": "Context Augmenting Layer"}, {"text": ". The output of this encoding captures the interaction among the context words conditioned on the supporting information. This is different from the initial encoding layer, which captures the interaction among words of the sentence independent of the supporting information.", "cite_spans": [], "ref_spans": [], "section": "Context Augmenting Layer"}, {"text": "The obtained contextual representations C i of query sentence s i are given as input sequence to a CRF layer [16] that produces a probability distribution over the output label sequence using the dependencies among the labels of the entire input sequence. In order to efficiently find the best sequence of labels for an input sentence, the Viterbi algorithm [7] is used.", "cite_spans": [{"start": 109, "end": 113, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 358, "end": 361, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Label Sequence Prediction Layer"}, {"text": "We use the dataset from [19] which comprises metadata of papers from several online digital libraries. The dataset contains metadata for 567,830 papers with a clear split as train, validation, and test sets provided by the authors, as follows: 527,830 were used for model training, 20,000 were used for validation and the rest 20,000 were used for testing. We refer to these sets as kp527k, kp20k-v and kp20k respectively. The metadata of each paper consists of title, abstract, and authorassigned keyphrases. The title and abstract of each paper are used to extract keyphrases, whereas the author-input keyphrases are used as gold-standard for evaluation.", "cite_spans": [{"start": 24, "end": 28, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Dataset"}, {"text": "We compare our approach, DAKE with the following baselines: Bi-LSTM-CRF [1] , CRF [8] , Bi-LSTM [1] , copy-RNN [19] , KEA [29] , Tf-Idf, TextRank [21] and SingleRank [27] . We also carry out an ablation test to understand the effectiveness of document-level attention and gating mechanism components by removing them. Similar to previous works, we evaluate the predictions of each method against the author-specified keyphrases that can be located in the corresponding paper abstracts in the dataset (\"gold standard\"). We present results for all our experiments using the precision, recall, and F1-score measures. For comparison of the methods, we choose the F1-score, which is the harmonic mean of precision and recall.", "cite_spans": [{"start": 72, "end": 75, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 82, "end": 85, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 96, "end": 99, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 111, "end": 115, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 122, "end": 126, "text": "[29]", "ref_id": "BIBREF29"}, {"start": 146, "end": 150, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 166, "end": 170, "text": "[27]", "ref_id": "BIBREF26"}], "ref_spans": [], "section": "Baselines and Evaluation Metrics"}, {"text": "We use pre-trained word embedding vectors obtained using GloVe [23] . We use SciBERT [2] , a BERT model trained on scientific text for the sentence encoder. For word representations, we use 300-dimensional pre-trained word embeddings and for sentence encoder, we use 768 dimensional representation obtained using SciBERT. The hidden state of the LSTM is set to 300 dimensions. The model is trained end-to-end using the Adam optimization method [15] . The learning rate is initially set as 0.001 and decayed by 0.5 after each epoch. For regularization to avoid over-fitting, dropout [26] is applied to each layer. We select the model with the best F1-score on the validation set, kp20k-v. Table 1a shows the results of our approach in comparison to various baselines. Our approach, DAKE outperforms all baselines in terms of the F1-score. Tf-Idf, TextRank and SingleRank are unsupervised extractive approaches while KEA, Bi-LSTM-CRF, CRF, Bi-LSTM follow supervised extractive approach. copyRNN is a recently proposed generative model based on sequence-to-sequence learning along with a copying mechanism. For the unsupervised models and the sequence-to-sequence learning model, we report the performance at top-5 predicted keyphrases since top-5 showed highest performance in the previous works for these models. From Table 1a , we observe that the deep learning-based approaches perform better than the traditional feature-based approaches. This indicates the importance of understanding the semantics of the text for keyphrase extraction. BiLSTM-CRF yields better results in terms of the F1-score over CRF (improvement of F1-score by 18.17% from 17.46% to 35.63%) and BiLSTM (improvement of F1-score by 18.88% from 16.75% to 35.63%) models alone. This result indicates that the combination of BiLSTM, which is powerful in capturing the semantics of the textual content, with CRF, which captures the dependencies among the output labels, helped boost the performance in identifying keyphrases. Our proposed method, DAKE outperforms the BiLSTM-CRF (improvement of F1-score by 6.67% from 35.63% to 42.30%) approach, which indicates that the incorporation of additional contextual information from other sentences in the document into the BiLSTM-CRF model helps to further boost the performance. Table 1b shows the ablation study. We observe that document-level attention increases the F1-score of the baseline BiLSTM-CRF by 0.84% (from 35.63% to 36.47%). This validates our hypothesis that additional supporting information boosts the performance for keyphrase extraction. But leveraging this additional supporting information has a downside of introducing noise into the representations, and to alleviate this, we used a gating mechanism which boosted the F1-score by 1.62% (from 36.47% to 38.09%). Document-level attention did not show great improvement when it has only one layer of BiSLTM because the final tagging predictions mainly depend on the local context of each word while additional context only supplements extra information. Therefore, our model needs another layer of BiLSTM to encode the sequential intermediate vectors containing additional context and local context, as evidenced from our F1-score improvement by 4.21% (from 38.09% to 42.30%). When CRF is removed from DAKE, the F1-score falls by 3.09%, showing that CRF successfully captures the output label dependencies.", "cite_spans": [{"start": 63, "end": 67, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 85, "end": 88, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 444, "end": 448, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 582, "end": 586, "text": "[26]", "ref_id": "BIBREF25"}], "ref_spans": [{"start": 688, "end": 696, "text": "Table 1a", "ref_id": "TABREF0"}, {"start": 1317, "end": 1325, "text": "Table 1a", "ref_id": "TABREF0"}, {"start": 2293, "end": 2301, "text": "Table 1b", "ref_id": "TABREF0"}], "section": "Implementation Details"}, {"text": "We proposed an architecture, DAKE, for keyword extraction from documents. It uses a BiLSTM-CRF network enhanced with a document-level attention mechanism to incorporate contextual information from the entire document, and gating mechanisms to balance between the global and the local contexts. It outperforms existing keyphrase extraction methods on a dataset of research papers. In future, we would like to integrate the relationships between documents such as those available from a citation network by enhancing our approach with contexts in which the document is referenced within a citation network.", "cite_spans": [], "ref_spans": [], "section": "Conclusion and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Bi-LSTM-CRF sequence labeling for keyphrase extraction from scholarly documents", "authors": [{"first": "R", "middle": [], "last": "Alzaidy", "suffix": ""}, {"first": "C", "middle": [], "last": "Caragea", "suffix": ""}, {"first": "C", "middle": ["L"], "last": "Giles", "suffix": ""}], "year": 2019, "venue": "Proceedings of The World Wide Web Conference", "volume": "", "issn": "", "pages": "2551--2557", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Scibert: pretrained contextualized embeddings for scientific text", "authors": [{"first": "I", "middle": [], "last": "Beltagy", "suffix": ""}, {"first": "A", "middle": [], "last": "Cohan", "suffix": ""}, {"first": "K", "middle": [], "last": "Lo", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1903.10676"]}}, "BIBREF2": {"ref_id": "b2", "title": "Opinion expression mining by exploiting keyphrase extraction", "authors": [{"first": "G", "middle": [], "last": "Berend", "suffix": ""}], "year": 2011, "venue": "Proceedings of the 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Citation-enhanced keyphrase extraction from research papers: a supervised approach", "authors": [{"first": "C", "middle": [], "last": "Caragea", "suffix": ""}, {"first": "F", "middle": ["A"], "last": "Bulgarov", "suffix": ""}, {"first": "A", "middle": [], "last": "Godea", "suffix": ""}, {"first": "S", "middle": ["D"], "last": "Gollapalli", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1435--1446", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Bert: pre-training of deep bidirectional transformers for language understanding", "authors": [{"first": "J", "middle": [], "last": "Devlin", "suffix": ""}, {"first": "M", "middle": ["W"], "last": "Chang", "suffix": ""}, {"first": "K", "middle": [], "last": "Lee", "suffix": ""}, {"first": "K", "middle": [], "last": "Toutanova", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1810.04805"]}}, "BIBREF5": {"ref_id": "b5", "title": "PositionRank: an unsupervised approach to keyphrase extraction from scholarly documents", "authors": [{"first": "C", "middle": [], "last": "Florescu", "suffix": ""}, {"first": "C", "middle": [], "last": "Caragea", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "volume": "1", "issn": "", "pages": "1105--1115", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "The Viterbi algorithm", "authors": [{"first": "G", "middle": ["D"], "last": "Forney", "suffix": ""}], "year": 1973, "venue": "Proc. IEEE", "volume": "61", "issn": "3", "pages": "268--278", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Incorporating expert knowledge into keyphrase extraction", "authors": [{"first": "S", "middle": ["D"], "last": "Gollapalli", "suffix": ""}, {"first": "X", "middle": ["L"], "last": "Li", "suffix": ""}, {"first": "P", "middle": [], "last": "Yang", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "CorePhrase: keyphrase extraction for document clustering", "authors": [{"first": "K", "middle": ["M"], "last": "Hammouda", "suffix": ""}, {"first": "D", "middle": ["N"], "last": "Matute", "suffix": ""}, {"first": "M", "middle": ["S"], "last": "Kamel", "suffix": ""}], "year": 2005, "venue": "MLDM 2005", "volume": "3587", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/11510888_26"]}}, "BIBREF9": {"ref_id": "b9", "title": "Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art", "authors": [{"first": "K", "middle": ["S"], "last": "Hasan", "suffix": ""}, {"first": "V", "middle": [], "last": "Ng", "suffix": ""}], "year": 2010, "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters", "volume": "", "issn": "", "pages": "365--373", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Long short-term memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "A study on automatically extracted keywords in text categorization", "authors": [{"first": "A", "middle": [], "last": "Hulth", "suffix": ""}, {"first": "B", "middle": ["B"], "last": "Megyesi", "suffix": ""}], "year": 2006, "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "537--544", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Phrasier: a system for interactive document retrieval using keyphrases", "authors": [{"first": "S", "middle": [], "last": "Jones", "suffix": ""}, {"first": "M", "middle": ["S"], "last": "Staveley", "suffix": ""}], "year": 1999, "venue": "Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "volume": "", "issn": "", "pages": "160--167", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Automatic keyphrase extraction from scientific articles", "authors": [{"first": "S", "middle": ["N"], "last": "Kim", "suffix": ""}, {"first": "O", "middle": [], "last": "Medelyan", "suffix": ""}, {"first": "M", "middle": ["Y"], "last": "Kan", "suffix": ""}, {"first": "T", "middle": [], "last": "Baldwin", "suffix": ""}], "year": 2013, "venue": "Lang. Res. Eval", "volume": "47", "issn": "3", "pages": "723--742", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Adam: a method for stochastic optimization", "authors": [{"first": "D", "middle": ["P"], "last": "Kingma", "suffix": ""}, {"first": "J", "middle": [], "last": "Ba", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}}, "BIBREF15": {"ref_id": "b15", "title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "authors": [{"first": "J", "middle": [], "last": "Lafferty", "suffix": ""}, {"first": "A", "middle": [], "last": "Mccallum", "suffix": ""}, {"first": "F", "middle": ["C"], "last": "Pereira", "suffix": ""}], "year": 2001, "venue": "Proceedings of the 18th International Conference on Machine Learning", "volume": "", "issn": "", "pages": "282--289", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Unsupervised keyphrase extraction: introducing new kinds of words to keyphrases", "authors": [{"first": "T", "middle": ["T N"], "last": "Le", "suffix": ""}, {"first": "M", "middle": ["L"], "last": "Nguyen", "suffix": ""}, {"first": "A", "middle": [], "last": "Shimazu", "suffix": ""}], "year": 2016, "venue": "AI 2016", "volume": "9992", "issn": "", "pages": "665--671", "other_ids": {"DOI": ["10.1007/978-3-319-50127-7_58"]}}, "BIBREF17": {"ref_id": "b17", "title": "Human-competitive tagging using automatic keyphrase extraction", "authors": [{"first": "O", "middle": [], "last": "Medelyan", "suffix": ""}, {"first": "E", "middle": [], "last": "Frank", "suffix": ""}, {"first": "I", "middle": ["H"], "last": "Witten", "suffix": ""}], "year": 2009, "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing", "volume": "3", "issn": "", "pages": "1318--1327", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Deep keyphrase generation", "authors": [{"first": "R", "middle": [], "last": "Meng", "suffix": ""}, {"first": "S", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "S", "middle": [], "last": "Han", "suffix": ""}, {"first": "D", "middle": [], "last": "He", "suffix": ""}, {"first": "P", "middle": [], "last": "Brusilovsky", "suffix": ""}, {"first": "Y", "middle": [], "last": "Chi", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1704.06879"]}}, "BIBREF19": {"ref_id": "b19", "title": "Document-level neural machine translation with hierarchical attention networks", "authors": [{"first": "L", "middle": [], "last": "Miculicich", "suffix": ""}, {"first": "D", "middle": [], "last": "Ram", "suffix": ""}, {"first": "N", "middle": [], "last": "Pappas", "suffix": ""}, {"first": "J", "middle": [], "last": "Henderson", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1809.01576"]}}, "BIBREF20": {"ref_id": "b20", "title": "Textrank: bringing order into text", "authors": [{"first": "R", "middle": [], "last": "Mihalcea", "suffix": ""}, {"first": "P", "middle": [], "last": "Tarau", "suffix": ""}], "year": 2004, "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "404--411", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Keyphrase extraction in scientific publications", "authors": [{"first": "T", "middle": ["D"], "last": "Nguyen", "suffix": ""}, {"first": "M.-Y", "middle": [], "last": "Kan", "suffix": ""}], "year": 2007, "venue": "ICADL 2007", "volume": "4822", "issn": "", "pages": "317--326", "other_ids": {"DOI": ["10.1007/978-3-540-77094-7_41"]}}, "BIBREF22": {"ref_id": "b22", "title": "GloVe: global vectors for word representation", "authors": [{"first": "J", "middle": [], "last": "Pennington", "suffix": ""}, {"first": "R", "middle": [], "last": "Socher", "suffix": ""}, {"first": "C", "middle": [], "last": "Manning", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1532--1543", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Citation summarization through keyphrase extraction", "authors": [{"first": "V", "middle": [], "last": "Qazvinian", "suffix": ""}, {"first": "D", "middle": ["R"], "last": "Radev", "suffix": ""}, {"first": "A", "middle": [], "last": "Ozgur", "suffix": ""}], "year": 2010, "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010)", "volume": "", "issn": "", "pages": "895--903", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Enhancing access to scholarly publications with surrogate resources", "authors": [{"first": "D", "middle": ["K"], "last": "Sanyal", "suffix": ""}, {"first": "P", "middle": ["K"], "last": "Bhowmick", "suffix": ""}, {"first": "P", "middle": ["P"], "last": "Das", "suffix": ""}, {"first": "S", "middle": [], "last": "Chattopadhyay", "suffix": ""}, {"first": "T", "middle": ["Y S S"], "last": "Santosh", "suffix": ""}], "year": 2019, "venue": "Scientometrics", "volume": "121", "issn": "2", "pages": "1129--1164", "other_ids": {"DOI": ["10.1007/s11192-019-03227-4"]}}, "BIBREF25": {"ref_id": "b25", "title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": [{"first": "N", "middle": [], "last": "Srivastava", "suffix": ""}, {"first": "G", "middle": [], "last": "Hinton", "suffix": ""}, {"first": "A", "middle": [], "last": "Krizhevsky", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "R", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": 2014, "venue": "J. Mach. Learn. Res", "volume": "15", "issn": "1", "pages": "1929--1958", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "Single document keyphrase extraction using neighborhood knowledge", "authors": [{"first": "X", "middle": [], "last": "Wan", "suffix": ""}, {"first": "J", "middle": [], "last": "Xiao", "suffix": ""}], "year": 2008, "venue": "", "volume": "8", "issn": "", "pages": "855--860", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "PTR: phrase-based topical ranking for automatic keyphrase extraction in scientific publications", "authors": [{"first": "M", "middle": [], "last": "Wang", "suffix": ""}, {"first": "B", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Y", "middle": [], "last": "Huang", "suffix": ""}, {"first": "A", "middle": [], "last": "Hirose", "suffix": ""}, {"first": "S", "middle": [], "last": "Ozawa", "suffix": ""}, {"first": "K", "middle": [], "last": "Doya", "suffix": ""}, {"first": "K", "middle": [], "last": "Ikeda", "suffix": ""}, {"first": "M", "middle": [], "last": "Lee", "suffix": ""}], "year": null, "venue": "ICONIP 2016", "volume": "9950", "issn": "", "pages": "120--128", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "KEA: practical automated keyphrase extraction", "authors": [{"first": "I", "middle": ["H"], "last": "Witten", "suffix": ""}, {"first": "G", "middle": ["W"], "last": "Paynter", "suffix": ""}, {"first": "E", "middle": [], "last": "Frank", "suffix": ""}, {"first": "C", "middle": [], "last": "Gutwin", "suffix": ""}, {"first": "C", "middle": ["G"], "last": "Nevill-Manning", "suffix": ""}], "year": 2005, "venue": "Design and Usability of Digital Libraries: Case Studies in the Asia Pacific", "volume": "", "issn": "", "pages": "129--152", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "Global attention for name tagging", "authors": [{"first": "B", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "S", "middle": [], "last": "Whitehead", "suffix": ""}, {"first": "L", "middle": [], "last": "Huang", "suffix": ""}, {"first": "H", "middle": [], "last": "Ji", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning", "volume": "", "issn": "", "pages": "86--96", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "World wide web site summarization. Web Intell. Agent Syst", "authors": [{"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "N", "middle": [], "last": "Zincir-Heywood", "suffix": ""}, {"first": "E", "middle": [], "last": "Milios", "suffix": ""}], "year": 2004, "venue": "Int. J", "volume": "2", "issn": "1", "pages": "39--53", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "[..]", "latex": null, "type": "figure"}, "FIGREF1": {"text": "An example of keyphrase extraction with author-specified keyphrases highlighted in bold.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Performance analysis of DAKE (a) Performance of different keyphrase extraction algorithms. (b) Ablation Study: BiLSTM-CRF used as baseline.", "latex": null, "type": "table"}}, "back_matter": [{"text": "Project sponsored by Ministry of Human Resource Development, Government of India at IIT Kharagpur.", "cite_spans": [], "ref_spans": [], "section": "Acknowledgements. This work is supported by National Digital Library of India"}]}