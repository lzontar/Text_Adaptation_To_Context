{"paper_id": "1f99ae34fd23e4a4155ad00b86806884937fd585", "metadata": {"title": "Quality-Aware Streaming Network Embedding with Memory Refreshing", "authors": [{"first": "Hsi-Wen", "middle": [], "last": "Chen", "suffix": "", "affiliation": {"laboratory": "", "institution": "National Taiwan University", "location": {"settlement": "Taipei", "country": "Taiwan"}}, "email": ""}, {"first": "Hong-Han", "middle": [], "last": "Shuai", "suffix": "", "affiliation": {"laboratory": "", "institution": "National Chiao Tung University", "location": {"settlement": "Hsinchu", "country": "Taiwan"}}, "email": "hhshuai@nctu.edu.tw"}, {"first": "Sheng-De", "middle": [], "last": "Wang", "suffix": "", "affiliation": {"laboratory": "", "institution": "National Taiwan University", "location": {"settlement": "Taipei", "country": "Taiwan"}}, "email": "sdwang@ntu.edu.tw"}, {"first": "De-Nian", "middle": [], "last": "Yang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Academia Sinica", "location": {"settlement": "Taipei", "country": "Taiwan"}}, "email": "dnyang@iis.sinica.edu.tw"}]}, "abstract": [{"text": "Static network embedding has been widely studied to convert sparse structure information into a dense latent space. However, the majority of real networks are continuously evolving, and deriving the whole embedding for every snapshot is computationally intensive. To avoid recomputing the embedding over time, we explore streaming network embedding for two reasons: 1) to efficiently identify the nodes required to update the embeddings under multi-type network changes, and 2) to carefully revise the embeddings to maintain transduction over different parts of the network. Specifically, we propose a new representation learning framework, named Graph Memory Refreshing (GMR), to preserve both global types of structural information efficiently. We prove that GMR maintains the consistency of embeddings (crucial for network analysis) for isomorphic structures better than existing approaches. Experimental results demonstrate that GMR outperforms the baselines with much smaller time.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Low-dimensional vector representation of nodes in large-scale networks has been widely applied to a variety of domains, such as social media [13] , molecular structure [7] , and transportation [9] . Previous approaches, e.g., DeepWalk [13] , LINE [16] , and SDNE [20] , are designed to reduce the sparse structure information to a dense latent space for node classification [13] , link prediction [16] , and network visualization [21] . However, the above embedding schemes were not designed for evolutionary networks. Current popular networks tend to evolve with time, e.g., the average number of friends increases from 155 in 2016 and to 338 in 2018 [8] . Ephemeral social networks, like Snapchat for short-term conversations, may disappear within weeks. However, retraining the whole embedding for each snapshot is computationally intensive for a massive network. Therefore, streaming network embedding is a desirable option to quickly update and generate new embeddings in a minimum amount of time.", "cite_spans": [{"start": 141, "end": 145, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 168, "end": 171, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 193, "end": 196, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 235, "end": 239, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 247, "end": 251, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 263, "end": 267, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 374, "end": 378, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 397, "end": 401, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 430, "end": 434, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 652, "end": 655, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "Different from dynamic network embeddings [12, 21] that analyze a sequence of networks to capture the temporal patterns, streaming network embedding 1 aims to update the network embedding from the changed part of the network to find the new embedding. Efficient streaming network embedding has the following four main challenges. 1) Multi-type change. Dynamic changes of networks with insertions and deletions of nodes and edges are usually frequent and complex. It is thus important to derive the new embedding in minimum time to timely reflect the new network status. 2) Evaluation of affected nodes. Updating the embeddings of only the nodes neighboring to the changed part ignores the ripple effect on the remaining nodes. It is crucial to identify the nodes required to update the embeddings and ensure that the nodes with similar structures share similar embeddings. 3) Transduction. When a network significantly changes, it is difficult to keep the local proximity between the changed part and the remaining part of the network. It is also important to reflect the change in the global structure. 4) Quality guarantee. For streaming embeddings based on neural networks (usually regarded as a black box), it is challenging to provide theoretical guarantees about the embedding quality.", "cite_spans": [{"start": 42, "end": 46, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 47, "end": 50, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Introduction"}, {"text": "To effectively address the above challenges, this paper proposes a new representation learning approach, named Graph Memory Refreshing (GMR) . GMR first derives the new embedding of the changed part by decomposing the loss function of Skip-Gram to support multi-type changes. It carefully evaluates the ripple-effect area and ensures the correctness by proposing a globally structureaware selecting strategy, named hierarchical addressing, to efficiently identify and update those affected nodes with beam search to avoid the overfitting problem. To effectively support streaming data, our idea is to interpret the update of embeddings as the memory networks with two controllers, a refreshing gate and percolation gate, to tailor the embeddings from the structural aspect and maintain the transduction. GMR then updates the embeddings according to the streaming information of the new network and the stored features (i.e., memory) of the current network to avoid recomputing the embedding of the whole network. Moreover, GMR aims to both preserve the global structural information and maintain the embeddings of isomorphic structures, i.e., ensuring that the nodes with similar local structures share similar embeddings. This property is essential to ensure the correctness of network analysis based on network embeddings [18] . We theoretically prove that GMR preserves the consistency of embeddings for isomorphic structures better than that of the existing approaches. The contributions of this paper are summarized as follows.", "cite_spans": [{"start": 135, "end": 140, "text": "(GMR)", "ref_id": null}, {"start": 1324, "end": 1328, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Introduction"}, {"text": "-GMR explores streaming network embedding with quality guarantees. The hierarchical addressing, refreshing gate, and percolation gate efficiently find and update the affected nodes under multi-type changes. -We prove that GMR embedding preserves isomorphic structures better than the existing approaches. According to our literature review, this is the first theoretical analysis for streaming network embedding. -Experimental results show that GMR outperforms the baselines by at least 10.5% for link prediction and node classification with a much shorter time.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Static network embedding has attracted a wide range of attention. Laplacian Eigenmaps [1] and IsoMaps [17] first constructed the adjacency matrix and then solved the matrix factorization, but the adjacency matrix was not scalable for massive networks. After Skip-Gram [11] was demonstrated to be powerful for representation learning, DeepWalk [13] and node2vec [5] employed random walks to learn network embedding, while LINE [16] and SDNE [20] were able to preserve the first-order and second-order proximity. GraphSAGE [6] and GAT [19] generated node representations in an inductive manner, by mapping and aggregating node features from the neighborhood. In addition, a recent line of research proposed to learn the embeddings from a sequence of networks over time for finding temporal behaviors [12, 21] . However, these approaches focused on capturing the temporal changes rather than the efficiency since they recomputed the embeddings of the whole network, instead of updating only the changed part. Another line of recent research studied the dynamic embedding without retraining. However, the SVD-based approach [22] was more difficult to support large-scale networks according to [5] . Besides, [10] only supported the edge insertion and ignored edge deletion, whereas the consistency of the embeddings for globally isomorphic structures was not ensured. Compared with the above research and [3] , the proposed GMR is the only one that provides a theoretical guarantee on the embedding quality (detailed later). It also more accurately preserves both the global structural information and the consistency of the embeddings.", "cite_spans": [{"start": 86, "end": 89, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 102, "end": 106, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 268, "end": 272, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 343, "end": 347, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 361, "end": 364, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 426, "end": 430, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 440, "end": 444, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 521, "end": 524, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 533, "end": 537, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 798, "end": 802, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 803, "end": 806, "text": "21]", "ref_id": "BIBREF20"}, {"start": 1120, "end": 1124, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 1189, "end": 1192, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 1204, "end": 1208, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 1401, "end": 1404, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Related Work"}, {"text": "In this section, we present the definitions for streaming network embeddings.", "cite_spans": [], "ref_spans": [], "section": "Problem Formulation"}, {"text": "A dynamic network G is a sequence of networks G = {G 1 , \u00b7 \u00b7 \u00b7 , G T } over time, where G t = (V t , E t ) is the network snapshot at timestamp t. \u0394G t = (\u0394V t , \u0394E t ) represents the streaming network with the changed part \u0394V t and \u0394E t as the sets of vertices and edges inserted or deleted between t and t + 1.", "cite_spans": [], "ref_spans": [], "section": "Definition 1 (Streaming Networks)."}, {"text": "Let z i,t denote the streaming network embedding that preserves the structural property of v i \u2208 G t at timestamp t. The streaming network embeddings are derived by \u03a6 s = (\u03c6 s 1 , \u00b7 \u00b7 \u00b7 , \u03c6 s t+1 , \u00b7 \u00b7 \u00b7 , \u03c6 s T ), where \u03c6 s t+1 updates the node embedding z i,t+1 at timestamp t + 1 according to z t and \u0394G t , i.e.,", "cite_spans": [], "ref_spans": [], "section": "Definition 2 (Streaming Network Embeddings)."}, {"text": "In other words, the inputs of the streaming network function are the embedding in the current time and the changed part of the network. In contrast, for [12, 21] , given a dynamic network G, the embedding is derived by a sequence of functions \u03a6 = (\u03c6 1 , \u00b7 \u00b7 \u00b7 , \u03c6 t+1 , \u00b7 \u00b7 \u00b7 , \u03c6 T ), where \u03c6 t+1 maps the node v i to the d-dimensional embedding z i,t+1 at timestamp t + 1, i.e., z i,t+1 = \u03c6 t+1 (v i , G t+1 ). Therefore, the inputs are the whole networks in the current and next time. In the following, we present the problem studied in this paper.", "cite_spans": [{"start": 153, "end": 157, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 158, "end": 161, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Definition 2 (Streaming Network Embeddings)."}, {"text": "Given a streaming network with \u0394V t and \u0394E t as the sets of the vertices and edges inserted or deleted between t and t+1, the goal is to find the streaming network embedding and derive the corresponding embedding quality to ensure that the nodes with similar structures share similar embeddings.", "cite_spans": [], "ref_spans": [], "section": "Definition 3 (Quality-aware Multi-type Streaming Network Embeddings)."}, {"text": "Later in Sect. 5, we formally present and theoretically analyze the quality of the embedding with a new metric, named isomorphic retaining score. Moreover, we prove that the proposed GMR better preserves the structures than other state-of-the-art methods in Theorems 1.", "cite_spans": [], "ref_spans": [], "section": "Definition 3 (Quality-aware Multi-type Streaming Network Embeddings)."}, {"text": "In this section, we propose Graph Memory Refreshing (GMR) to support multitype embedding updates, to identify the affected nodes required to update the embeddings by hierarchical addressing, and to ensure that the nodes with similar structures share similar embeddings. To effectively support streaming data, we leverage the controllers (refreshing and percolation gates) of memory networks [4] to refresh the memory (update the embedding) according to the current state (the current embedding) and new input (streaming network).", "cite_spans": [{"start": 391, "end": 394, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Graph Memory Refreshing"}, {"text": "For each node v i , the Skip-Gram model predicts the context nodes v j \u2208 N (v i ) and maximizes the log probability,", "cite_spans": [], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "However, it is computationally intensive to derive the above probabilities for all nodes. Therefore, the probabilities are approximated by negative sampling [11] ,", "cite_spans": [{"start": 157, "end": 161, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "where \u03c3(x) = 1/(1 + e \u2212x ) is the sigmoid function, z i and z j are respectively the embedding vectors of v i and v j , and P N (v i ) is the noise distribution for negative sampling. The two terms respectively model the observed neighborhoods and the negative samples (i.e., node pairs without an edge) drawn from distribution P N (v i ). However, Eq. (4.2) focuses on only the edge insertion. To support the edge deletion, the second part in Eq. (4.2) is revised to consider unpaired negative samples and the deletion as follows,", "cite_spans": [], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "where D is the set of deleted edges, and \u03b1 is required to be set greater than 1 because the samples from D usually provide more information than the unpaired negative samples P (v i ). 2 Note that node deletion is handled by removing all incident edges of a node, while adding a node with new edges is regarded as the edge insertion. 3", "cite_spans": [{"start": 185, "end": 186, "text": "2", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "(a) Construction of the addressing tree, t = 1.", "cite_spans": [], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "(b) Searching of the most affected nodes for v4 on the addressing tree, t = 2. ", "cite_spans": [], "ref_spans": [], "section": "Multi-type Embedding Updating"}, {"text": "For streaming network embedding, previous computationally intensive approaches [4] find the embeddings of all nodes by global addressing. A more efficient way is updating only the neighboring nodes of the changed part with local addressing [10] . However, the ripple-effect area usually has an arbitrary shape (i.e., including not only the neighboring nodes). Therefore, instead of extracting the neighboring nodes with heuristics, hierarchical addressing systematically transforms the original network into a search tree that is aware of the global structure for the efficient identification of the affected nodes to update their embeddings. Hierarchical addressing has the following advantages: 1) Efficient. It can be regarded as a series of binary classifications (on a tree), whereas global addressing and local addressing belong to multi-class classification (on the candidate list). Therefore, the time complexity to consider each node in \u0394V t is reduced from", "cite_spans": [{"start": 79, "end": 82, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 240, "end": 244, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Hierarchical Addressing"}, {"text": "where k is the number of search beams (explained later). 2) Topology-aware. It carefully examines the graph structure to evaluate the proximity and maintain the isomorphic structure, i.e., ensuring that the nodes with similar structures share similar embeddings. This property is essential for the correctness of network analysis with network embeddings [18] .", "cite_spans": [{"start": 354, "end": 358, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Hierarchical Addressing"}, {"text": "Specifically, hierarchical addressing first exploits graph coarsening to build an addressing tree for the efficient search of the affected nodes. Graph coarsening includes both first-hop and second-hop collapsing: first-hop collapsing preserves the first-order proximity by merging two adjacent nodes into a supernode; second-hop collapsing aggregates the nodes with a common neighbor into a supernode, where the embedding of the supernode is averaged from its child nodes [2] . Second-hop collapsing is prioritized because it can effectively compress the network into a smaller tree.", "cite_spans": [{"start": 473, "end": 476, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Hierarchical Addressing"}, {"text": "The network is accordingly transformed into an addressing tree with each node v \u2208 V t as a leaf node. Afterward, for each node v i \u2208 \u0394V t , we search for the node v j \u2208 V t sharing the highest similarity with v i as the first affected node for v i by comparing their cosine similarity [4] along the addressing tree. For each node in the tree, if the left child node shares a greater similarity to v i , the search continues on the left subtree; otherwise, it searches the right subtree. The similarity search ends when it reaches the leaf node with the highest similarity to v i , and any node in V t (not only the neighbors of v i ) is thereby allowed to be extracted. In other words, hierarchical addressing enables GMR to extract the affected nodes located in different locations of the network (not necessary to be close to v i ), whereas previous approaches [3, 10, 21] update only the neighboring nodes of v i . Afterward, hierarchical addressing extracts the top-1 result for all nodes in \u0394V t as the initially affected nodes (more will be included later), where the nodes with the similarity smaller than a threshold h are filtered. To prevent over-fitting in a local minimum, hierarchical addressing can also extract the top-k results at each iteration with the beam search. 4 Figure 1 presents an example of hierarchical addressing with the dimension of embeddings as 2. At timestamp t = 1 ( Fig. 1(a) ), we construct the addressing tree by first merging nodes v 1 and v 2 into supernode u 12 through second-hop collapsing. The embedding of u 12 is 0.5 \u00b7 (0.4, 0.4) + 0.5 \u00b7 (0.2, 0.8) = (0.3, 0.6). Afterward, v 3 merges u 12 into u 123 through first-hop collapsing, and u 123 is the root of the tree. At t = 2 ( Fig. 1(b) ), if a new node v 4 is linked to v 1 with the embedding as (0.3, 0.2), we identify the affected nodes with bream search (k = 2) and start from the root u 123 . First, we insert v 3 and u 12 into the search queue with the size as 2 since k = 2, to compare the similarity of v 4 with that of v 3 and u 12 . Both u 12 and v 3 are then popped out from the queue because v 1 and v 2 have higher similarity i.e., the top-2 results (0.78 and 0.98), compared with 0.73 for v 3 .", "cite_spans": [{"start": 285, "end": 288, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 863, "end": 866, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 867, "end": 870, "text": "10,", "ref_id": "BIBREF9"}, {"start": 871, "end": 874, "text": "21]", "ref_id": "BIBREF20"}, {"start": 2046, "end": 2048, "text": "12", "ref_id": "BIBREF11"}], "ref_spans": [{"start": 1286, "end": 1294, "text": "Figure 1", "ref_id": "FIGREF0"}, {"start": 1402, "end": 1411, "text": "Fig. 1(a)", "ref_id": "FIGREF0"}, {"start": 1723, "end": 1732, "text": "Fig. 1(b)", "ref_id": "FIGREF0"}], "section": "Hierarchical Addressing"}, {"text": "After identifying the nodes required to update the embeddings by hierarchical addressing, a simple approach is to update the embeddings of those affected nodes with a constant shift [6, 20] . However, a streaming network with a topology change on only a subset of nodes usually leads to different shifts for the nodes in distinct locations. Moreover, updating only the nodes extracted from hierarchical addressing is insufficient to ensure consistency of embeddings for the nodes with similar structures when the embeddings are tailored independently.", "cite_spans": [{"start": 182, "end": 185, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 186, "end": 189, "text": "20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Refresh and Percolate"}, {"text": "To effectively support streaming data, inspired by the gating mechanism in GRU [4] , we parameterize the update of the embedding according to the current embedding and incoming streaming network. Specifically, GMR decomposes the update procedure into two controller gates: a refreshing gate g r and percolation gate g p . For each node v j selected in hierarchical addressing for each v i \u2208 \u0394V t , the refreshing gate first updates the embedding of v j according the new embedding of v i , and the percolation gate then updates the embedding for every neighbor v k of v j from the new embedding of v j . The refreshing gate quantifies the embedding update for v j from an incoming stream (i.e., one-toone update), while the percolation gate transduces the embedding of v j to its neighborhoods (i.e., one-to-many update) to preserve better local structure. The two gates are the cornerstones to maintain isomorphic structure, as proved later in the Theorem 1.", "cite_spans": [{"start": 79, "end": 82, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Refresh and Percolate"}, {"text": "To update the embeddings of v j , i.e., updating z j,t+1 from z j,t , we first define a shared function a r to find the refreshing coefficient \u03c1 r , which represents the correlation between the embedding of v j and the new embedding of v i , i.e., \u03c1 r = a r (z i,t+1 , z j,t ). The refreshing gate selects the correlation function [19] as the shared function a r to extract the residual relation [19] between the two embeddings, instead of directly adopting a constant shift as was done in previous work. Here a r \u2208 R 2d is a shift projection, and \u03c1 r is derived by a r T [z i,t+1 ||z j,t ], where || is the vector concatenation operation. After this, we regulate refreshing coefficient \u03c1 r into [0, 1] by a sigmoid function g r = \u03c3(\u03c1 r ) to provide a non-linear transformation. Therefore, g r quantifies the extent that z i,t+1 affects z j,t ,", "cite_spans": [{"start": 331, "end": 335, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 396, "end": 400, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Refresh and Percolate"}, {"text": "Thereafter, the percolation gate revises the embedding of the neighbor nodes of v j to ensure the consistency of the embeddings for the nodes with similar structures. The percolation gate learns another sharable vector a p \u2208 R 2d and finds the percolation coefficient \u03c1 p = a p T [z j,t+1 ||z k,t ], to quantify the extent that v j affects v k . Similarly, we regulate \u03c1 p by g p = \u03c3(\u03c1 p ) to update z k,t as follows,", "cite_spans": [], "ref_spans": [], "section": "Refresh and Percolate"}, {"text": "(4.5)", "cite_spans": [], "ref_spans": [], "section": "Refresh and Percolate"}, {"text": "Therefore, when the refreshing and percolation gates are 0, the streaming network is ignored. In contrast, when both gates become 1, the previous snapshot embedding is dropped accordingly. In summary, the refreshing and percolation gates act as decision makers to learn the impact of the streaming network on different nodes. For the percolation gate, when node v j is updated, the percolation gate tailors the embedding of each v k \u2208 N 1 (v j ), 5 by evaluating the similarity of v j and v k according to the embeddings z k and z j . If v j and v k share many common neighbors, the percolation value of (v j , v k ) will increase to draw z k and z j closer to each other. The idea is similar for the refreshing gate. Note that a r and a p are both differentiable and can be trained in an unsupervised setting by maximize the objective Eq. (4.3). The unsupervised loss can also be replaced or augmented by a task-oriented objective (e.g., cross-entropy loss) when labels are provided. We alternatively update the embeddings (i.e., z i,t and z j,t ) and the correlation parameters (i.e., a r and a p ) to achieve better convergence. Figure 2 illustrates an example of updating the node v 3 . After the embedding of v 3 updated from (0.8, 0.1) to (0.9, 0.1), GMR uses the percolation gate to transduce the embedding to the neighborhood nodes (i.e., v 1 , v 2 , and v 4 ) to preserve the local structure. Since v 1 shares more common neighbors (v 4 ) with v 3 than v 2 (none), the values of percolation gate for v 1 and v 2 are 0.8 and 0.5, respectively. The embeddings of node v 1 and v 2 become (0.76, 0.16) = 0.2 \u00b7 (0.4, 0.4) + 0.8 \u00b7 (0.9, 0.1) and (0.55, 0.45) = 0.5 \u00b7 (0.2, 0.8) + 0.5 \u00b7 (0.9, 0.1) through the percolation gate from v 3 , respectively. Therefore, relative distance between z 3 \u2212 z 2 and z 3 \u2212 z 1 can be maintained.", "cite_spans": [], "ref_spans": [{"start": 1132, "end": 1140, "text": "Figure 2", "ref_id": "FIGREF1"}], "section": "Refresh and Percolate"}, {"text": "The quality of network embedding can be empirically evaluated from the experiment of network analysis, e.g., link prediction [16] and node classification [13] , since the network embedding algorithm is unsupervised learning without knowing the ground truth. In contrast, when the network analysis task is unknown a priori, it is important to theoretically analyze the quality of network embedding. To achieve this goal, we first define the isomorphic pairs and prove that the embeddings of isomorphic pairs are the same in GMR. This property has been regarded as a very important criterion to evaluate the quality of network embedding [18] , because the nodes with similar structures are necessary to share similar embeddings. Moreover, the experimental results in Sect. 6 manifest that a higher quality leads to better performance on task-oriented metrics. Pair ) . Any two different nodes v i and v j form an isomorphic pair if the sets of their first-hop neighbors N 1 (.) are the same.", "cite_spans": [{"start": 125, "end": 129, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 154, "end": 158, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 635, "end": 639, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [{"start": 858, "end": 864, "text": "Pair )", "ref_id": null}], "section": "Theoretical Analysis"}, {"text": "is also an isomorphic pair.", "cite_spans": [], "ref_spans": [], "section": "Definition 4 (Isomorphic"}, {"text": "Proof: According to Definition 4,", "cite_spans": [], "ref_spans": [], "section": "Definition 4 (Isomorphic"}, {"text": "is also an isomorphic pair.", "cite_spans": [], "ref_spans": [], "section": "Definition 4 (Isomorphic"}, {"text": "Proof: We first prove the sufficient condition. If (v i , v j ) is an isomorphic pair with z i = z j , the probability of v i to predict the context nodes is not to equal to that of v j (Eq. (4.1)). Therefore, there exists a better solution that makes z i and z j be equal, contradicting the condition that the algorithm has converged. For the necessary condition, if z i = z j but (v i , v j ) is not an isomorphic pair, since the probabilities are equal and the algorithm has converged, N (v i ) should be identical to N (v j ) for Eq. (4.1), contradicting that (v i , v j ) is not an isomorphic pair. The lemma follows.", "cite_spans": [], "ref_spans": [], "section": "Lemma 2. The embeddings z i and z j are the same after GMR converges if and only if (v i , v j ) is an isomorphic pair."}, {"text": "As proved in [14] , the network embedding algorithms can be unified into the factorization of the affinity matrix. Therefore, nodes with the same first-hop neighborhood have the same embedding when the decomposition ends.", "cite_spans": [{"start": 13, "end": 17, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Lemma 2. The embeddings z i and z j are the same after GMR converges if and only if (v i , v j ) is an isomorphic pair."}, {"text": "Based on Lemma 2, we define the isomorphic retaining score as follows. Retaining Score) . The isomorphic retaining score, denoted as S t , is the summation of the cosine similarity over every isomorphic pair in G t , S t \u2208 [\u22121, 1]. Specifically,", "cite_spans": [], "ref_spans": [{"start": 71, "end": 87, "text": "Retaining Score)", "ref_id": null}], "section": "Lemma 2. The embeddings z i and z j are the same after GMR converges if and only if (v i , v j ) is an isomorphic pair."}, {"text": "where s ij,t is the cosine similarity between z i,t and z j,t , and \u03be t is the set of isomorphic pairs in G t . In other words, the embeddings of any two nodes v i and v j with the same structure are more consistent to each other if s ij,t is close to 1 [18] . Experiment results in the next section show that higher isomorphic retaining scores lead to better performance of 1) the AUC score for link prediction and 2) the Macro-F1 score for node classification.", "cite_spans": [{"start": 254, "end": 258, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Definition 5 (Isomorphic"}, {"text": "The following theorem proves that GMR retains the isomorphic structure better than other Skip-Gram-based approaches, e.g., [5, 13, 16] , under edge insertion. Afterward, the time complexity analysis is presented.", "cite_spans": [{"start": 123, "end": 126, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 127, "end": 130, "text": "13,", "ref_id": "BIBREF12"}, {"start": 131, "end": 134, "text": "16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Definition 5 (Isomorphic"}, {"text": "Proof: Due to the space constraint, Theorem 1 is proved in the online version. 6 Time Complexity. In GMR, the initialization of the addressing tree involves O(|V 1 |) time. For each t, GMR first updates the embeddings of \u0394V t in O(|\u0394V t | log(|\u0394V t |)) time. After this, hierarchical addressing takes O(k|\u0394V t | log(|V t |)) time to identify the affected nodes. Notice that it requires O(|\u0394V t | log(|V t |) time to update the addressing tree. To update the affected nodes, the refreshing and percolation respectively involve O (1) and O(d max ) time for one affected node, where d max is the maximum node degree of the network. Therefore, updating all the affected nodes requires O(kd max |\u0394V t |). Therefore, the overall time complexity of GMR is O(kd max |\u0394V t | + k|\u0394V t | log(|V t |)), while retraining the whole network requires O(|V t | log(|V t |)) time at each timestamp. Since k is a small constant, d max |V t |, and |\u0394V t | |V t |, GMR is faster than retraining.", "cite_spans": [{"start": 528, "end": 531, "text": "(1)", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Theorem 1. GMR outperforms other Skip-Gram-based models regarding the isomorphic retaining score under edge insertion after each update by gradient descent."}, {"text": "To evaluate the effectiveness and efficiency of GMR, we compare GMR with the state-of-the-art methods on two tasks, i.e., link prediction and node classification. For the baselines, we compare GMR with 1) Full, which updates the whole network with DeepWalk [13] ; 2) change [3] , which only takes the changed part as the samples with DeepWalk; 7 3) GraphSAGE [6] , which derives the embeddings from graph inductive learning; 4) SDNE [20] , which extends the auto-encoder model to generate the embeddings of new nodes from the embeddings of neighbors; 5) CTDNE [12] , which performs the biased random walk on the dynamic network; 8 and 6) DNE [3] , which updates only one affected node; 7) SLA [10] , which handles only node/edge insertion; 8) DHPE [22] , which is an SVD method based on matrix perturbation theory. The default \u03b1, h, k, d, batch size, and learning rate are 1, 0.8, 3, 64, 16, and 0.001, respectively. Stochastic gradient descent (SGD) with Adagrad is adopted to optimize the loss function.", "cite_spans": [{"start": 257, "end": 261, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 274, "end": 277, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 359, "end": 362, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 433, "end": 437, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 560, "end": 564, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 642, "end": 645, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 693, "end": 697, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 748, "end": 752, "text": "[22]", "ref_id": "BIBREF21"}], "ref_spans": [], "section": "Experiments"}, {"text": "For link prediction, three real datasets [15] for streaming networks are evaluated: Facebook (63,731 nodes, 1,269,502 edges, and 736,675 timestamps), Yahoo (100,001 nodes, 3,179,718 edges, and 1,498,868 timestamps), and Epinions (131,828 nodes, 841,372 edges, and 939 timestamps). 9 The concatenated embedding [z i ||z j ] of pair (v i , v j ) is employed as the feature to predict the link by logistic regression. 10 Table 1 reports the AUC [5] , isomorphic retaining score S in Eq. (5.1), and running time of different methods. 11 The results show that the proposed GMR achieves the best AUC among all streaming network embedding algorithms. Compared with other state-of-the-art baselines, GMR outperforms other three baselines in terms of AUC by at least 17.1%, 15.7% and 11.3% on Facebook, Yahoo and Epinions, respectively. Besides, GMR is close to that of Full(1.7% less on Facebook, 0.6% more on Yahoo and 2.2% less on Epinions), but the running time is only 4.7%. Moreover, GraphSAGE has relatively weak performance since it cannot preserve the structural information without node features. The running time of SDNE is 2.1\u00d7 greater than that of GMR due to the processing of the deep structure, while the AUC of SDNE is at least 12.5% less than that of GMR on all datasets. 9 Facebook and Epinions contain both the edge insertion and deletion, represented by \"i j -1 t\" for removing edge (i, j) at timestamp t. Yahoo lacks deletion since it is a message network. 10 For link prediction, at time t, we predict the new edges for time t + 1 (excluding the edges incident to the nodes arriving at time t + 1). 11 For Full, due to high computational complexity in retraining the networks for all timestamps, we partition all timestamps into 50 parts [23] with the network changes aggregated in each part.", "cite_spans": [{"start": 41, "end": 45, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 281, "end": 282, "text": "9", "ref_id": "BIBREF8"}, {"start": 442, "end": 445, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 530, "end": 532, "text": "11", "ref_id": "BIBREF10"}, {"start": 1280, "end": 1281, "text": "9", "ref_id": "BIBREF8"}, {"start": 1751, "end": 1755, "text": "[23]", "ref_id": "BIBREF22"}], "ref_spans": [{"start": 418, "end": 425, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Link Prediction"}, {"text": "Compared to other streaming network embedding methods (e.g., DNE, SLA, and DHPE), GMR achieves at least 10.8% of improvement because the embeddings of other methods are updated without considering the global topology. In contrast, GMR selects the affected nodes by globally structure-aware hierarchical addressing, and the selected nodes are not restricted to the nearby nodes. Furthermore, GMR outperforms baselines regarding the isomorphic retraining score since it percolates the embeddings to preserve the structural information. Note that the isomorphic retaining score S is highly related to the AUC with a correlation coefficient of 0.92, demonstrating that it is indeed crucial to ensure the embedding consistency for the nodes with similar structures.", "cite_spans": [], "ref_spans": [], "section": "Link Prediction"}, {"text": "For node classification, we compare different approaches on BlogCatalog [16] (10,132 nodes, 333,983 edges, and 39 classes), Wiki [5] (2,405 nodes, 17,981 edges, and 19 classes), and DBLP [22] (101,253 nodes, 223,810 edges, 48 timestamps, and 4 classes). DBLP is a real streaming network by extracting the paper citation network of four research areas from 1970 to 2017. BlogCatalog and Wiki are adopted in previous research [3] to generate the streaming networks. 12 The learned embeddings are employed to classify the nodes according to the labels. Cross-entropy is adopted in the loss function for classification with logistic regression. We randomly sample 20% of labels for training and 80% of labels for testing, and the average results from 50 runs are reported. 13 Table 2 demonstrates that GMR outperforms Change by 27.1% regarding Macro-F1 [13] , and it is close to Full but with 20.7\u00d7 speed-up. The Macro-F1 scores of GraphSAGE and SDNE are at least 40% worse than that of GMR, indicating that GraphSAGE and SDNE cannot adequately handle multi-type changes in dynamic networks. Moreover, GMR achieves better improvement on BlogCatalog than on DBLP, because the density (i.e., the average degree) of BlogCatalog is larger, enabling hierarchical addressing of GMR to exploit more structural information for updating multiple nodes. For DBLP, GMR also achieves the performance close to Full. It is worth noting that the isomorphic retaining score S is also positively related to Macro-F1. We further investigate the percentages of isomorphic pairs with the same label on different datasets. The results manifest that 88%, 92% and 97% of isomorphic pairs share the same labels on BlogCatalog, Wiki, and DBLP, respectively. Therefore, it is crucial to maintain the consistency between isomorphic pairs since similar embeddings of isomorphic pairs are inclined to be classified with the same labels.", "cite_spans": [{"start": 72, "end": 76, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 129, "end": 132, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 187, "end": 191, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 424, "end": 427, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 464, "end": 466, "text": "12", "ref_id": "BIBREF11"}, {"start": 769, "end": 771, "text": "13", "ref_id": "BIBREF12"}, {"start": 849, "end": 853, "text": "[13]", "ref_id": "BIBREF12"}], "ref_spans": [{"start": 772, "end": 779, "text": "Table 2", "ref_id": "TABREF1"}], "section": "Node Classification"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "authors": [{"first": "M", "middle": [], "last": "Belkin", "suffix": ""}, {"first": "P", "middle": [], "last": "Niyogi", "suffix": ""}], "year": 2002, "venue": "Advances in NIPS", "volume": "", "issn": "", "pages": "585--591", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "HARP: hierarchical representation learning for networks", "authors": [{"first": "H", "middle": [], "last": "Chen", "suffix": ""}, {"first": "B", "middle": [], "last": "Perozzi", "suffix": ""}, {"first": "Y", "middle": [], "last": "Hu", "suffix": ""}, {"first": "S", "middle": [], "last": "Skiena", "suffix": ""}], "year": 2018, "venue": "Thirty-Second AAAI", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Dynamic network embedding: an extended approach for skip-gram based network embedding", "authors": [{"first": "L", "middle": [], "last": "Du", "suffix": ""}, {"first": "Y", "middle": [], "last": "Wang", "suffix": ""}, {"first": "G", "middle": [], "last": "Song", "suffix": ""}, {"first": "Z", "middle": [], "last": "Lu", "suffix": ""}, {"first": "J", "middle": [], "last": "Wang", "suffix": ""}], "year": 2018, "venue": "IJCAI", "volume": "", "issn": "", "pages": "2086--2092", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Neural turing machines", "authors": [{"first": "A", "middle": [], "last": "Graves", "suffix": ""}, {"first": "G", "middle": [], "last": "Wayne", "suffix": ""}, {"first": "I", "middle": [], "last": "Danihelka", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1410.5401"]}}, "BIBREF4": {"ref_id": "b4", "title": "node2vec: scalable feature learning for networks", "authors": [{"first": "A", "middle": [], "last": "Grover", "suffix": ""}, {"first": "J", "middle": [], "last": "Leskovec", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 22nd ACM SIGKDD", "volume": "", "issn": "", "pages": "855--864", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Inductive representation learning on large graphs", "authors": [{"first": "W", "middle": [], "last": "Hamilton", "suffix": ""}, {"first": "Z", "middle": [], "last": "Ying", "suffix": ""}, {"first": "J", "middle": [], "last": "Leskovec", "suffix": ""}], "year": 2017, "venue": "Advances in NIPS", "volume": "", "issn": "", "pages": "1024--1034", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Mol2vec: unsupervised machine learning approach with chemical intuition", "authors": [{"first": "S", "middle": [], "last": "Jaeger", "suffix": ""}, {"first": "S", "middle": [], "last": "Fulle", "suffix": ""}, {"first": "S", "middle": [], "last": "Turk", "suffix": ""}], "year": 2018, "venue": "J. Chem. Inf. Model", "volume": "58", "issn": "1", "pages": "27--35", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "SNAP datasets: Stanford large network dataset collection", "authors": [{"first": "J", "middle": [], "last": "Leskovec", "suffix": ""}, {"first": "A", "middle": [], "last": "Krevl", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Multi-layered network embedding", "authors": [{"first": "J", "middle": [], "last": "Li", "suffix": ""}, {"first": "C", "middle": [], "last": "Chen", "suffix": ""}, {"first": "H", "middle": [], "last": "Tong", "suffix": ""}, {"first": "H", "middle": [], "last": "Liu", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 SIAM ICDM", "volume": "", "issn": "", "pages": "684--692", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Real-time streaming graph embedding through local actions", "authors": [{"first": "X", "middle": [], "last": "Liu", "suffix": ""}, {"first": "P", "middle": ["C"], "last": "Hsieh", "suffix": ""}, {"first": "N", "middle": [], "last": "Duffield", "suffix": ""}, {"first": "R", "middle": [], "last": "Chen", "suffix": ""}, {"first": "M", "middle": [], "last": "Xie", "suffix": ""}, {"first": "X", "middle": [], "last": "Wen", "suffix": ""}], "year": 2019, "venue": "Proceedings of the WWW", "volume": "", "issn": "", "pages": "285--293", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Distributed representations of words and phrases and their compositionality", "authors": [{"first": "T", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "K", "middle": [], "last": "Chen", "suffix": ""}, {"first": "G", "middle": ["S"], "last": "Corrado", "suffix": ""}, {"first": "J", "middle": [], "last": "Dean", "suffix": ""}], "year": 2013, "venue": "Advances in NIPS", "volume": "", "issn": "", "pages": "3111--3119", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Continuoustime dynamic network embeddings", "authors": [{"first": "G", "middle": ["H"], "last": "Nguyen", "suffix": ""}, {"first": "J", "middle": ["B"], "last": "Lee", "suffix": ""}, {"first": "R", "middle": ["A"], "last": "Rossi", "suffix": ""}, {"first": "N", "middle": ["K"], "last": "Ahmed", "suffix": ""}, {"first": "E", "middle": [], "last": "Koh", "suffix": ""}, {"first": "S", "middle": [], "last": "Kim", "suffix": ""}], "year": 2018, "venue": "Proceedings of the WWW", "volume": "", "issn": "", "pages": "969--976", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "DeepWalk: online learning of social representations", "authors": [{"first": "B", "middle": [], "last": "Perozzi", "suffix": ""}, {"first": "R", "middle": [], "last": "Al-Rfou", "suffix": ""}, {"first": "S", "middle": [], "last": "Skiena", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 20th ACM SIGKDD", "volume": "", "issn": "", "pages": "701--710", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Network embedding as matrix factorization: unifying DeepWalk, LINE, PTE, and node2vec", "authors": [{"first": "J", "middle": [], "last": "Qiu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Dong", "suffix": ""}, {"first": "H", "middle": [], "last": "Ma", "suffix": ""}, {"first": "J", "middle": [], "last": "Li", "suffix": ""}, {"first": "K", "middle": [], "last": "Wang", "suffix": ""}, {"first": "J", "middle": [], "last": "Tang", "suffix": ""}], "year": 2018, "venue": "Proceedings of the WSDM", "volume": "", "issn": "", "pages": "459--467", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "The network data repository with interactive graph analytics and visualization", "authors": [{"first": "R", "middle": [], "last": "Rossi", "suffix": ""}, {"first": "N", "middle": [], "last": "Ahmed", "suffix": ""}], "year": 2015, "venue": "Twenty-Ninth AAAI", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "LINE: large-scale information network embedding", "authors": [{"first": "J", "middle": [], "last": "Tang", "suffix": ""}, {"first": "M", "middle": [], "last": "Qu", "suffix": ""}, {"first": "M", "middle": [], "last": "Wang", "suffix": ""}, {"first": "M", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "J", "middle": [], "last": "Yan", "suffix": ""}, {"first": "Q", "middle": [], "last": "Mei", "suffix": ""}], "year": 2015, "venue": "Proceedings of the WWW", "volume": "", "issn": "", "pages": "1067--1077", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "A global geometric framework for nonlinear dimensionality reduction", "authors": [{"first": "J", "middle": ["B"], "last": "Tenenbaum", "suffix": ""}, {"first": "V", "middle": [], "last": "De Silva", "suffix": ""}, {"first": "J", "middle": ["C"], "last": "Langford", "suffix": ""}], "year": 2000, "venue": "Science", "volume": "290", "issn": "5500", "pages": "2319--2323", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "VERSE: versatile graph embeddings from similarity measures", "authors": [{"first": "A", "middle": [], "last": "Tsitsulin", "suffix": ""}, {"first": "D", "middle": [], "last": "Mottin", "suffix": ""}, {"first": "P", "middle": [], "last": "Karras", "suffix": ""}, {"first": "E", "middle": [], "last": "M\u00fcller", "suffix": ""}], "year": 2018, "venue": "Proceedings of the the WWW", "volume": "", "issn": "", "pages": "539--548", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Graph attention networks", "authors": [{"first": "P", "middle": [], "last": "Veli\u010dkovi\u0107", "suffix": ""}, {"first": "G", "middle": [], "last": "Cucurull", "suffix": ""}, {"first": "A", "middle": [], "last": "Casanova", "suffix": ""}, {"first": "A", "middle": [], "last": "Romero", "suffix": ""}, {"first": "P", "middle": [], "last": "Lio", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1710.10903"]}}, "BIBREF19": {"ref_id": "b19", "title": "Structural deep network embedding", "authors": [{"first": "D", "middle": [], "last": "Wang", "suffix": ""}, {"first": "P", "middle": [], "last": "Cui", "suffix": ""}, {"first": "W", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 22nd ACM SIGKDD", "volume": "", "issn": "", "pages": "1225--1234", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Dynamic network embedding by modeling triadic closure process", "authors": [{"first": "L", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Y", "middle": [], "last": "Yang", "suffix": ""}, {"first": "X", "middle": [], "last": "Ren", "suffix": ""}, {"first": "F", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhuang", "suffix": ""}], "year": 2018, "venue": "Thirty-Second AAAI", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "High-order proximity preserved embedding for dynamic networks", "authors": [{"first": "D", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "P", "middle": [], "last": "Cui", "suffix": ""}, {"first": "Z", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "J", "middle": [], "last": "Pei", "suffix": ""}, {"first": "W", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2018, "venue": "Trans. Knowl. Data Eng", "volume": "30", "issn": "", "pages": "2134--2144", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Online learning to rank in stochastic click models", "authors": [{"first": "M", "middle": [], "last": "Zoghi", "suffix": ""}, {"first": "T", "middle": [], "last": "Tunys", "suffix": ""}, {"first": "M", "middle": [], "last": "Ghavamzadeh", "suffix": ""}, {"first": "B", "middle": [], "last": "Kveton", "suffix": ""}, {"first": "C", "middle": [], "last": "Szepesvari", "suffix": ""}, {"first": "Z", "middle": [], "last": "Wen", "suffix": ""}], "year": 2017, "venue": "Proceedings of the ICML", "volume": "", "issn": "", "pages": "4199--4208", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Example of hierarchical addressing.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Example of percolation gate.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "The streaming network G = {G1, ..., GT } is generated from the original network by first sampling half of the original network as G1. For each timestamp t, \u0394Gt is constructed by sampling 200 edges (not in Gt\u22121) from the original network and adding them (and the corresponding terminal nodes) to Gt\u22121, whereas 100 edges of Gt\u22121 are deleted.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Experiment results of link prediction.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Facebook </td><td>\u00a0</td><td>Yahoo </td><td>\u00a0</td><td>\u00a0</td><td>Epinions\n</td><td>\u00a0</td></tr><tr><td>\u00a0</td><td>AUC </td><td>S </td><td>sec </td><td>AUC </td><td>S </td><td>sec </td><td>AUC </td><td>S </td><td>sec\n</td></tr><tr><td>GMR </td><td>0.7943 </td><td>0.94 </td><td>3325 </td><td>0.7674 </td><td>0.93 </td><td>3456 </td><td>0.9294 </td><td>0.92 </td><td>3507\n</td></tr><tr><td>Full </td><td>0.8004 </td><td>0 95 </td><td>66412 </td><td>0.7641 </td><td>0.95 </td><td>72197 </td><td>0.9512 </td><td>0.96 </td><td>61133\n</td></tr><tr><td>Change </td><td>0.6926 </td><td>0.79 </td><td>2488 </td><td>0.6326 </td><td>0.82 </td><td>2721 </td><td>0.8233 </td><td>0.84 </td><td>2429\n</td></tr><tr><td>GraphSAGE </td><td>0.6569 </td><td>0.77 </td><td>4094 </td><td>0.6441 </td><td>0.79 </td><td>5117 </td><td>0.8158 </td><td>0.85 </td><td>4588\n</td></tr><tr><td>SDNE </td><td>0.6712 </td><td>0.81 </td><td>7078 </td><td>0.6585 </td><td>0.83 </td><td>7622 </td><td>0.8456 </td><td>0.88 </td><td>6799\n</td></tr><tr><td>CTDNE </td><td>0.7091 </td><td>0.85 </td><td>4322 </td><td>0.6799 </td><td>0.84 </td><td>5136 </td><td>0.8398 </td><td>0.90 </td><td>5097\n</td></tr><tr><td>DNE </td><td>0.7294 </td><td>0.87 </td><td>2699 </td><td>0.6892 </td><td>0.86 </td><td>2843 </td><td>0.8648 </td><td>0.92 </td><td>2613\n</td></tr><tr><td>SLA </td><td>0.7148 </td><td>0.86 </td><td>2398 </td><td>0.6910 </td><td>0.86 </td><td>2438 </td><td>0.8598 </td><td>0.91 </td><td>2569\n</td></tr><tr><td>DHPE </td><td>0.7350 </td><td>0.88 </td><td>3571 </td><td>0.7102 </td><td>0.88 </td><td>3543 </td><td>0.8458 </td><td>0.90 </td><td>3913\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Experiment results of node classification.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>BlogCatalog </td><td>\u00a0</td><td>Wiki </td><td>\u00a0</td><td>\u00a0</td><td>DBLP\n</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>\u00a0</td><td>F1 </td><td>S </td><td>sec </td><td>F1 </td><td>S </td><td>sec </td><td>F1 </td><td>S </td><td>sec\n</td></tr><tr><td>GMR </td><td>0.2059 </td><td>0.90 </td><td>1998 </td><td>0.4945 </td><td>0.92 </td><td>199 </td><td>0.7619 </td><td>0.93 </td><td>7638\n</td></tr><tr><td>Full </td><td>0.2214 </td><td>0.91 </td><td>37214 </td><td>0.5288 </td><td>0.93 </td><td>3811 </td><td>0.7727 </td><td>0.94 </td><td>149451\n</td></tr><tr><td>Change </td><td>0.1651 </td><td>0.71 </td><td>1237 </td><td>0.3597 </td><td>0.79 </td><td>122 </td><td>0.6841 </td><td>0.86 </td><td>5976\n</td></tr><tr><td>GraphSAGE </td><td>0.1558 </td><td>0.81 </td><td>2494 </td><td>0.3419 </td><td>0.82 </td><td>173 </td><td>0.6766 </td><td>0.86 </td><td>11410\n</td></tr><tr><td>SDNE </td><td>0.1723 </td><td>0.83 </td><td>2795 </td><td>0.3438 </td><td>0.84 </td><td>266 </td><td>0.6914 </td><td>0.87 </td><td>16847\n</td></tr><tr><td>CTDNE </td><td>0.1808 </td><td>0.84 </td><td>2923 </td><td>0.4013 </td><td>0.85 </td><td>301 </td><td>0.7171 </td><td>0.88 </td><td>9115\n</td></tr><tr><td>DNE </td><td>0.1848 </td><td>0.86 </td><td>1547 </td><td>0.4187 </td><td>0.86 </td><td>141 </td><td>0.7302 </td><td>0.90 </td><td>6521\n</td></tr><tr><td>SLA </td><td>0.1899 </td><td>0.87 </td><td>1399 </td><td>0.3998 </td><td>0.85 </td><td>149 </td><td>0.7110 </td><td>0.88 </td><td>6193\n</td></tr><tr><td>DHPE </td><td>0.1877 </td><td>0.87 </td><td>2047 </td><td>0.4204 </td><td>0.86 </td><td>215 </td><td>0.7311 </td><td>0.90 </td><td>8159\n</td></tr></table></body></html>"}}, "back_matter": [{"text": "In this paper, we propose GMR for streaming network embeddings featuring the hierarchical addressing, refreshing gate, and percolation gate to preserve the structural information and consistency. We also prove that the embeddings generated by GMR are more consistent than the current network embedding schemes under insertion. The experiment results demonstrate that GMR outperforms the state-of-the-art methods in link prediction and node classification. Moreover, multi-type updates with the beam search improve GMR in both taskoriented scores and the isomorphic retaining score. Our future work will extend GMR to support multi-relations in knowledge graphs.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}]}