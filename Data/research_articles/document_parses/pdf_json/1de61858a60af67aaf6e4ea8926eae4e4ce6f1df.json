{"paper_id": "1de61858a60af67aaf6e4ea8926eae4e4ce6f1df", "metadata": {"title": "A Comparison of Evolutionary Approaches to the Shortest Common Supersequence Problem", "authors": [{"first": "Carlos", "middle": [], "last": "Cotta", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of M\u00e1laga", "location": {"addrLine": "Campus de Teatinos", "postCode": "29071", "settlement": "M\u00e1laga", "country": "Spain"}}, "email": "ccottap@lcc.uma.es"}]}, "abstract": [{"text": "The Shortest Common Supersequence problem is a hard combinatorial optimization problem with numerous practical applications. Several evolutionary approaches are proposed for this problem, considering the utilization of penalty functions, GRASP-based decoders, or repairing mechanisms. An empirical comparison is conducted, using an extensive benchmark comprising problem instances of different size and structure. The empirical results indicate that there is no single best approach, and that the size of the alphabet, and the structure of strings are crucial factors for determining performance. Nevertheless, the repairbased EA seems to provide the best performance tradeoff.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "The Shortest Common Supersequence (SCS) problem is a classical problem from the realm of string analysis. Roughly speaking, the SCS problem amounts to finding a minimal-length sequence S of symbols such that every string in a certain set L can be generated from S by removing some symbols of the latter. The resulting combinatorial problem is enormously interesting, not only from the point of view of Theoretical Computer Science, but also from an applied perspective. Indeed, it has applications in planning, data compression, and bioinformatics among other fields [1, 2, 3] .", "cite_spans": [{"start": 567, "end": 570, "text": "[1,", "ref_id": "BIBREF0"}, {"start": 571, "end": 573, "text": "2,", "ref_id": "BIBREF1"}, {"start": 574, "end": 576, "text": "3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Introduction"}, {"text": "Unfortunately, the SCS problem has been shown to be hard under various formulation and restrictions [4, 5, 6] (a summary of these hardness results is provided in Sect. 2). This way, although exact approaches have been proposed to tackle this problem (see e.g., [7] ), these are impractical for even moderatesize problem instances. Hence, heuristic approaches are in order. In this sense, greedy approaches have been popular. For example, one can cite the Majority Merge (MM) algorithm, and related variants [8] . However, these heuristics are not the ultimate solvers for this problem due to their myopic functioning. More sophisticated techniques such as evolutionary algorithms (EAs) can be used to overcome the limitations of greedy techniques.", "cite_spans": [{"start": 100, "end": 103, "text": "[4,", "ref_id": "BIBREF3"}, {"start": 104, "end": 106, "text": "5,", "ref_id": "BIBREF4"}, {"start": 107, "end": 109, "text": "6]", "ref_id": "BIBREF5"}, {"start": 261, "end": 264, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 507, "end": 510, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "This work will analyze and compare four different evolutionary approaches to the SCS problem, involving either a direct search in the space of supersequences, or using auxiliary search spaces and more complex decoding mechanisms for obtaining high-quality solutions.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Let us start by formally defining the SCS problem in its decisional version:", "cite_spans": [], "ref_spans": [], "section": "The Shortest Common Supersequence Problem"}, {"text": "Shortest Common Supersequence Problem Instance: A set L of m strings {s 1 , \u00b7 \u00b7 \u00b7 , s m } or arbitrary length over an alphabet \u03a3 (i.e., s i \u2208 \u03a3 * , for 1 i m), and a positive integer k. Question: Does there exist a string s \u2208 \u03a3 * , |s| k, such that s is a supersequence 1 of every s i \u2208 L?", "cite_spans": [], "ref_spans": [], "section": "The Shortest Common Supersequence Problem"}, {"text": "Having defined the problem, let us now consider its computational complexity.", "cite_spans": [], "ref_spans": [], "section": "The Shortest Common Supersequence Problem"}, {"text": "Complexity Results for the SCS Problem The SCS problem can be shown to be NP-hard, even if strong constraints are posed on L, or on \u03a3. For example, it is NP-hard in general when all s i have length two [2] , or when the alphabet size |\u03a3| is two [5] . It must be noted that -despite being important-NP-hard results are usually over-stressed; in fact, there are many problems that can be efficiently solved in practice, yet they are NP-hard.", "cite_spans": [{"start": 202, "end": 205, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 245, "end": 248, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "2.1"}, {"text": "Parameterized complexity [9] tries to deal with this issue, providing a more sensible characterization of hardness. The key idea is to isolate hardness (i.e., non-polynomial behavior) within a certain set of parameters. This way, if these parameters are kept fixed, the problem can be efficiently 2 solved for large problem sizes. Vertex Cover is a good example of this situation: it is NP-hard, but it can be solved in linear time in the number of vertices, when the size of the vertex cover sought is kept fixed. Problems such as Vertex Cover for which this hardness-isolation is possible are termed fixed-parameter tractable (FPT). Non-FPT problem will fall under some class in the W \u2212hierarchy. Hardness for the parameterized class W [1] is the current measure of intractability.", "cite_spans": [{"start": 25, "end": 28, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 738, "end": 741, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "2.1"}, {"text": "Several parameterizations are possible for the SCS problem. Firstly, the maximum length k of the supersequence sought can be taken as a parameter. If the alphabet size is constant, or another parameter, then the problem turns in this case to be FPT, since there are at most |\u03a3| k supersequences. However, this is not very useful in practice because k max |s i |. If the number of strings m is used as parameter, then SCS is W [1]\u2212hard, and remains so even if |\u03a3| is taken as another parameter [3] , or is constant [6] . Failure of finding FPT results in this latter scenario is particularly relevant since the alphabet size in biological problems is fixed (e.g., there are just four nucleotides in DNA). Furthermore, notice that absence of FPT algorithms implies the non-existence of fully polynomial time approximation schemes (FPTAS) for the corresponding problem.", "cite_spans": [{"start": 493, "end": 496, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 514, "end": 517, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "2.1"}, {"text": "The hardness results mentioned in the previous subsection motivate the utilization of heuristic approaches for tackling the SCSP. One of the most popular algorithms for this purpose is Majority Merge (MM). This is a greedy algorithm that constructs a supersequence incrementally by adding the symbol most frequently found at the front of strings in L, and removing these symbols from the corresponding strings. More precisely:", "cite_spans": [], "ref_spans": [], "section": "Heuristics for the SCS Problem"}, {"text": "The myopic functioning of MM makes it incapable of grasping the global structure of strings in L though. In particular, MM misses the fact that the strings can have different lengths [8] . This implies that symbols at the front of short strings will have more chances to be removed, since the algorithm has still to scan the longer strings. For this reason, it is less urgent to remove those symbols. In other words, it is better to concentrate in shortening longer strings first. This can be done by assigning a weight to each symbol, depending of the length of the string in whose front is located. Branke et al. [8] propose to use precisely this string length as weight, i.e., step 2a in the previous pseudocode would be modified to have \u03bd(\u03b1) \u2190 si=\u03b1s i |s i |. This modified heuristic will be termed weighted MM (WMM).", "cite_spans": [{"start": 183, "end": 186, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 615, "end": 618, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Heuristics for the SCS Problem"}, {"text": "Several other heuristics were also defined in [8] on the basis of WMM. For example, one of them has \u03bd(\u03b1) \u2190 |WMM ({s 1 | \u03b1 , \u00b7 \u00b7 \u00b7 , s m | \u03b1 })| (to be minimized), where s| \u03b1 is the string obtained by removing \u03b1 from the front of s; ties are broken by maximizing |WMM ({s i | s i = \u03b1s i })|. A more interesting heuristic results from the combination of EAs and WMM. In this heuristic, the EA is used to evolve weights for each character of every string. These weights are utilized within step 2a, modifying the influence of symbols in each string. This is done by multiplying the WMM weight with the evolved weight, i.e., \u03bd(\u03b1) \u2190 si=\u03b1s i w p\u03b1i,i |s i |, where p \u03b1i is the position of the current front symbol \u03b1 in the original string s i . In a further refinement, the EA is used also to evolve a basic value to be added to each weight before evaluation.", "cite_spans": [{"start": 46, "end": 49, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Heuristics for the SCS Problem"}, {"text": "This EA approach is similar to the EA used in [10] for the multidimensional knapsack problem, in which a greedy heuristic was used to generate solutions, and weights were evolved in order to modify the value of objects (thus making the underlying heuristic take different decisions). Next section will explore alternative EA definitions in which the search is conducted in search spaces different to this weight space.", "cite_spans": [{"start": 46, "end": 50, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Heuristics for the SCS Problem"}, {"text": "Clearly, one of the difficulties faced by an EA when applied to the SCS problem is the existence of feasibility constraints, i.e., an arbitrary string s \u2208 \u03a3 * , no matter its length, is not necessarily a supersequence of strings in L. Typically, these situations can be solved in three ways: (i) allowing the generation of infeasible solutions and penalizing accordingly, (ii) using a repairing mechanism for mapping infeasible solutions to feasible solutions, and (iii) defining appropriate operators and/or problem representation to avoid the generation of infeasible solutions. These three possibilities will be explored below.", "cite_spans": [], "ref_spans": [], "section": "Evolutionary Approaches to the SCS Problem"}, {"text": "Let us firstly consider the simplest version, namely the use of a penalty function. The idea is to have an EA evolving strings in \u03a3 * , using the supersequence length as the quality measure for feasible solutions, and adding an extra penalty term for infeasible solutions. This has been implemented as follows: let s be the tentative solution provided by the EA, and let L = {s 1 , \u00b7 \u00b7 \u00b7 s m } be the target strings; then, the fitness (to be minimized) is ", "cite_spans": [], "ref_spans": [], "section": "Evolutionary Approaches to the SCS Problem"}, {"text": "As it can be seen, this repairing function not only completes s in order to have a valid supersequence, but also removes intermediate symbols that are not present at the front of any string at a certain step. Thus, it also serves the purpose of local improver. This algorithm is termed Repair EA. The third approach is to have the EA handling uniquely feasible solutions. In this case, we have considered the utilization of an auxiliary space, and a smart decoder in order to perform the mapping to the sequence space. This decoder can be based on either MM or WMM, and borrows some ideas from greedy randomized adaptive search procedures (GRASP). This latter metaheuristic also relies on an underlying greedy heuristic, and proceeds iteratively by selecting at each step an attribute of the solution from a candidate list [11] . This selection can be typically done by using a qualitative criterion (i.e., a candidate is selected among the best k elements in the candidate list, k being a parameter), or a quantitative criterion (i.e., a candidate is selected among the elements whose quality is within a certain range).", "cite_spans": [{"start": 823, "end": 827, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Evolutionary Approaches to the SCS Problem"}, {"text": "One of the potential problems of the basic GRASP procedure described before relies on the selection of the parameter for selecting an attribute value from the candidate list. As shown in [12] , using a single fixed value for this parameter may hinder finding high-quality solutions. Several options are possible to solve this problem. On one hand, a learning-based strategy termed reactive GRASP was proposed [13] . On the other hand, the utilization of EAs to evolve the sequence of selection decisions was presented in [14] . This latter approach is precisely considered here. To be precise, the EA is used to evolve a sequence of integers \u03b4 1 , \u00b7 \u00b7 \u00b7 , \u03b4 n ), \u03b4 i \u2208 [1..|\u03a3|], where n = m i=1 |s i |. At each step of the decoding process, a ranked list of the potential symbols to be added to the supersequence is constructed using either the MM or the WMM criterion; the value \u03b4 i indicates that the \u03b4 i -th best symbol is chosen at the i-th step. Notice that the construction of the supersequence will in general be accomplished in many less steps than n.", "cite_spans": [{"start": 187, "end": 191, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 409, "end": 413, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 521, "end": 525, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Evolutionary Approaches to the SCS Problem"}, {"text": "The experiments have been done with a steady-state EA (popsize = 100, p X = .9, p m = 1/n, maxevals = 100, 000), using binary tournament selection, uniform crossover, and random-substitution mutation. Three different sets of problem instances have been considered in the experimentation. The first one is composed of random strings with different lengths. To be precise, each instance is composed of eight strings, four of them with 40 symbols, and the remaining four with 80 symbols. Each of these strings is randomly built, using an alphabet \u03a3. Four subsets of instances have been defined using different alphabet sizes, namely |\u03a3| =2, 4, 8, and 16. For each alphabet size, five different instances have been generated.", "cite_spans": [], "ref_spans": [], "section": "Experimental Validation"}, {"text": "The second set of instances comprises strings whose structure is deceptive for greedy heuristics such as MM or WMM. These are the following (cf. [8] ):", "cite_spans": [{"start": 145, "end": 148, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Experimental Validation"}, {"text": "The optimal solution for this instance is to firstly remove the three b's, and then remove the forty a's. It thus has length 43. Finally, a more realistic benchmark consisting of strings with a common origin has been considered. A DNA sequence from a SARS coronavirus strain has been retrieved from a genomic database 3 , and has been taken as supersequence; then, different sequences are obtained from this supersequence by scanning it from left to right, and skipping nucleotides with a certain fixed probability. In these experiments, the length of the supersequence is 158, the gap probability is 10%, 15%, or 20% and the number of so-generated sequences is 10. First of all, the results for random strings are shown in Table 1 . WMM performs better than MM, and both the repair-based EA and the WMM-based EA outperform the remaining algorithms. The MM-based EA is capable of beating MM, and produce similar results to those of WMM; however, it cannot compete with the repair-based EA or the WMM-based EA. Notice also the poor results of the plain Direct EA (it cannot even produce a feasible solution for |\u03a3| = 16).", "cite_spans": [], "ref_spans": [{"start": 724, "end": 731, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Experimental Validation"}, {"text": "It is interesting to note the U-shaped behavioral pattern of the EAs with respect to the basic heuristics. This is illustrated in Fig. 1 , that shows the ratio between the percentage of improvement of the EAs (with respect to MM), and the percentage of improvement of WMM (also with respect to MM). The pattern for the MM-based EA and the WMM-based EA is the same (save for the scale factor) with a minimum at |\u03a3| = 4; however, this pattern is shifted for the repair-based EA that has its minimum at |\u03a3| = 8. At any rate, the performance of the repair-based EA is here very similar to that of the WMM-based EA.", "cite_spans": [], "ref_spans": [{"start": 130, "end": 136, "text": "Fig. 1", "ref_id": null}], "section": "Experimental Validation"}, {"text": "The results for the deceptive problem instances are shown in Table 2 . As expected, both MM and WMM are fooled by the problem structure. However, the EAs are capable of finding consistently the optimal solutions without major difficulties. This emphasizes the important role played by the evolutionary search in this problem.", "cite_spans": [], "ref_spans": [{"start": 61, "end": 68, "text": "Table 2", "ref_id": null}], "section": "Experimental Validation"}, {"text": "Finally, the results for the strings from the SARS DNA sequence are shown in Table 3 . The basic heuristics perform quite well for low gap probability (i.e., for larger strings). However, neither the MM-based EA nor the WMM-based EA can match this performance (recall that |\u03a3| = 4, that is, the lower performance point for these EAs). For the larger gap probability there is a remarkable performance drop of the basic heuristics, and GRASP-based EAs can catch up with these. The repair-based EA offers the best results throughout the three problem instances, performing significantly better than the remaining algorithms.", "cite_spans": [], "ref_spans": [{"start": 77, "end": 84, "text": "Table 3", "ref_id": "TABREF1"}], "section": "Experimental Validation"}, {"text": "Four different EAs have been proposed and compared for the SCS problem. The main goal has been to determine which of the typical constraint-handling procedures is more appropriate for this problem. The experimental results seem to indicate that the best performance tradeoff is provided by the repair-based EA: it behaves much better than the other EAs in some problem instances, and similarly to these in the remaining ones. The GRASP-based EAs are located at the next performance level, the WMM-based EA performing better than the MM-based EA. Future work will be directed to test other underlying heuristics for decoding solutions. The repair-based EA seems to be the adversary with which such new EAs should be confronted.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Theory and algorithms for plan merging", "authors": [{"first": "D", "middle": [], "last": "Foulser", "suffix": ""}, {"first": "M", "middle": [], "last": "Li", "suffix": ""}, {"first": "Q", "middle": [], "last": "Yang", "suffix": ""}], "year": 1992, "venue": "Artificial Intelligence", "volume": "57", "issn": "", "pages": "143--181", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Complexity of common subsequence and supersequence problems and related problems", "authors": [{"first": "V", "middle": [], "last": "Timkovsky", "suffix": ""}], "year": 1990, "venue": "Cybernetics", "volume": "25", "issn": "", "pages": "565--580", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "An integrated complexity analysis of problems from computational biology", "authors": [{"first": "M", "middle": [], "last": "Hallet", "suffix": ""}], "year": 1996, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "The parameterized complexity of sequence alignment and consensus", "authors": [{"first": "H", "middle": [], "last": "Bodlaender", "suffix": ""}, {"first": "R", "middle": [], "last": "Downey", "suffix": ""}, {"first": "M", "middle": [], "last": "Fellows", "suffix": ""}, {"first": "H", "middle": [], "last": "Wareham", "suffix": ""}], "year": 1994, "venue": "Theoretical Computer Science", "volume": "147", "issn": "", "pages": "31--54", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "More on the complexity of common superstring and supersequence problems", "authors": [{"first": "M", "middle": [], "last": "Middendorf", "suffix": ""}], "year": 1994, "venue": "Theoretical Computer Science", "volume": "125", "issn": "", "pages": "205--228", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "On the parameterized complexity of the fixed alphabet shortest common supersequence and longest common subsequence problems", "authors": [{"first": "K", "middle": [], "last": "Pietrzak", "suffix": ""}], "year": 2003, "venue": "Journal of Computer and System Sciences", "volume": "67", "issn": "", "pages": "757--771", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Subsequences and Supersequences", "authors": [{"first": "C", "middle": [], "last": "Fraser", "suffix": ""}], "year": 1995, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Improved heuristics and a genetic algorithm for finding short supersequences", "authors": [{"first": "J", "middle": [], "last": "Branke", "suffix": ""}, {"first": "M", "middle": [], "last": "Middendorf", "suffix": ""}, {"first": "F", "middle": [], "last": "Schneider", "suffix": ""}], "year": 1998, "venue": "OR-Spektrum", "volume": "20", "issn": "", "pages": "39--45", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Parameterized Complexity", "authors": [{"first": "R", "middle": [], "last": "Downey", "suffix": ""}, {"first": "M", "middle": [], "last": "Fellows", "suffix": ""}], "year": 1998, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "A hybrid genetic algorithm for the 0-1 multiple knapsack problem", "authors": [{"first": "C", "middle": [], "last": "Cotta", "suffix": ""}, {"first": "J", "middle": [], "last": "Troya", "suffix": ""}], "year": 1998, "venue": "Artificial Neural Nets and Genetic Algorithms", "volume": "", "issn": "", "pages": "251--255", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Greedy randomized adaptive search procedures", "authors": [{"first": "T", "middle": [], "last": "Feo", "suffix": ""}, {"first": "M", "middle": [], "last": "Resende", "suffix": ""}], "year": 1995, "venue": "Journal of Global Optimization", "volume": "6", "issn": "", "pages": "109--133", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Parameter variation in GRASP procedures", "authors": [{"first": "M", "middle": [], "last": "Prais", "suffix": ""}, {"first": "C", "middle": [], "last": "Ribeiro", "suffix": ""}], "year": 2000, "venue": "Investigaci\u00f3n Operativa", "volume": "9", "issn": "", "pages": "1--20", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Reactive GRASP: an application to a matrix decomposition problem in TDMA traffic assignment", "authors": [{"first": "M", "middle": [], "last": "Prais", "suffix": ""}, {"first": "C", "middle": [], "last": "Ribeiro", "suffix": ""}], "year": 2000, "venue": "INFORMS Journal on Computing", "volume": "12", "issn": "", "pages": "164--176", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "A hybrid GRASP-evolutionary algorithm approach to golomb ruler search", "authors": [{"first": "C", "middle": [], "last": "Cotta", "suffix": ""}, {"first": "A", "middle": [], "last": "Fern\u00e1ndez", "suffix": ""}], "year": 2004, "venue": "Lecture Notes in Computer Science", "volume": "3242", "issn": "", "pages": "481--490", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "\u2200i : s i = 1 + f itness(s , L| \u03b1 ) if \u2203i : s i = and s = \u03b1s |MM(L)| if \u2203i : s i = and s = (1) This algorithm will be termed Penalty EA, and relies in MM for providing a heuristic assessment on how much longer a string should be to account for uncovered string suffixes in L. An obvious variant consists of injecting the actual sequence returned by MM back to the candidate solution. Thus, the sequence is repaired, resulting in a feasible solution. More precisely, the repairing function \u03c1 : \u03a3 * \u00d7 (\u03a3 * ) m \u2192 \u03a3 * can be described as follows: \u03c1 (s, L) \u2200i : s i = \u03c1(s , L) if \u2203i : s i = and i : s i = \u03b1s i and s = \u03b1s \u03b1\u03c1(s , L| \u03b1 ) if \u2203i : s i = \u03b1s i and s = \u03b1s MM(L) if \u2203i : s i = and s =", "latex": null, "type": "figure"}, "FIGREF1": {"text": "a 40 , 4 \u00d7 b 13 a 27 , 2 \u00d7 b 26 a 14 , 1 \u00d7 b 39 a}. This instance is similar to L 1 , and again the optimal solution is to firstly remove the b's, i.e., b 39 a 40 , a solution of length 79. -L 3 = {8 \u00d7 a 20 b 20 , 8 \u00d7 b 20 c 20 }. The optimal solution in this case is a 20 b 20 c 20 .", "latex": null, "type": "figure"}, "TABREF0": {"text": "Results of the different heuristics on 8 random strings (4 of length 40, and 4 of length 80), for different alphabet sizes |\u03a3|. The results of MM and WMM are averaged over 150 executions, and the results of the EAs are averaged over 30 runs. In all cases, the results are further averaged over five different problem instances", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>MM </td><td>\u00a0</td><td>WMM </td><td>Penalty EA best mean \u00b1 std.dev.\n</td></tr><tr><td>|\u03a3| </td><td>best mean \u00b1 std.dev. </td><td>best mean \u00b1 std.dev. </td><td>\u00a0</td></tr><tr><td>2 </td><td>115.0 </td><td>120.46 \u00b1 2.16 </td><td>114.8 </td><td>116.13 \u00b1 0.89 </td><td>119.0 123.61 \u00b1 2.52\n</td></tr><tr><td>4 </td><td>164.2 </td><td>175.47 \u00b1 5.02 </td><td>157.8 </td><td>161.85 \u00b1 2.81 </td><td>200.2 223.31 \u00b1 12.18\n</td></tr><tr><td>8 </td><td>227.0 </td><td>249.33 \u00b1 6.92 </td><td>210.4 </td><td>219.61 \u00b1 4.90 </td><td>366.8 445.94 \u00b1 23.48\n</td></tr><tr><td>16 </td><td>309.2 </td><td>333.57 \u00b1 9.08 </td><td>282.8 </td><td>296.07 \u00b1 5.31 </td><td>538.2 569.74 \u00b1 11.73\n</td></tr><tr><td>|\u03a3| </td><td>Repair EA best mean \u00b1 std.dev. </td><td>GRASP-EA(MM) best mean \u00b1 std.dev. </td><td>GRASP-EA(WMM) best mean \u00b1 std.dev.\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>2 </td><td>111.2 </td><td>112.58 \u00b1 0.75 </td><td>113.0 </td><td>116.95 \u00b1 1.97 </td><td>110.8 113.31 \u00b1 1.49\n</td></tr><tr><td>4 </td><td>151.6 </td><td>155.17 \u00b1 1.85 </td><td>160.6 </td><td>167.47 \u00b1 3.11 </td><td>151.6 157.81 \u00b1 2.98\n</td></tr><tr><td>8 </td><td>205.4 </td><td>213.47 \u00b1 3.97 </td><td>217.6 </td><td>228.18 \u00b1 4.22 </td><td>204.0 211.01 \u00b1 3.05\n</td></tr><tr><td>16 </td><td>267.0 </td><td>281.81 \u00b1 5.88 </td><td>286.2 </td><td>297.53 \u00b1 4.94 </td><td>271.2 278.15 \u00b1 2.97\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Results of the different heuristics on the strings from the SARS DNA sequence for different gap probabilities. The results of MM and WMM are averaged over 150 executions, and the results of the EAs are averaged over 30 runs Fig. 1. Improvement ratios of the different EAs with respect to WWM for different alphabet sizes", "latex": null, "type": "table"}}, "back_matter": [{"text": "Acknowledgements. This work is partially supported by Spanish MCyT and FEDER under contract TIC2002-04498-C05-02.", "cite_spans": [], "ref_spans": [], "section": "acknowledgement"}]}