{"paper_id": "0ecb63ffe3fbc9fffca18f04eed107f64d13a85b", "metadata": {"title": "NUMA-Awareness as a Plug-In for an Eventify-Based Fast Multipole Method", "authors": [{"first": "Laura", "middle": [], "last": "Morgenstern", "suffix": "", "affiliation": {"laboratory": "", "institution": "Forschungszentrum J\u00fclich", "location": {"postCode": "52425", "settlement": "J\u00fclich", "country": "Germany"}}, "email": "l.morgenstern@fz-juelich.de"}, {"first": "David", "middle": [], "last": "Haensel", "suffix": "", "affiliation": {"laboratory": "", "institution": "Forschungszentrum J\u00fclich", "location": {"postCode": "52425", "settlement": "J\u00fclich", "country": "Germany"}}, "email": "d.haensel@fz-juelich.de"}, {"first": "Andreas", "middle": [], "last": "Beckmann", "suffix": "", "affiliation": {"laboratory": "", "institution": "Forschungszentrum J\u00fclich", "location": {"postCode": "52425", "settlement": "J\u00fclich", "country": "Germany"}}, "email": "a.beckmann@fz-juelich.de"}, {"first": "Ivo", "middle": [], "last": "Kabadshow", "suffix": "", "affiliation": {"laboratory": "", "institution": "Forschungszentrum J\u00fclich", "location": {"postCode": "52425", "settlement": "J\u00fclich", "country": "Germany"}}, "email": "i.kabadshow@fz-juelich.de"}]}, "abstract": [{"text": "Following the trend towards Exascale, today's supercomputers consist of increasingly complex and heterogeneous compute nodes. To exploit the performance of these systems, research software in HPC needs to keep up with the rapid development of hardware architectures. Since manual tuning of software to each and every architecture is neither sustainable nor viable, we aim to tackle this challenge through appropriate software design. In this article, we aim to improve the performance and sustainability of FMSolvr, a parallel Fast Multipole Method for Molecular Dynamics, by adapting it to Non-Uniform Memory Access architectures in a portable and maintainable way. The parallelization of FMSolvr is based on Eventify, an event-based tasking framework we co-developed with FMSolvr. We describe a layered software architecture that enables the separation of the Fast Multipole Method from its parallelization. The focus of this article is on the development and analysis of a reusable NUMA module that improves performance while keeping both layers separated to preserve maintainability and extensibility. By means of the NUMA module we introduce diverse NUMA-aware data distribution, thread pinning and work stealing policies for FMSolvr. During the performance analysis the modular design of the NUMA module was advantageous since it facilitates combination, interchange and redesign of the developed policies. The performance analysis reveals that the runtime of FMSolvr is reduced by 21% from 1.48 ms to 1.16 ms through these policies.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "The trend towards higher clock rates stagnates and heralds the start of the exascale era. The resulting supply of higher core counts leads to the rise of Non-Uniform Memory Access (NUMA) systems for reasons of scalability. This requires not only the exploitation of fine-grained parallelism, but also the handling of hierarchical memory architectures in a sustainable and thus portable way. We aim to tackle this challenge through suitable software design since manual adjustment of research software to each and every architecture is neither sustainable nor viable for reasons of development time and staff expenses.", "cite_spans": [], "ref_spans": [], "section": "Challenge"}, {"text": "Our use case is FMSolvr, a parallel Fast Multipole Method (FMM) for Molecular Dynamics (MD). The parallelization of FMSolvr is based on Eventify, a tailor-made tasking library that allows for the description of fine-grained task graphs through events [7, 8] . Eventify and FMSolvr are published as open source under LGPL v2.1 and available at www.fmsolvr.org. In this article, we aim to improve the performance and sustainability of FMSolvr by adapting it to NUMA architectures through the following contributions: 1. A layered software design that separates the algorithm from its parallelization and thus facilitates the development of new features and the support of new hardware architectures. 2. A reusable NUMA module for Eventify that models hierarchical memory architectures in software and enables rapid development of algorithmdependent NUMA policies. 3. Diverse NUMA-aware data distribution, thread pinning and work stealing policies for FMSolvr based on the NUMA module. 4. A detailed comparative performance analysis of the developed NUMA policies for the FMM on two different NUMA machines.", "cite_spans": [{"start": 251, "end": 254, "text": "[7,", "ref_id": "BIBREF7"}, {"start": 255, "end": 257, "text": "8]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Challenge"}, {"text": "MD has become a vital research method in biochemistry, pharmacy and materials science. MD simulations target strong scaling since their problem size is typically small. Thus, the computational effort per compute node is very low and MD applications tend to be latency-and synchronization-critical. To exploit the performance of NUMA systems, MD applications need to adapt to the differences in latency and throughput caused by hierarchical memory.", "cite_spans": [], "ref_spans": [], "section": "State of the Art"}, {"text": "In this work, we focus on the FMM [6] with computational complexity O(N ). We consider the hierarchical structure of the FMM as a good fit for hierarchical memory architectures. We focus on the analysis of NUMA effects on FMSolvr as a demonstrator for latency-and synchronization-critical applications.", "cite_spans": [{"start": 34, "end": 37, "text": "[6]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "State of the Art"}, {"text": "We aim at a NUMA module for FMSolvr since various research works [1, 3, 4] prove the positive impact of NUMA awareness on performance and scalability of the FMM. Subsequently, we summarize the efforts to support NUMA in current FMM implementations.", "cite_spans": [{"start": 65, "end": 68, "text": "[1,", "ref_id": "BIBREF0"}, {"start": 69, "end": 71, "text": "3,", "ref_id": "BIBREF2"}, {"start": 72, "end": 74, "text": "4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "State of the Art"}, {"text": "ScalFMM [3] is a parallel, C++ FMM-library. The main objectives of its software architecture are maintainability and understandability. A lot of research about task-based and data-driven FMMs is based on ScalFMM. The authors devise the parallel data-flow of the FMM for shared memory architectures in [3] and for distributed memory architectures in [2] . In ScalFMM NUMA policies can be set by the user via the OpenMP environment variables OMP PROC BIND and OMP PLACES. However, to the best of our knowledge, there is no performance analysis regarding these policies for ScalFMM.", "cite_spans": [{"start": 8, "end": 11, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 301, "end": 304, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 349, "end": 352, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "State of the Art"}, {"text": "KIFMM [15] is a kernel-independent FMM. In [4] NUMA awareness is analyzed dependent on work unit granularity and a speed-up of 4 on 48 cores is gained.", "cite_spans": [{"start": 6, "end": 10, "text": "[15]", "ref_id": "BIBREF15"}, {"start": 43, "end": 46, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "State of the Art"}, {"text": "However, none of the considered works provides an implementation and comparison of different NUMA-aware thread pinning and work stealing policies. From our point of view, the implementation and comparison of different policies is of interest since NUMA awareness is dependent on the properties of the input data set, the FMM operators and the hardware. Therefore, the focus of the current work is to provide a software design that enables the rapid development and testing of multiple NUMA policies for the FMM.", "cite_spans": [], "ref_spans": [], "section": "State of the Art"}, {"text": "We follow the definition of sustainability provided in [11] : Definition 1. Sustainability. A long-living software system is sustainable if it can be cost-efficiently maintained and evolved over its entire life-cycle.", "cite_spans": [{"start": 55, "end": 59, "text": "[11]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Sustainability"}, {"text": "According to [11] a software system is further on long-living if it must be operated for more than 15 years. Due to a relatively stable problem set, a large user base and the great performance optimization efforts, HPC software is typically long-living. This holds e.g. for the molecular dynamics software GROMACS or Coulomb-solvers such as ScaFaCos. Fortran FMSolvr (included in ScaFaCos), the predecessor of C++ FMSolvr, is roughly 20 years old. Hence, sustainability is our main concern regarding C++ FMSolvr.", "cite_spans": [{"start": 13, "end": 17, "text": "[11]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Sustainability"}, {"text": "According to [11] , sustainability comprises non-functional requirements such as maintainability, modifiability, portability and evolvability. We add performance and performance portability to the properties of sustainability, to adjust the term to software development in HPC.", "cite_spans": [{"start": 13, "end": 17, "text": "[11]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Sustainability"}, {"text": "Regarding performance portability, we follow the definition provided in [14] : Definition 2. Performance Portability. For a given set of platforms H, the performance portability \u03a6 of an application a solving problem p is:", "cite_spans": [{"start": 72, "end": 76, "text": "[14]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Performance Portability"}, {"text": "where e i (a, p) is the performance efficiency of application a solving problem p on platform i.", "cite_spans": [], "ref_spans": [], "section": "Performance Portability"}, {"text": "NUMA is a shared memory architecture for modern multi-processor and multicore systems. A NUMA system consists of several NUMA nodes. Each NUMA node is a set of cores together with their local memory. NUMA nodes are connected via a NUMA interconnect such as Intel's Quick Path Interconnect (QPI) or AMD's HyperTransport. The cores of each NUMA node can access the memory of remote NUMA nodes only by traversing the NUMA interconnect. However, this exhibits notably higher memory access latencies and lower bandwidths than accessing local memory. According to [12] , remote memory access latencies are about 50% higher than local memory access latencies. Hence, memory-bound applications have to take data locality into account to exploit the performance and scalability of NUMA architectures.", "cite_spans": [{"start": 558, "end": 562, "text": "[12]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Non-uniform Memory Access"}, {"text": "Sequential Algorithm. The fast multipole method for MD is a hierarchical fast summation method (HFSM) for the evaluation of Coulombic interactions in N -body simulations. The FMM computes the Coulomb force F i acting on each particle i, the electrostatic field E and the Coulomb potential \u03a6 in each time step of the simulation. The FMM reduces the computational complexity of classical Coulomb solvers from O(N 2 ) to O(N ) by use of hierarchical multipole expansions for the computation of long-range interactions.", "cite_spans": [], "ref_spans": [], "section": "Fast Multipole Method"}, {"text": "The algorithm starts out with a hierarchical space decomposition to group particles. This is done by recursively bisecting the simulation box in each of its three dimensions. We refer to the developing octree as FMM tree. The input data set to create the FMM tree consists of location x i and charge q i of each particle i in the system as well as the algorithmic parameters multipole order p, maximal tree depth d max and well-separateness criterion ws. All three algorithmic parameters influence the time to solution and the precision of the results.", "cite_spans": [], "ref_spans": [], "section": "Fast Multipole Method"}, {"text": "We subsequently introduce the relations between the boxes of the FMM tree referring to [5] , since the data dependencies between the steps of the algorithm are based on those: Based on the FMM tree, the sequential workflow of the FMM referring to [10] is stated in Algorithm 1, with steps 1. to 5. computing far field interactions and step 6. computing near field interactions.", "cite_spans": [{"start": 87, "end": 90, "text": "[5]", "ref_id": "BIBREF5"}, {"start": 247, "end": 251, "text": "[10]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Fast Multipole Method"}, {"text": "Input: Positions and charges of particles Output: Electrostatic field E, Coulomb forces F, Coulomb potential \u03a6", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1. Fast Multipole Method"}, {"text": "Hierarchical space decomposition 1. Particle to Multipole (P2M): Expansion of particles in each box on the lowest level of the FMM tree into multipole moments \u03c9 relative to the center of the box.", "cite_spans": [], "ref_spans": [], "section": "Create FMM Tree:"}, {"text": "Accumulative upwards-shift of the multipole moments \u03c9 to the centers of the parent boxes.", "cite_spans": [], "ref_spans": [], "section": "Multipole to Multipole (M2M):"}, {"text": "Translation of the multipole moments \u03c9 of the boxes covered by the interaction set of box i into a local moment \u03bc for i.", "cite_spans": [], "ref_spans": [], "section": "Multipole to Local (M2L):"}, {"text": "Accumulative downwards-shift of the local moments \u03bc to the centers of the child boxes.", "cite_spans": [], "ref_spans": [], "section": "Local to Local (L2L):"}, {"text": "Transformation of the local moment \u03bc of each box i on the lowest level into far field force for each particle in i.", "cite_spans": [], "ref_spans": [], "section": "Local to Particle (L2P):"}, {"text": "Evaluation of the near field forces between the particles contained by box i and its near neighbors for each box on the lowest level by computing the direct interactions.", "cite_spans": [], "ref_spans": [], "section": "Particle to Particle (P2P):"}, {"text": "FMSolvr: An Eventify-Based FMM. FMSolvr is a task-parallel implementation of the FMM. According to the tasking approach for FMSolvr with Eventify [7, 8] , the steps of the sequential algorithm do not have to be computed completely sequentially, but may overlap. Based on the sequential workflow, we differentiate six types of tasks (P2M, M2M, M2L, L2L, L2P and P2P) that span a tree-structured task graph. Since HFSMs such as the FMM are based on a hierarchical decomposition, they exhibit tree-structured, acyclic task graphs and use tree-based data structures. Figure 1 provides a schematic overview of the horizontal and vertical task dependencies of the FMM implied by this parallelization scheme. For reasons of comprehensible illustration, the dependencies are depicted for a binary tree and thus a one-dimensional system, even though the FMM simulates three-dimensional systems and thus works with octrees. In this work, we focus on the tree-based properties of the FMM since these are decisive for its adaption to NUMA architectures. For further details on the functioning of Eventify and its usage for FMSolvr please refer to [7, 8] .", "cite_spans": [{"start": 146, "end": 149, "text": "[7,", "ref_id": "BIBREF7"}, {"start": 150, "end": 152, "text": "8]", "ref_id": "BIBREF8"}, {"start": 1135, "end": 1138, "text": "[7,", "ref_id": "BIBREF7"}, {"start": 1139, "end": 1141, "text": "8]", "ref_id": "BIBREF8"}], "ref_spans": [{"start": 563, "end": 571, "text": "Figure 1", "ref_id": "FIGREF1"}], "section": "Particle to Particle (P2P):"}, {"text": "3 Software Architecture", "cite_spans": [], "ref_spans": [], "section": "Particle to Particle (P2P):"}, {"text": "Figure 2 provides an excerpt of the software architecture of FMSolvr by means of UML. FMSolvr is divided into two main layers: the algorithm layer and the parallelization layer. The algorithm layer encapsulates the mathematical details of the FMM, e.g. exchangeable FMM operators with different time complexities and memory footprints. The parallelization layer hides the details of parallel hardware from the algorithm developer. It contains modules for threading and vectorization and is designed to be extended with specialized modules, e.g. for locking policy, NUMA-, and GPU-support. In this article, we focus on the description of the NUMA module. Hence, Fig. 2 contains only that part of the software architecture which is relevant to NUMA. We continuously work on further decoupling of the parallelization layer as independent parallelization library, namely Eventify, to increase its reusability and the reusability of its modules, e.g. the NUMA module described in Sect. 4. Figure 3 shows the software architecture of the NUMA module and its integration in the software architecture of FMSolvr. Regarding the integration, we aim to preserve the layering and the according separation of concerns as effectively as possible. However, effective NUMA-awareness requires information from the data layout, which is part of the algorithm layer, and from the hardware, which is part of the parallelization layer. Hence, a slight blurring of both layers is unfortunately unavoidable in this case. After all, we preserve modularization by providing an interface for the algorithm layer, namely NumaAllocator, and an interface for the parallelization layer, namely NumaModule. Based on the hardware information provided by the NUMA distance table, we model the NUMA architecture of the compute node in software. Hence, a compute node is a Resource that consists of NumaNodes, which consist of Cores, which in turn consist of ProcessingUnits. To reuse the NUMA module for other parallel applications, developers are required to redevelop or adjust the NUMA policies (see Sect. 4) contained in class NumaModule only. ", "cite_spans": [], "ref_spans": [{"start": 661, "end": 667, "text": "Fig. 2", "ref_id": "FIGREF2"}, {"start": 984, "end": 992, "text": "Figure 3", "ref_id": "FIGREF3"}], "section": "Layering: Separation of Algorithm and Parallelization"}, {"text": "As described in Sect. 2.4, the FMM exhibits a tree-structured task graph. As shown in Fig. 1 , this task graph and its dedicated data are distributed to NUMA nodes through an equal partitioning of the tasks on each tree level. To assure data locality, a thread and the data it works on are assigned to the same NUMA node. Even though this is an important step to improve data locality, there are still task dependencies that lead to data transfer between NUMA nodes. In order to reduce this inter-node data transfer, we present different thread pinning policies.", "cite_spans": [], "ref_spans": [{"start": 86, "end": 92, "text": "Fig. 1", "ref_id": "FIGREF1"}], "section": "Data Distribution"}, {"text": "Equal Pinning. With Equal Pinning (EP), we pursue a classical load balancing approach (cf. Scatter Principally [13] ). This means that the threads are equally distributed among the NUMA nodes. Hence, this policy is suitable for NUMA systems with homogeneous NUMA nodes only.", "cite_spans": [{"start": 111, "end": 115, "text": "[13]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Thread Pinning Policies"}, {"text": "Algorithm 2 shows the pseudocode for NUMA-aware thread pinning via policy EP. Let t be the number of threads, n be the number of NUMA nodes and tpn be the number of threads per NUMA node.", "cite_spans": [], "ref_spans": [], "section": "Thread Pinning Policies"}, {"text": "The determined number of threads is mapped to the cores of each NUMA node in an ascending order of physical core-ids. This means that threads with successive logical ids are pinned to neighboring cores. This is reasonable with regard to architectures where neighboring cores share a cache-module. In addition, strict pinning to cores serves the avoidance of side-effects due to the behavior of the process scheduler, which may vary dependent on the systems state and complicate an analysis of NUMA-effects.", "cite_spans": [], "ref_spans": [], "section": "Thread Pinning Policies"}, {"text": "if i < r then // number of threads per node for nodes 0, . . . , r \u2212 1 tpn = t/n + 1 Assign threads i \u00b7 tpn, . . . , (i \u00b7 tpn) + tpn \u2212 1 to node i else // number of threads per node for nodes r, . . . , n \u2212 1 tpn = t/n Assign threads i \u00b7 tpn + r, . . . , (i \u00b7 tpn + r) + tpn \u2212 1 to node i end if end for Compact Pinning. The thread pinning policy Compact Pinning (CP) combines the advantages of the NUMA-aware thread pinning policies Equal Pinning and Compact Ideally (cf. [13] ). The aim of CP is to avoid data transfer via the NUMA interconnect by using as few NUMA nodes as possible while avoiding the use of SMT.", "cite_spans": [{"start": 473, "end": 477, "text": "[13]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Algorithm 2. Equal Pinning"}, {"text": "Algorithm 3 shows the pseudocode for the NUMA-aware thread pinning policy CP. Let c be the total number of cores of the NUMA system and c n be the number of cores per NUMA node excl. SMT-threads. Considering CP, threads are assigned to a single NUMA node as long as the NUMA node has got cores to which no thread is assigned to. Only if a thread is assigned to each core of a NUMA node, the next NUMA node is filled up with threads. This means especially that data transfer via the NUMA interconnect is fully avoided if t \u2264 c n holds. If t \u2265 c holds, thread pinning policy EP becomes effective to reduce the usage of SMT. For this policy we apply strict pinning based on the neighboring cores principle as described in Sect. 4.2, too.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 2. Equal Pinning"}, {"text": "if t < c then for i = 0; i < n; i + + do Assign threads i \u00b7 cn, . . . , (i \u00b7 cn) + cn \u2212 1 to node i end for else", "cite_spans": [], "ref_spans": [], "section": "Algorithm 3. Compact Pinning"}, {"text": "Use policy Equal Pinning end if CP is tailor-made for the FMM as well as for tree-structured task graphs with horizontal and vertical task dependencies in general. CP aims to keep the vertical cut through the task graph as short as possible. Hence, as few as possible task dependencies require data transfer via the NUMA interconnect.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 3. Compact Pinning"}, {"text": "Local and Remote Node. Local and Remote Node (LR) is the default work stealing policy that does not consider the NUMA architecture of the system at all.", "cite_spans": [], "ref_spans": [], "section": "Work Stealing Policies"}, {"text": "Prefer Local Node. Applying the NUMA-aware work stealing policy Prefer Local Node (PL) means that threads preferably steal tasks from threads located on the same NUMA node. However, threads are allowed to steal tasks from threads located on remote NUMA nodes if no tasks are available on the local NUMA node.", "cite_spans": [], "ref_spans": [], "section": "Work Stealing Policies"}, {"text": "Local Node Only. If the NUMA-aware work stealing policy Local Node Only (LO) is applied, threads are allowed to steal tasks from threads that are located on the same NUMA node only. According to [13] , we would expect this policy to improve performance if stealing across NUMA nodes is more expensive than idling. This means that stealing a task, e.g. transferring its data, takes too long in comparison to the execution time of the task.", "cite_spans": [{"start": 195, "end": 199, "text": "[13]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Work Stealing Policies"}, {"text": "The runtime measurements for the performance analysis were conducted on a 2-NUMA-node system and a 4-NUMA-node system. The 2-NUMA-node system is a single compute node of Jureca. The dual-socket system is equipped with two Intel Xeon E5-2680 v3 CPUs (Haswell) which are connected via QPI. Each CPU consists of 12 two-way SMT-cores, meaning, 24 processing units. Hence, the system provides 48 processing units overall. Each core owns an L1 data and instruction cache with 32 kB each. Furthermore, it owns an L2 cache with 256 kB. Hence, each two processing units share an L1 and an L2 cache. L3 cache and main memory are shared between all cores of a CPU.", "cite_spans": [], "ref_spans": [], "section": "Measurement Approach"}, {"text": "The 4-NUMA-node system is a quad-socket system equipped with four Intel Xeon E7-4830 v4 CPUs (Haswell) which are connected via QPI. Each CPU consists of 14 two-way SMT-cores, meaning, 28 SMT-threads. Hence, the system provides 112 SMT-threads overall. Each core owns an L1 data and instruction cache with 32 kB each. Furthermore, it owns an L2 cache with 256 kB. L3 cache and main memory are shared between all cores of a CPU.", "cite_spans": [], "ref_spans": [], "section": "Measurement Approach"}, {"text": "During the measurements Intel's Turbo Boost was disabled. Turbo Boost is a hardware feature that accelerates applications by varying clock frequencies dependent on the number of active cores and the workload. Even though Turbo Boost is a valuable, runtime-saving feature in production runs, it distorts scaling measurements by boosting the sequential run through a higher clock frequency.", "cite_spans": [], "ref_spans": [], "section": "Measurement Approach"}, {"text": "The runtime measurements are performed with high resolution clock from std::chrono. FMSolvr was executed 1000\u00d7 for each measuring point, with each execution covering the whole workflow of the FMM for a single time step of the simulation. Afterwards, the 75%-quantile of these measuring points was computed. This means that 75% of the measured runtimes were below the plotted value. This procedure leads to stable timing results since the influence of clock frequency variations during the starting phase of the measurements is eliminated.", "cite_spans": [], "ref_spans": [], "section": "Measurement Approach"}, {"text": "As input data set a homogeneous particle ensemble with only a thousand particles is used. The values of positions and charges of the particles are defined by random numbers in range [0, 1). The measurements are performed with multipole order p = 0 and tree depth d = 3. Due to the small input data set and the chosen FMM parameters, the computational effort is very low. With this setup, FMSolvr tends to be latency-and synchronization-critical. Hence, this setup is most suitable to analyze the influence of NUMA-effects on applications that aim for strong scaling and small computational effort per compute node.", "cite_spans": [], "ref_spans": [], "section": "Measurement Approach"}, {"text": "We consider the runtime plot of FMSolvr in Fig. 4 (top) to analyze the impact of the NUMA-aware thread pinning policies EP and CP on the 2-NUMA-node system without applying a NUMA-aware work stealing. It can be seen that both policies lead to an increase in runtime in comparison to the non-NUMA-aware base implementation for the vast majority of cases. The most considerable runtime improvement occurs for pinning policy CP at #Threads = 12 with a speed-up of 1.6. The reason for this is that data transfer via the NUMA interconnect is fully avoided by CP since all threads fit on a single NUMA node for #Threads \u2264 12. Nevertheless, the best runtime is not reached for 12, but for 47 threads with policy CP. Hence, the practically relevant speed-up in comparison to the best runtime with the base implementation is 1.19. Figure 4 (bottom) shows the runtime plot of FMSolvr with the thread pinning policies EP and CP on the 4-NUMA-node system without applying NUMAaware work stealing. Here, too, the most considerable runtime improvement is reached for pinning policy CP if all cores of a single NUMA node are in use. Accordingly, the highest speed-up on the 4-NUMA-node system is reached at #Threads = 14 with a value of 2.1. In this case, none of the NUMA-aware implementations of FMSolvr outperforms the base implementation. Even though the best runtime is reached by the base implementation at #Threads = 97 with 1.77 ms, we get close to this runtime with 1.83 ms using only 14 threads. Hence, we can save compute resources and energy by applying CP. Figure 5 (bottom) shows the runtime plot of FMSolvr on the 4-NUMA-node for CP in combination with the work stealing policies LR, PL and LO. As already observed for the NUMA-aware thread pinning policies in Sect. 5.2, none of the implemented NUMA-awareness policies outperforms the minimal runtime of the base implementation with 1.76 ms. Hence, supplementing the NUMAaware thread pinning policies with NUMA-aware work stealing policies is not sufficient and there is still room for the improvements described in Sect. 7. ", "cite_spans": [], "ref_spans": [{"start": 43, "end": 49, "text": "Fig. 4", "ref_id": "FIGREF4"}, {"start": 822, "end": 830, "text": "Figure 4", "ref_id": "FIGREF4"}, {"start": 1555, "end": 1563, "text": "Figure 5", "ref_id": null}], "section": "NUMA-Aware Thread Pinning"}, {"text": "We evaluate performance portability based on Definition 2 with two different performance efficiency metrics e: architectural efficiency for \u03a6 arch and strong scaling efficiency for \u03a6 scal . Assumptions regarding \u03a6 arch : The theoretical double precision peak performance P 2NN of the 2-NUMA-node system is 960 GFLOPS (2 sockets with 480 GFLOPS [9] each), while the theoretical peak performance P 4NN of the 4-NUMA-node system is 1792 GFLOPS (4 sockets with 448 GFLOPS [9] each). With the considered input data set, FMSolvr executes 2.4 Million floating point operations per time step.", "cite_spans": [{"start": 344, "end": 347, "text": "[9]", "ref_id": null}, {"start": 468, "end": 471, "text": "[9]", "ref_id": null}], "ref_spans": [], "section": "Performance Portability"}, {"text": "Assumptions regarding \u03a6 scal : The considered strong scaling efficiency is for each platform and application determined for the lowest runtime and the according amount of threads for which this runtime is reached.", "cite_spans": [], "ref_spans": [], "section": "Performance Portability"}, {"text": "As can be seen from Table 1 the determined performance portability varies greatly dependent on the applied performance efficiency metric. However, the NUMA Plug-In increases performance portability in both cases.", "cite_spans": [], "ref_spans": [{"start": 20, "end": 27, "text": "Table 1", "ref_id": "TABREF2"}], "section": "Performance Portability"}, {"text": "Even though we aimed at a well-defined description of our main research objective -a sustainable support of NUMA architectures for FMSolvr, the evaluation of this objective is subject to several limitations. We did not fully prove that FMSolvr and the presented NUMA module are sustainable since we analyzed only performance and performance portability in a quantitative way. In extending this work, we should quantitatively analyze maintainability, modifiability and evolvability as the remaining properties of sustainability according to Definition 1.", "cite_spans": [], "ref_spans": [], "section": "Threats to Validity"}, {"text": "Our results regarding the evaluation of the NUMA module are not generalizable to its use in other applications or on other NUMA architectures since we so far tested it for FMSolvr on a limited set of platforms only. Hence, the NUMA module should be applied and evaluated within further applications and on further platforms.", "cite_spans": [], "ref_spans": [], "section": "Threats to Validity"}, {"text": "The chosen input data set is very small and exhibits a very low amount of FLOPs to analyze the parallelization overhead of Eventify and the occurring NUMA effects. For lack of a performance metric for latency-and synchronizationcritical applications, we applied double precision peak performance as reference value to preserve comparability with compute-bound inputs and applications. However, Eventify FMSolvr is not driven by FLOPs, e.g. it does not yet make explicit use of vectorization. In extending this work, we should reconsider the performance efficiency metrics applied to evaluate performance portability.", "cite_spans": [], "ref_spans": [], "section": "Threats to Validity"}, {"text": "Based on the properties of NUMA systems and the FMM, we described NUMAaware data distribution and work stealing policies with respect to [13] . Furthermore, we present the NUMA-aware thread pinning policy CP based on the performance analysis provided in [13] .", "cite_spans": [{"start": 137, "end": 141, "text": "[13]", "ref_id": "BIBREF13"}, {"start": 254, "end": 258, "text": "[13]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Conclusion and Future Work"}, {"text": "We found that the minimal runtime of FMSolvr is reached on the 2-NUMAnode system when thread pinning policy CP is applied in combination with work stealing policy LO. The minimal runtime is then 1.16 ms and is reached for 48 threads. However, none of the described NUMA-awareness policies outperforms the non-NUMA-aware base implementation on the 4-NUMA-node system. This is unexpected and needs further investigation since the performance analysis on another NUMA-node system provided in [13] revealed that NUMA-awareness leads to increasing speed-ups with an increasing number of NUMA nodes. Nevertheless, we can save compute resources and energy by applying CP since the policy leads to a runtime close to the minimal one with considerably less cores.", "cite_spans": [{"start": 489, "end": 493, "text": "[13]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Conclusion and Future Work"}, {"text": "Next up on our agenda is the implementation of a NUMA-aware pool allocator, the determination of more accurate NUMA distance information and the implementation of a more balanced task graph partitioning. In this article, suitable software design paid off regarding development time and application performance. Hence, we aim at further decoupling of the parallelization layer Eventify and its modules to be reusable by other research software engineers.", "cite_spans": [], "ref_spans": [], "section": "Conclusion and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Performance evaluation of computation and communication kernels of the fast multipole method on intel manycore architecture", "authors": [{"first": "M", "middle": [], "last": "Abduljabbar", "suffix": ""}, {"first": "M", "middle": [], "last": "Al Farhan", "suffix": ""}, {"first": "R", "middle": [], "last": "Yokota", "suffix": ""}, {"first": "D", "middle": [], "last": "Keyes", "suffix": ""}], "year": 2017, "venue": "LNCS", "volume": "10417", "issn": "", "pages": "553--564", "other_ids": {"DOI": ["10.1007/978-3-319-64203-1_40"]}}, "BIBREF1": {"ref_id": "b1", "title": "Task-based fast multipole method for clusters of multicore processors", "authors": [{"first": "E", "middle": [], "last": "Agullo", "suffix": ""}, {"first": "B", "middle": [], "last": "Bramas", "suffix": ""}, {"first": "O", "middle": [], "last": "Coulaud", "suffix": ""}, {"first": "M", "middle": [], "last": "Khannouz", "suffix": ""}, {"first": "L", "middle": [], "last": "Stanisic", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Taskbased FMM for multicore architectures", "authors": [{"first": "E", "middle": [], "last": "Agullo", "suffix": ""}, {"first": "B", "middle": [], "last": "Bramas", "suffix": ""}, {"first": "O", "middle": [], "last": "Coulaud", "suffix": ""}, {"first": "E", "middle": [], "last": "Darve", "suffix": ""}, {"first": "M", "middle": [], "last": "Messner", "suffix": ""}, {"first": "T", "middle": [], "last": "Takahashi", "suffix": ""}], "year": 2014, "venue": "SIAM J. Sci. Comput", "volume": "36", "issn": "1", "pages": "66--93", "other_ids": {"DOI": ["10.1137/130915662"]}}, "BIBREF3": {"ref_id": "b3", "title": "Scaling FMM with data-driven OpenMP tasks on multicore architectures", "authors": [{"first": "A", "middle": [], "last": "Amer", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "A short course on fast multipole methods", "authors": [{"first": "R", "middle": [], "last": "Beatson", "suffix": ""}, {"first": "L", "middle": [], "last": "Greengard", "suffix": ""}], "year": 1997, "venue": "Wavelets Multilevel Methods Elliptic PDEs", "volume": "1", "issn": "", "pages": "1--37", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "A fast algorithm for particle simulations", "authors": [{"first": "L", "middle": [], "last": "Greengard", "suffix": ""}, {"first": "V", "middle": [], "last": "Rokhlin", "suffix": ""}], "year": 1987, "venue": "J. Comput. Phys", "volume": "73", "issn": "2", "pages": "90140--90149", "other_ids": {"DOI": ["10.1016/0021-9991(87)90140-9"]}}, "BIBREF7": {"ref_id": "b7", "title": "A C++-based MPI-enabled tasking framework to efficiently parallelize fast multipole methods for molecular", "authors": [{"first": "D", "middle": [], "last": "Haensel", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Eventify: event-based task parallelism for strong scaling", "authors": [{"first": "D", "middle": [], "last": "Haensel", "suffix": ""}, {"first": "L", "middle": [], "last": "Morgenstern", "suffix": ""}, {"first": "A", "middle": [], "last": "Beckmann", "suffix": ""}, {"first": "I", "middle": [], "last": "Kabadshow", "suffix": ""}, {"first": "H", "middle": [], "last": "Dachsel", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Periodic boundary conditions and the error-controlled fast multipole method", "authors": [{"first": "I", "middle": [], "last": "Kabadshow", "suffix": ""}], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Sustainability evaluation of software architectures: a systematic review", "authors": [{"first": "H", "middle": [], "last": "Koziolek", "suffix": ""}], "year": 2011, "venue": "Proceedings of the Joint ACM SIGSOFT Conference", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1145/2000259.2000263"]}}, "BIBREF12": {"ref_id": "b12", "title": "NUMA (Non-Uniform Memory Access): an overview", "authors": [{"first": "C", "middle": [], "last": "Lameter", "suffix": ""}], "year": 2013, "venue": "Queue", "volume": "11", "issn": "7", "pages": "", "other_ids": {"DOI": ["10.1145/2508834.2513149"]}}, "BIBREF13": {"ref_id": "b13", "title": "A NUMA-aware task-based load-balancing scheme for the fast multipole method", "authors": [{"first": "L", "middle": [], "last": "Morgenstern", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "A metric for performance portability", "authors": [{"first": "S", "middle": ["J"], "last": "Pennycook", "suffix": ""}, {"first": "J", "middle": ["D"], "last": "Sewall", "suffix": ""}, {"first": "V", "middle": ["W"], "last": "Lee", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "A kernel-independent adaptive fast multipole algorithm in two and three dimensions", "authors": [{"first": "L", "middle": [], "last": "Ying", "suffix": ""}, {"first": "G", "middle": [], "last": "Biros", "suffix": ""}, {"first": "D", "middle": [], "last": "Zorin", "suffix": ""}], "year": 2004, "venue": "J. Comput. Phys", "volume": "196", "issn": "", "pages": "591--626", "other_ids": {"DOI": ["10.1016/j.jcp.2003.11.021"]}}}, "ref_entries": {"FIGREF0": {"text": "Parent-Child Relation: We refer to box x as parent box of box y if x and y are directly connected when moving towards the root of the tree. -Near Neighbor: We refer to two boxes as near neighbors if they are at the same refinement level and share a boundary point. -Interaction Set: We refer to the interaction set of box i as the set consisting of the children of the near neighbors of i's parent box which are well separated from i.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Exemplary horizontal and vertical data dependencies that lead to inter-node data transfer.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Layer-based software architecture of FMSolvr. The light gray layer shows an excerpt of the UML-diagram of the algorithm layer that encapsulates the algorithmic details of the FMM. The dark gray layer provides an excerpt of the UML-diagram of the parallelization layer that encapsulates hardware and parallelization details. Both layers are coupled by using the interfaces FMMHandle and TaskingHandle. (Color figure online)", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Software architecture of the NUMA module and its connection to FMMHandle and TaskingHandle.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Comparison of NUMA-aware thread pinning policies EP and CP with work stealing policy LR on 2-NUMA-node and 4-NUMA-node system. NUMA-Aware Work Stealing. Figure 5 (top) shows the runtime of FMSolvr for CP in combination with the work stealing policies LR, PL and LO on the 2-NUMA-node system dependent on the number of threads. The minimal runtime of FMSolvr is reached with 1.16 ms for 48 threads if CP is applied in combination with LO. The practically relevant speed-up in comparison with the minimal runtime of the base implementation is 21%.", "latex": null, "type": "figure"}, "TABREF1": {"text": "Fig. 5. Comparison of NUMA-aware work stealing policies LR, PL and LO based on NUMA-aware thread pinning policy CP on a 2-NUMA-node and 4-NUMA-node system", "latex": null, "type": "table"}, "TABREF2": {"text": "Performance portabilities \u03a6 arch and \u03a6 scal for Eventify FMSolvr with and without NUMA Plug-In. \u03a6 arch \u03a6 scal FMSolvr without NUMA Plug-In 0.10% 11.22% FMSolvr with NUMA Plug-In 0.11% 30.41%", "latex": null, "type": "table"}}, "back_matter": []}