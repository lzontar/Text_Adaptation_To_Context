{"paper_id": "0ca41c96d6b58e27160d08673f76012fa4f47b02", "metadata": {"title": "Vision-Based Blind Spot Warning System by Deep Neural Networks", "authors": [{"first": "V\u00edctor", "middle": ["R"], "last": "Virgilio", "suffix": "", "affiliation": {"laboratory": "", "institution": "Av. Juan de Dios Batiz S/N", "location": {"postCode": "07738", "settlement": "Gustavo A. Madero, Ciudad de Mexico", "country": "Mexico"}}, "email": "vvirgiliog@gmail.com"}, {"first": "G", "middle": [], "last": "", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Humberto", "middle": [], "last": "Sossa", "suffix": "", "affiliation": {"laboratory": "", "institution": "Av. Juan de Dios Batiz S/N", "location": {"postCode": "07738", "settlement": "Gustavo A. Madero, Ciudad de Mexico", "country": "Mexico"}}, "email": "hsossa@cic.ipn.mx"}, {"first": "Erik", "middle": [], "last": "Zamora", "suffix": "", "affiliation": {"laboratory": "", "institution": "Av. Juan de Dios Batiz S/N", "location": {"postCode": "07738", "settlement": "Gustavo A. Madero, Ciudad de Mexico", "country": "Mexico"}}, "email": "ezamorag@ipn.mx"}]}, "abstract": [{"text": "Traffic accidents represent one of the most serious problems around the world. Many efforts have been concentrated on implementing Advanced Driver Assistance Systems (ADAS) to increase safety by reducing critical tasks faced by the driver. In this paper, a Blind Spot Warning (BSW) system capable of virtualizing cars around the driver's vehicle is presented. The system is based on deep neural models for car detection and depth estimation using images captured with a camera located on top of the main vehicle, then transformations are applied to the image and to generate the appropriate information format. Finally the cars in the environment are represented in a 3D graphical interface. We present a comparison between car detectors and another one between depth estimators from which we choose the best performance ones to be implemented in the BSW system. In particular, our system offers a more intuitive assistance interface for the driver allowing a better and quicker understanding of the environment from monocular cameras.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Transport can be involved in daily traffic accidents which are one of the most serious problems currently facing modern societies. According to 2017 figures from the World Health Organization (WHO), each year around 1.3 million people die in road accidents worldwide, and between 20 and 50 million suffer non-fatal injuries that cause disabilities [17] . According to data from the National Institute of Public Health (INSP), Mexico ranks seventh in the world and third in the Latin American region in terms of road deaths, with 22 deaths of young people between 15 and 29 years of age per day [9] .", "cite_spans": [{"start": 348, "end": 352, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 594, "end": 597, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Introduction"}, {"text": "Road specialists and road safety experts report that behind every vehicle accident the human factor is involved in 90% [6] . So for several years, car manufacturers have implemented technologies such as ADAS which assist the driver in the driving process. ADAS goal is to increase automobile safety and road safety in general using Human-Machine Interfaces (HMI). These systems use multiple sensors (radar, lidar, camera, GPS, etc.) to identify the environment with which the vehicle interacts.", "cite_spans": [{"start": 119, "end": 122, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Introduction"}, {"text": "When driving a vehicle the driver depends on rear-view mirrors and body movements to observe other vehicles approaching, however, this practice represents risks due to the generation of areas where vision is partially or completely occluded, these areas are called \"blind spots\". Due to the large number of accidents caused by this situation, BSW systems have been developed which provide the driver with information about the vehicles around him to avoid possible collisions.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "This document is organized as follows. Section 2 describes details of the existing State-of-the-Art (SOTA) of BSW systems and the type of processing they perform. Section 3 presents the different techniques and technologies implemented in the proposed BSW system, such as the neural models, the applied transformations and the visualization platform used. Section 4 shows the results obtained qualitatively and quantitatively from the implemented technologies. Section 5 the conclusions obtained with the development of this work are presented.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Although the objective is the same (alerting the driver to the presence of vehicles in occlusion areas) BSW systems can be developed from different technologies and implement different sensors such as: ultrasonic, optical, radar, cameras, etc; in addition, they can provide visual (e.g. outside image), audio (e.g. voice prompt) or tactile (e.g. steering wheel vibration) information to indicate that it is not safe to change lanes. Typically there are two basic approaches to obtain and processing information: range-based and vision-based.", "cite_spans": [], "ref_spans": [], "section": "Related Work"}, {"text": "Works such as presented in [14, 16, 22, 23] describe range-based systems that implement ultrasonic or radar devices mounted around the vehicle to estimate the distance of approaching objects, subsequently alert the driver by means of indicators on the side mirrors.", "cite_spans": [{"start": 27, "end": 31, "text": "[14,", "ref_id": "BIBREF13"}, {"start": 32, "end": 35, "text": "16,", "ref_id": "BIBREF15"}, {"start": 36, "end": 39, "text": "22,", "ref_id": "BIBREF21"}, {"start": 40, "end": 43, "text": "23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Related Work"}, {"text": "Vision-based systems aim to obtain information from the environment using cameras and then perform image analysis for obstacles detection while driving. Most BSW systems employ classic image processing techniques for their development. Such as [3, 13, 18, 19, 23] histogram of oriented gradients (HOG), filters for edge detection, entropy, optical flow, Gabor's filter, among others are used to extract useful information and techniques such as clustering and vector support machines [13, 18] to classify where vehicles are. In [11] the concept of depth estimation is implemented to determine whether a vehicle is near or far from the driver's vehicle, they make use of features such as texture and blur in the image, and techniques such as principal component analysis (PCA) and discrete cosine transformation.", "cite_spans": [{"start": 244, "end": 247, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 248, "end": 251, "text": "13,", "ref_id": "BIBREF12"}, {"start": 252, "end": 255, "text": "18,", "ref_id": "BIBREF17"}, {"start": 256, "end": 259, "text": "19,", "ref_id": "BIBREF18"}, {"start": 260, "end": 263, "text": "23]", "ref_id": "BIBREF22"}, {"start": 484, "end": 488, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 489, "end": 492, "text": "18]", "ref_id": "BIBREF17"}, {"start": 528, "end": 532, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Related Work"}, {"text": "In recent years, neural models have been implemented for the classification and detection of objects in images due to good performance obtained. In [15, 21] fully connected neural networks (FCN) are used for vehicle detection in blind spot areas, in addition to techniques such as HOG, heat mapping and threshold levels for pre-processing of images.", "cite_spans": [{"start": 148, "end": 152, "text": "[15,", "ref_id": "BIBREF14"}, {"start": 153, "end": 156, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Related Work"}, {"text": "Other types of BSW systems have been developed with more complex neural models; such is the case of [26] where first the objects are located by classic image segmentation, then the candidates are classified with a Convolutional Neural Network (CNN) and the vehicle is tracked using optical flow analysis. On the other hand in [27] blind spot vehicles are treated as a classification problem in which a CNN takes full responsibility for classifying whether or not a vehicle exists in the predetermined area.", "cite_spans": [{"start": 100, "end": 104, "text": "[26]", "ref_id": "BIBREF25"}, {"start": 326, "end": 330, "text": "[27]", "ref_id": "BIBREF26"}], "ref_spans": [], "section": "Related Work"}, {"text": "Lastly, in [19] a BSW system is developed implementing multi-object tracking (MOT) from a fusion of sensors, including cameras, LIDAR, among others; in addition, techniques such as decision by Markov models and reinforcement learning for information processing are applied.", "cite_spans": [{"start": 11, "end": 15, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Related Work"}, {"text": "We propose a BSW system capable of providing a driver assistance interface that virtualizes the cars around him on a 3D platform. The system contains (i) a neuronal model for car detection, (ii) a neuronal model for depth estimation, (iii) a processing module to generate car location and (iv) a graphical interface module to visualize the cars, as illustrated in Fig. 1 .", "cite_spans": [], "ref_spans": [{"start": 364, "end": 370, "text": "Fig. 1", "ref_id": null}], "section": "Proposed Method"}, {"text": "The presented system was implemented using monocular images from the KITTI database [8] . KITTI provides stereoscopic images (1242 \u00d7 375) of front view using cameras mounted on top of the vehicle at a rate of 10 frames per second. All scenes are recorded in similar weather conditions during the day.", "cite_spans": [{"start": 84, "end": 87, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Proposed Method"}, {"text": "For car detection in the images, two very popular neural architectures were tested: YOLOv3 and Detectron2.", "cite_spans": [], "ref_spans": [], "section": "Car Detection"}, {"text": "YOLOv3 [20] is a neural model for object detection that processes approximately 30 images per second in COCO test-set obtaining an average precision of 33% and consists of 53 convolutional layers (Darknet 53). This model has several advantages over systems based on classifiers and sliding window, for example, it examines the entire image at the time of inference so that predictions have information about the overall context of the image. In addition, it develops the predictions with a single evaluation of the image which makes it a very fast network.", "cite_spans": [{"start": 7, "end": 11, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Car Detection"}, {"text": "Detectron2 is a neural model developed by Facebook AI Research that implements SOTA object detection algorithms. It is a rewrite of the previous version, Detectron, and originates from the benchmark Mask R-CNN [12] . The average precision of this model is 39.8% obtained in COCO test-set. Fig. 1 . Proposed system diagram. The images are passed through the detector to infer areas where there are cars, subsequently distance is estimated in the previously detected areas using the neuronal model (depth, Z axis) and the BEV transformation (horizontal, X axis). Later, the information is given to the 3D graphical interface to visualize the cars.", "cite_spans": [{"start": 210, "end": 214, "text": "[12]", "ref_id": "BIBREF11"}], "ref_spans": [{"start": 289, "end": 295, "text": "Fig. 1", "ref_id": null}], "section": "Car Detection"}, {"text": "Considering that car detecting in images does not give us clear information about the distance they are, which is fundamental for the understanding of a scene, a single-image depth estimation (SIDE) has been implemented to know the distance in the Z-axis (deep). Different neuronal models were considered.", "cite_spans": [], "ref_spans": [], "section": "Depth Estimation"}, {"text": "DenseDepth [2] is a model that consists of a convolutional neural network for computing a high-resolution depth map given a single RGB image. Following a standard encoder-decoder architecture, they leverage features extracted using high performing pre-trained networks when initializing the encoder along with augmentation and training strategies that lead to more accurate results.", "cite_spans": [{"start": 11, "end": 14, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Depth Estimation"}, {"text": "MonoDepth2 [10] is a depth estimation network is based on the general U-Net architecture with skip connections, enabling to represent both deep abstract features as well as local information. They use a ResNet18 as encoder, unlike the larger and slower DispNet and ResNet50 models used in existing SOTA.", "cite_spans": [{"start": 11, "end": 15, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Depth Estimation"}, {"text": "monoResMatch [24] is a deep architecture designed to infer depth from a single input image by synthesizing features from a different point of view, horizontally aligned with the input image, performing stereo matching between the two cues. In contrast to previous works sharing this rationale, this network is the first trained end-to-end from scratch.", "cite_spans": [{"start": 13, "end": 17, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Depth Estimation"}, {"text": "Following the steps described in [25] , and using OpenCV, we apply a bird's eye view transformation (BEV) to estimate the distance of the vehicles in the X axis (horizontal). Then we organize and give the detections and estimated distances to virtualization platform.", "cite_spans": [{"start": 33, "end": 37, "text": "[25]", "ref_id": "BIBREF24"}], "ref_spans": [], "section": "Car Location"}, {"text": "In this module, we generate a 3D graphical interface to achieve a more natural and intuitive interface for the driver. Unlike the typical 2D interface, which BEV is presented, UBER's interface [1] virtualizes the cars in 3D which represents an environment similar to the one humans face daily, so it directly impacts on the speed of assimilation/interpretation of the environment.", "cite_spans": [{"start": 193, "end": 196, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "3D Graphical Interface"}, {"text": "In this section we present the qualitative and quantitative results obtained by the neuronal models for car detection and depth estimation, as well as ones obtained by the BEV transformation. In addition, the final results of the BSW system are shown through the interface generated by the UBER platform.", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "Although the BSW system aims to process complete information on the environment around the vehicle, the results presented are the first tests carried out using the KITTI database. However, the system could be evaluated with another database that offers images of the complete environment using cameras located at different points of the vehicle as well as scenes recorded in more challenging weather conditions. This work aims to demonstrate the feasibility of using deep neural models in BSW systems, the experiments were individually carried out offline using a Tesla P100 (16 GB) GPU.", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "Car Detection. Following [4] , we evaluate car detectors using 3,769 images for validation set at KITTI 2D detection benchmark [8] . Evaluation is done for car class in three regimes: Easy, Moderate and Hard, which contain objects of different box sizes, and different levels of occlusion and truncation. The results in Table 1 show that, in general, car detection is feasible even in high complexity situations such as moderate and hard KITTI levels, with 0.07 s for images with few detected cars (less than 5) and 0.1 s for many cars (more than 10). Figure 2 shows some results of detectors in the validation set.", "cite_spans": [{"start": 25, "end": 28, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 127, "end": 130, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [{"start": 320, "end": 327, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 552, "end": 560, "text": "Figure 2", "ref_id": null}], "section": "Results"}, {"text": "The main reason why the AP is below 50% is because both models have not been retrained in the KITTI database; instead, these models have been pre-trained in the COCO database with almost 100 classes. In addition, both neural models are the most popular and intuitive to implement but not the best performing in the SOTA. Based on the experimental results we conclude that Detectron2 is a better choice for this type of problem in a BSW system. Depth Estimation. SOTA single-image depth estimators were compared in KITTI's benchmark [7] . Table 2 shows that neural models compared present a very good and similar performance, which demonstrates that they are a good alternative to the problem of depth estimation; it is worth mentioning that MonoDepth2 processes information in a considerably less amount of time than the other methods, which would be important when testing the system on embedded hardware. Implementing SOTA depth estimation models allows us to obtain more precise information about the location of previously detected trolleys. Based on the experimental results we conclude that DenseDepth is the best option to depth estimation problem in a BSW. Figure 3 shows some results of depth estimators. BEV Transformation. Some results of the BEV transform are presented in Fig. 4 . Later, the information was organized to be sent to the graphic interface platform. Blind Spot Warning System. To test the BSW system we use the previously chosen neural models, then we apply the BEV transformation and give the data to the UBER platform to generate the 3D graphical interface. Figure 5 shows the result of the BSW system, where different 3D views generated by the graphical interface are displayed in addition to the indication (by color) of the closest cars, which offers greater assistance and comfort to the driver in terms of how he or she perceives the environment. ", "cite_spans": [{"start": 532, "end": 535, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [{"start": 538, "end": 545, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 1165, "end": 1173, "text": "Figure 3", "ref_id": null}, {"start": 1285, "end": 1291, "text": "Fig. 4", "ref_id": "FIGREF0"}, {"start": 1587, "end": 1595, "text": "Figure 5", "ref_id": "FIGREF1"}], "section": "Results"}, {"text": "A single-image BSW system was developed based on artificial intelligence technologies such as neural models for object detection and depth estimation. In addition, the visualization system development on a 3D graphics platform offers the driver a much more intuitive interface than SOTA BSW systems and presents a much faster way to understand the behavior of the vehicles around.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "This work shows a BSW system with complex interfaces through the unique use of vision sensors, which represents a cost reduction compared to range-based systems that employ sensors such as LIDAR or radar. Finally, the presented system contributes to the approach of understanding the scene, since it offers an alternative of car virtualization that works as a reference for the perception of the environment.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Introducing AVS, an open standard for autonomous vehicle visualization from uber. Accessed", "authors": [{"first": "X", "middle": [], "last": "Chen", "suffix": ""}, {"first": "J", "middle": [], "last": "Lisee", "suffix": ""}, {"first": "T", "middle": [], "last": "Wojtaszek", "suffix": ""}, {"first": "A", "middle": [], "last": "Gupta", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "High Quality Monocular Depth Estimation via Transfer Learning", "authors": [{"first": "I", "middle": [], "last": "Alhashim", "suffix": ""}, {"first": "P", "middle": [], "last": "Wonka", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "A blind spot detection warning system based on gabor filtering and optical flow for e-mirror applications", "authors": [{"first": "S", "middle": ["M"], "last": "Chang", "suffix": ""}, {"first": "C", "middle": ["C"], "last": "Tsai", "suffix": ""}, {"first": "J", "middle": ["I"], "last": "Guo", "suffix": ""}], "year": 2018, "venue": "Proceedings -IEEE International Symposium on Circuits and Systems 2018-May", "volume": "", "issn": "", "pages": "1--5", "other_ids": {"DOI": ["10.1109/ISCAS.2018.8350927"]}}, "BIBREF3": {"ref_id": "b3", "title": "3D object proposals using stereo imagery for accurate object class detection", "authors": [{"first": "X", "middle": [], "last": "Chen", "suffix": ""}, {"first": "K", "middle": [], "last": "Kundu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "H", "middle": [], "last": "Ma", "suffix": ""}, {"first": "S", "middle": [], "last": "Fidler", "suffix": ""}, {"first": "R", "middle": [], "last": "Urtasun", "suffix": ""}], "year": 2018, "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "volume": "40", "issn": "5", "pages": "1259--1272", "other_ids": {"DOI": ["10.1109/TPAMI.2017.2706685"]}}, "BIBREF4": {"ref_id": "b4", "title": "Depth map prediction from a single image using a multi-scale deep network", "authors": [{"first": "D", "middle": [], "last": "Eigen", "suffix": ""}, {"first": "C", "middle": [], "last": "Puhrsch", "suffix": ""}, {"first": "R", "middle": [], "last": "Fergus", "suffix": ""}], "year": 2014, "venue": "Adv. Neural Inf. Process. Syst", "volume": "3", "issn": "", "pages": "2366--2374", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Las causas m\u00e1s comunes en accidentes de tr\u00e1nsito -Seguridad Vial. Accessed", "authors": [{"first": "Carlos", "middle": [], "last": "Fundaci\u00f3n", "suffix": ""}, {"first": "", "middle": [], "last": "Slim", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Vision meets robotics: The KITTI dataset", "authors": [{"first": "A", "middle": [], "last": "Geiger", "suffix": ""}, {"first": "P", "middle": [], "last": "Lenz", "suffix": ""}, {"first": "C", "middle": [], "last": "Stiller", "suffix": ""}, {"first": "R", "middle": [], "last": "Urtasun", "suffix": ""}], "year": 2013, "venue": "Int. J. Robot. Res", "volume": "32", "issn": "11", "pages": "1231--1237", "other_ids": {"DOI": ["10.1177/0278364913491297"]}}, "BIBREF7": {"ref_id": "b7", "title": "Are we ready for autonomous driving? the KITTI vision benchmark suite", "authors": [{"first": "A", "middle": [], "last": "Geiger", "suffix": ""}, {"first": "P", "middle": [], "last": "Lenz", "suffix": ""}, {"first": "R", "middle": [], "last": "Urtasun", "suffix": ""}], "year": 2012, "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "volume": "", "issn": "", "pages": "3354--3361", "other_ids": {"DOI": ["10.1109/CVPR.2012.6248074"]}}, "BIBREF8": {"ref_id": "b8", "title": "Accidentes viales, primera causa de muerte en los j\u00f3venes. Accessed", "authors": [{"first": "M\u00e9xico", "middle": [], "last": "Gobierno De", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Digging into self-supervised monocular depth estimation", "authors": [{"first": "C", "middle": [], "last": "Godard", "suffix": ""}, {"first": "O", "middle": [], "last": "Mac Aodha", "suffix": ""}, {"first": "M", "middle": [], "last": "Firman", "suffix": ""}, {"first": "G", "middle": [], "last": "Brostow", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "3828--3838", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Blind spot obstacle detection from monocular camera images with depth cues extracted by CNN", "authors": [{"first": "Y", "middle": [], "last": "Guo", "suffix": ""}, {"first": "I", "middle": [], "last": "Kumazawa", "suffix": ""}, {"first": "C", "middle": [], "last": "Kaku", "suffix": ""}], "year": 2018, "venue": "Automot. Innov", "volume": "1", "issn": "4", "pages": "362--373", "other_ids": {"DOI": ["10.1007/s42154-018-0036-6"]}}, "BIBREF11": {"ref_id": "b11", "title": "Mask R-CNN", "authors": [{"first": "K", "middle": [], "last": "He", "suffix": ""}, {"first": "G", "middle": [], "last": "Gkioxari", "suffix": ""}, {"first": "P", "middle": [], "last": "Doll\u00e1r", "suffix": ""}, {"first": "R", "middle": [], "last": "Girshick", "suffix": ""}], "year": 2020, "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "volume": "42", "issn": "2", "pages": "386--397", "other_ids": {"DOI": ["10.1109/TPAMI.2018.2844175"]}}, "BIBREF12": {"ref_id": "b12", "title": "Vision-based blind spot monitoring using rear-view camera and its real-time implementation in an embedded system", "authors": [{"first": "K", "middle": ["H"], "last": "Jung", "suffix": ""}, {"first": "K", "middle": [], "last": "Yi", "suffix": ""}], "year": 2018, "venue": "J. Comput. Sci. Eng", "volume": "12", "issn": "3", "pages": "127--138", "other_ids": {"DOI": ["10.5626/JCSE.2018.12.3.127"]}}, "BIBREF13": {"ref_id": "b13", "title": "Prevention device for blind spot accident detection and protection", "authors": [{"first": "P", "middle": [], "last": "Kedarkar", "suffix": ""}, {"first": "M", "middle": [], "last": "Chaudhari", "suffix": ""}, {"first": "C", "middle": [], "last": "Dasarwar", "suffix": ""}, {"first": "P", "middle": ["B"], "last": "Domakondwar", "suffix": ""}], "year": 2019, "venue": "Int. Res. J. Eng. Technol. (IRJET)", "volume": "6", "issn": "1", "pages": "624--627", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "A study on development of the camera-based blind spot detection system using the deep learning methodology", "authors": [{"first": "D", "middle": [], "last": "Kwon", "suffix": ""}, {"first": "R", "middle": [], "last": "Malaiya", "suffix": ""}, {"first": "G", "middle": [], "last": "Yoon", "suffix": ""}, {"first": "J", "middle": ["T"], "last": "Ryu", "suffix": ""}, {"first": "S", "middle": ["Y"], "last": "Pi", "suffix": ""}], "year": 2019, "venue": "Appl. Sci", "volume": "9", "issn": "14", "pages": "", "other_ids": {"DOI": ["10.3390/app9142941"]}}, "BIBREF15": {"ref_id": "b15", "title": "A radar-based blind spot detection and warning system for driver assistance", "authors": [{"first": "G", "middle": [], "last": "Liu", "suffix": ""}, {"first": "L", "middle": [], "last": "Wang", "suffix": ""}, {"first": "S", "middle": [], "last": "Zou", "suffix": ""}], "year": 2017, "venue": "Proceedings of 2017 IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference", "volume": "", "issn": "", "pages": "2204--2208", "other_ids": {"DOI": ["10.1109/IAEAC.2017.8054409"]}}, "BIBREF16": {"ref_id": "b16", "title": "Accidentes de tr\u00e1nsito", "authors": [{"first": "Salud", "middle": [], "last": "Organizaci\u00f3n Mundial De La", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Part-based vehicle detection in siderectilinear images for blind-spot detection", "authors": [{"first": "M", "middle": [], "last": "Ra", "suffix": ""}, {"first": "H", "middle": ["G"], "last": "Jung", "suffix": ""}, {"first": "J", "middle": ["K"], "last": "Suhr", "suffix": ""}, {"first": "W", "middle": ["Y"], "last": "Kim", "suffix": ""}], "year": 2018, "venue": "Expert Syst. Appl", "volume": "101", "issn": "", "pages": "116--128", "other_ids": {"DOI": ["10.1016/j.eswa.2018.02.005"]}}, "BIBREF18": {"ref_id": "b18", "title": "No blind spots: full-surround multi-object tracking for autonomous vehicles using cameras & LiDARs", "authors": [{"first": "A", "middle": [], "last": "Rangesh", "suffix": ""}, {"first": "M", "middle": ["M"], "last": "Trivedi", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "1--12", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "YOLOv3: an incremental improvement", "authors": [{"first": "J", "middle": [], "last": "Redmon", "suffix": ""}, {"first": "A", "middle": [], "last": "Farhadi", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Framework of blind spot information system using feedforward neural networks", "authors": [{"first": "A", "middle": [], "last": "Rusiecki", "suffix": ""}, {"first": "P", "middle": [], "last": "Roma", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.13140/RG.2.1.3252.3921/1"]}}, "BIBREF21": {"ref_id": "b21", "title": "Semi-Truck Blind Spot Detection System Group", "authors": [{"first": "D", "middle": [], "last": "Sheets", "suffix": ""}], "year": 2016, "venue": "", "volume": "32", "issn": "", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Survey on blind spot detection and lane departure warning systems", "authors": [{"first": "P", "middle": [], "last": "Tigadi", "suffix": ""}, {"first": "P", "middle": ["B"], "last": "Gujanatti", "suffix": ""}, {"first": "R", "middle": [], "last": "Patil", "suffix": ""}], "year": 2015, "venue": "Int. J. Adv. Res. Eng", "volume": "2", "issn": "5", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Learning monocular depth estimation infusing traditional stereo knowledge", "authors": [{"first": "F", "middle": [], "last": "Tosi", "suffix": ""}, {"first": "F", "middle": [], "last": "Aleotti", "suffix": ""}, {"first": "M", "middle": [], "last": "Poggi", "suffix": ""}, {"first": "S", "middle": [], "last": "Mattoccia", "suffix": ""}], "year": 2019, "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2019-June", "volume": "", "issn": "", "pages": "9791--9801", "other_ids": {"DOI": ["10.1109/CVPR.2019.01003"]}}, "BIBREF24": {"ref_id": "b24", "title": "Distance determination for an automobile environment using inverse perspective mapping in OpenCV", "authors": [{"first": "S", "middle": [], "last": "Tuohy", "suffix": ""}, {"first": "D", "middle": [], "last": "O&apos;cualain", "suffix": ""}, {"first": "E", "middle": [], "last": "Jones", "suffix": ""}, {"first": "M", "middle": [], "last": "Glavin", "suffix": ""}], "year": 2010, "venue": "IET Conference Publications 2010(566 CP)", "volume": "", "issn": "", "pages": "100--105", "other_ids": {"DOI": ["10.1049/cp.2010.0495"]}}, "BIBREF25": {"ref_id": "b25", "title": "Overtaking vehicle detection techniques based on optical flow and convolutional neural network", "authors": [{"first": "L", "middle": ["T"], "last": "Wu", "suffix": ""}, {"first": "H", "middle": ["Y"], "last": "Lin", "suffix": ""}], "year": 2018, "venue": "VEHITS 2018 -Proceedings of the 4th International Conference on Vehicle Technology and Intelligent Transport Systems 2018-March(Vehits)", "volume": "", "issn": "", "pages": "133--140", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "Camera-based blind spot detection with a general purpose lightweight neural network", "authors": [{"first": "Y", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "L", "middle": [], "last": "Bai", "suffix": ""}, {"first": "Y", "middle": [], "last": "Lyu", "suffix": ""}, {"first": "X", "middle": [], "last": "Huang", "suffix": ""}], "year": 2019, "venue": "Electronics", "volume": "8", "issn": "2", "pages": "", "other_ids": {"DOI": ["10.3390/electronics8020233"]}}}, "ref_entries": {"FIGREF0": {"text": "BEV transformation results. The original image (first row) and Bird's Eye View transformation (second row) are presented.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "The original image (top left), driver's view (center left), bird's view (bottom left) and perspective view (right) generated in the graphical interface are presented.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Performance on KITTI validation set for Car class using the KITTI standar metric, Average Precision Metric (AP).", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>AP (%) </td><td>\u00a0</td><td>Time (s)\n</td></tr><tr><td>\u00a0</td><td>Easy </td><td>Moderate </td><td>Hard </td><td>min </td><td>max\n</td></tr><tr><td>YOLOv3 </td><td>47.3 </td><td>41.4 </td><td>30.4 </td><td>0.07 </td><td>0.1\n</td></tr><tr><td>Detectron2 </td><td>46.1 </td><td>49.5 </td><td>39.9 </td><td>0.07 </td><td>0.1\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Quantitative evaluation on the test set of KITTI dataset[7] using the standard six metrics used in[5], maximum depth: 80 m.Fig. 3. The original image (first row), depth estimation by DenseDepth (second row), depth estimation by MonoDepth2 (third row), depth estimation by MonoResMatch (fourth row) are presented. It is possible to observe that, in the case of defined shapes (such as cars and people), DenseDepth has a higher level of detail than the rest.", "latex": null, "type": "table"}}, "back_matter": []}