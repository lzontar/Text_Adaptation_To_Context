{"paper_id": "71e3321180fc3b618a4d87956fa1e7aab7c94d57", "metadata": {"title": "Discretization and Feature Selection Based on Bias Corrected Mutual Information Considering High-Order Dependencies", "authors": [{"first": "Puloma", "middle": [], "last": "Roy", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Dhaka", "location": {"settlement": "Dhaka", "country": "Bangladesh"}}, "email": "pulomaa92@gmail.com"}, {"first": "Sadia", "middle": [], "last": "Sharmin", "suffix": "", "affiliation": {"laboratory": "", "institution": "Islamic University of Technology", "location": {"settlement": "Gazipur City", "country": "Bangladesh"}}, "email": "sharmin@iut-dhaka.edu"}, {"first": "Amin", "middle": ["Ahsan"], "last": "Ali", "suffix": "", "affiliation": {"laboratory": "", "institution": "Independent University of Bangladesh", "location": {"settlement": "Dhaka", "country": "Bangladesh"}}, "email": "aminali@iub.edu.bd"}, {"first": "Mohammad", "middle": [], "last": "Shoyaib", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Dhaka", "location": {"settlement": "Dhaka", "country": "Bangladesh"}}, "email": "shoyaib@du.ac.bd"}]}, "abstract": [{"text": "Mutual Information (MI) based feature selection methods are popular due to their ability to capture the nonlinear relationship among variables. However, existing works rarely address the error (bias) that occurs due to the use of finite samples during the estimation of MI. To the best of our knowledge, none of the existing methods address the bias issue for the high-order interaction term which is essential for better approximation of joint MI. In this paper, we first calculate the amount of bias of this term. Moreover, to select features using \u03c7 2 based search, we also show that this term follows \u03c7 2 distribution. Based on these two theoretical results, we propose Discretization and feature Selection based on bias corrected Mutual information (DSbM). DSbM is extended by adding simultaneous forward selection and backward elimination (DSbM fb ). We demonstrate the superiority of DSbM over four state-of-the-art methods in terms of accuracy and the number of selected features on twenty benchmark datasets. Experimental results also demonstrate that DSbM outperforms the existing methods in terms of accuracy, Pareto Optimality and Friedman test. We also observe that compared to DSbM, in some dataset DSbM fb selects fewer features and increases accuracy.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "In classification tasks, the objective of feature selection (FS) process is to choose the most useful features that contribute to the prediction of class variable. Usually, all the features of a dataset do not have equal importance, rather some may Electronic supplementary material The online version of this chapter (https:// doi.org/10.1007/978-3-030-47426-3 64) contains supplementary material, which is available to authorized users. create noise or be redundant. FS methods are used to remove such irrelevant and redundant features and can be divided into three broad categories namely Wrapper [14, 18] , Embedded [20] , and Filter methods [13, 15, 16] . Among these, filter methods do not depend on a classifier to select a feature. It thus works faster, which is preferable for handling large feature sets [12] .", "cite_spans": [{"start": 600, "end": 604, "text": "[14,", "ref_id": "BIBREF14"}, {"start": 605, "end": 608, "text": "18]", "ref_id": "BIBREF18"}, {"start": 620, "end": 624, "text": "[20]", "ref_id": "BIBREF20"}, {"start": 646, "end": 650, "text": "[13,", "ref_id": "BIBREF13"}, {"start": 651, "end": 654, "text": "15,", "ref_id": "BIBREF15"}, {"start": 655, "end": 658, "text": "16]", "ref_id": "BIBREF16"}, {"start": 814, "end": 818, "text": "[12]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Introduction"}, {"text": "Again, Mutual information (MI) is usually popular in filter based methods. MI can capture non-linear relationships among features and class variable, can be computed for both categorical and numerical data, and can deal with multiple classes [7] . For these reasons, in this paper, we focus on MI based filter methods.", "cite_spans": [{"start": 242, "end": 245, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Introduction"}, {"text": "In MI based filter methods, the main goal is to select a subset of features S from the original feature set, F = {f 1 , f 2 , f 3 , ..., f n } in such a way that it will maximize joint MI (I(S; C)) with the class variable, C as showed in Eq. 1. ", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "However, the computation of I(S; C) is a NP-hard problem [7] . To overcome this problem, different approximations such as MIFS [1] , mRMR [10] , JMI [19] , RelaxMRMR [17] have been proposed over the last decades. In these methods, MI terms such as feature relevancy(R), redundancy(r), conditional redundancy(c) and interaction(i) are considered in order to achieve a better approximation. However, none of the aforementioned methods correct \"bias\" due to finite samples in calculating MI terms. In a recent method mDSM [16] , it is shown that incorporating bias correction for R, r, and c terms improves the classification performance. However, the interaction term is not considered in mDSM which needs to be addressed for better approximation [17] .", "cite_spans": [{"start": 57, "end": 60, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 127, "end": 130, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 138, "end": 142, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 149, "end": 153, "text": "[19]", "ref_id": "BIBREF19"}, {"start": 166, "end": 170, "text": "[17]", "ref_id": "BIBREF17"}, {"start": 519, "end": 523, "text": "[16]", "ref_id": "BIBREF16"}, {"start": 745, "end": 749, "text": "[17]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Introduction"}, {"text": "Apart from the evaluation criteria, searching is an important step in the FS methods to find out the combination of feature subset that performs well. Most popular searching techniques are forward selection, backward elimination, genetic algorithms (GA) based search [11] . Forward selection and backward elimination are greedy searching strategy that select/delete a feature one at a time. The limitation of these approaches are after selecting/deleting a feature, it cannot be deleted/re-selected later which may add redundant features [6] . On the other hand, GA based methods are computationally expensive and for a dataset with large number of features, it is not feasible to apply. Convex based Relaxation Approximation (COBRA) is proposed in [7] which provides a global solution for MI based FS. Another search strategy is introduced in mDSM where a small subset of features is selected using \u03c7 2 based forward selection that uses dynamic discretization. However, it cannot deselect a feature once it is already selected and do not show whether it is possible to use \u03c7 2 based search for interaction term. Considering the aforementioned issues, we propose a method called Discretization and feature Selection based on bias corrected MI (DSbM) and make the following major contribution: First, we calculate bias for the interaction terms and propose to use it for FS. Second, we show that the interaction terms follow \u03c7 2 distribution and proposed to use it in \u03c7 2 based search. Third, to obtain reduced number of feature, keeping similar performances with DSbM we propose a new method for simultaneous forward selection and backward elimination (DSbM fb ).", "cite_spans": [{"start": 267, "end": 271, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 538, "end": 541, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 749, "end": 752, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Introduction"}, {"text": "The main objective of MI based features selection methods is to determine a subset of features that have maximum dependency with the given class as shown in Eq. 1. Alternatively, this problem can be formulated for incremental feature selection that is to add one feature at a time in the selected subset to maximize I(S; C). From a given set F with n number of features, a new feature f m is added to the selected set, S = {f 1 , f 2 , ....., f m\u22121 }, that maximizes the score for a feature f m :", "cite_spans": [], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "Since I(S; C) remains constant with respect to f m , we choose f m that maximizes I(f m ; C | S). Using MI identities, this term can be expressed as", "cite_spans": [], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "here, the terms I(f m ; C), I(f m ; S) and I(f m ; S|C) represent feature relevancy, redundancy and conditional redundancy respectively [2] . Hence the score J(f m ) increases if the relevancy of the feature f m is large and redundancy with the existing features is low. However, the score also increases if the conditional redundancy is higher than the redundancy term. Hence, there is a trade-off, and the overall score is what needs to be maximized. Brown et al. in [2] further shows under the assumption that (a) the selected features in S are independent given the feature f m and (b) the selected features are class-conditionally independent given the feature f m and removing terms that have no effect on the choice of f m one can obtain the following equivalent score function:", "cite_spans": [{"start": 136, "end": 139, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 453, "end": 465, "text": "Brown et al.", "ref_id": null}, {"start": 469, "end": 472, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "with \u03b2 = 1 and \u03b3 = 1, this is what we call the Rrc criterion. It can then be easily shown that the incremental FS criterion or score function of well known MI based method such as MIFS [1] , mRMR [10] , Extended mRMR [9] , JMI [19] , and MIM [5] can be derived from this parameterized version of the score function. For example, JMI [19] criteria can be derived setting the value of \u03b2 = \u03b3 = 1 |S| . In [17] , the authors propose a new criterion by relaxing the the first assumption. They show under the relaxed assumption that the selected features are conditionally independent given the f m and another feature f i in S, the redundancy term can be approximated as the following", "cite_spans": [{"start": 185, "end": 188, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 196, "end": 200, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 217, "end": 220, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 227, "end": 231, "text": "[19]", "ref_id": "BIBREF19"}, {"start": 242, "end": 245, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 333, "end": 337, "text": "[19]", "ref_id": "BIBREF19"}, {"start": 402, "end": 406, "text": "[17]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "where \u03a9 is not dependent on f m . Instead of finding a feature f i to condition on, they propose to average the right-hand side over all f i \u2208 S, resulting in the following score function", "cite_spans": [], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "here, the I(f m ; f j | f i ) terms are the second order interaction term between the features. It should be noted that sum of the second order terms is normalized by", "cite_spans": [], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "The authors note that this is to prevent this sum to out-weight other terms. It can be seen that one can approximate the redundancy term using 3rd or higher order interaction terms by further relaxing the assumption. However, it is shown that the joint MI is more influenced by lower-order interaction terms in case of forward selection methods [4] .", "cite_spans": [{"start": 345, "end": 348, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "Practically, all aforementioned MI terms that have been used for the approximation need bias correction due to the finite number of samples. To solve this issue, a recent method namely, mDSM [16] is proposed where bias corrected MI has been used for calculating relevancy, redundancy and complementary term. They show incorporating bias correction improves the accuracy of classification. Also, it is theoretically shown that these three terms follow \u03c7 2 distributions.", "cite_spans": [{"start": 191, "end": 195, "text": "[16]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "here, M and I are the number of intervals in feature f m and f i respectively. K is number of class and N is total number of samples. The limitation of mDSM is that it does not consider the interaction term while proposing bias corrected MI to calculate the feature score which is necessary for better approximation of joint MI.", "cite_spans": [], "ref_spans": [], "section": "Information Theoretic Feature Selection Methods"}, {"text": "In this paper, we propose Discretization and feature Selection based on bias corrected MI (DSbM) which incorporates bias correction for MI based selection criteria. DSbM also uses dynamic discretization and greedy \u03c7 2 based forward selection. Moreover, a simultaneous forward selection and backward elimination is also proposed. These are described in the following subsections.", "cite_spans": [], "ref_spans": [], "section": "Proposed Method"}, {"text": "DSbM incorporates the bias correction for all four terms mentioned in Eq. 6 as it is necessary for better approximation of joint MI. The bias for the first three terms are given in Eq. 7. Theorem 1 shows the amount of bias for the interaction term and Theorem 2 shows that this term follows \u03c7 2 distribution. Proof of the theorems are given as supplementary materials due to page limitation. Incorporating this bias corrected Interaction term with Eq. 7, DSbM uses the following criteria for discretization and feature selection.", "cite_spans": [], "ref_spans": [], "section": "Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM)"}, {"text": "Based on Theorem 2, the critical value of the Interaction term will be as Eq. 9", "cite_spans": [], "ref_spans": [], "section": "Theorem 1. Bias is"}, {"text": "As the other three terms of Eq. 6 also follows \u03c7 2 distribution, we can use their critical values (shown in [16] ) for selecting a new feature. The overall process of DSbM is given in Algorithm 1. First, each feature f m \u2208 F is discretized with minimum number of intervals (d m ) for which its relevancy with the class variable (", "cite_spans": [{"start": 108, "end": 112, "text": "[16]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Theorem 1. Bias is"}, {"text": "If the feature is not significant even with some predefined maximum number of intervals (d max ), it is dropped. The selected candidate features (F c ) are then sorted according to their relevance J c in descending order (line 2-12 in Algorithm 1). The first feature f 1 is then included to the final selected feature set S. The remaining features of F c are evaluated incrementally maximizing the Rrci criteria. The score of J DSbM (Eq. 8) is compared (in line 15) with its' critical value (\u03c7 2 (Rrci) ), to select a new feature f m if it is not significantly redundant. Otherwise, f m is discarded considering that it does not contribute to the score significantly. While selecting a new feature, its discretization level is also shifted by a small value \u03b4 from its original value (as selected previously based on J rel as shown in line [16] [17] [18] [19] [20] [21] . This process helps to select the discretization level of features dynamically considering its dependency with other feature. In this way, all the features are discretized and selected simultaneously.", "cite_spans": [{"start": 839, "end": 843, "text": "[16]", "ref_id": "BIBREF16"}, {"start": 844, "end": 848, "text": "[17]", "ref_id": "BIBREF17"}, {"start": 849, "end": 853, "text": "[18]", "ref_id": "BIBREF18"}, {"start": 854, "end": 858, "text": "[19]", "ref_id": "BIBREF19"}, {"start": 859, "end": 863, "text": "[20]", "ref_id": "BIBREF20"}, {"start": 864, "end": 868, "text": "[21]", "ref_id": null}], "ref_spans": [], "section": "Theorem 1. Bias is"}, {"text": "Elimination (DSbM fb )", "cite_spans": [], "ref_spans": [], "section": "DSbM with Simultaneous Forward Selection and Backward"}, {"text": "DSbM follows \u03c7 2 based forward searching strategy where a feature can not be discarded once it is added to the selected subset S. When a candidate feature f m", "cite_spans": [], "ref_spans": [], "section": "DSbM with Simultaneous Forward Selection and Backward"}, {"text": "Input: Set of n features, F , Maximum discretization level dmax, Class C Output: Selected set of features, S = {f1, f2, \u00b7 \u00b7 \u00b7 , f k } with discretization, D = {d1, d2, \u00b7 \u00b7 \u00b7 , d k } 1: Subset of r candidate features, Fc \u2190 \u2205 2: for each fm \u2208 F do 3:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "for all l = 2 to dmax do 4:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "Discretize fm with l interval 5:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "Calculate J rel for feature fm 6:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "if J rel (fm) > \u03c7 2 (R) then 7:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "Fc is found redundant with respect to the selected features from S, DSbM does not consider f m for selection. However, it may happen that f m is more important and contains extra information compared to the already selected features. In this case, removing the redundant features from S is more appropriate. Therefore, we modify DSbM by including backward elimination and propose DSbM fb where simultaneous selection and elimination is incorporated.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "The process of backward elimination is described in Algorithm 2. Here, the redundant candidate feature f m is rechecked based on its interaction value to decide whether this feature f m is able to replace some features from S. This checking can be done by several ways such as considering all possible combination of three way interaction of f m with f i and f j and selecting the feature pair whose replacement can increase the J DSbM score significantly. However, it is computationally expensive to check all possible combination pairs of features. Hence, we consider the pair for which we obtain the highest interaction value Fc \u21d0 Fc \\ fm; 23: end for 24: Return S and their respective D (line 9-15) and replace that feature pair with f m if their removal from S passes the \u03c7 2 value and increases the total score (line [17] [18] . As a result, DSbM fb obtains a smaller subset of features compared to DSbM.", "cite_spans": [{"start": 823, "end": 827, "text": "[17]", "ref_id": "BIBREF17"}, {"start": 828, "end": 832, "text": "[18]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Algorithm 1 : DSbM"}, {"text": "In this section, the experimental setup and evaluation process of different methods along with the proposed ones is presented. Furthermore, a number of experiments are performed to highlight the effectiveness of the proposed contributions.", "cite_spans": [], "ref_spans": [], "section": "Experimental Result"}, {"text": "In this experiment, twenty benchmark datasets collected from UCI Machine Learning Repository [3] are used as they are also employed in [16] and [19] . The description of these datasets are given in Table 1 . For classification, we use SVM and KNN, and conduct 10-fold cross-validation on each dataset.", "cite_spans": [{"start": 93, "end": 96, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 135, "end": 139, "text": "[16]", "ref_id": "BIBREF16"}, {"start": 144, "end": 148, "text": "[19]", "ref_id": "BIBREF19"}], "ref_spans": [{"start": 198, "end": 205, "text": "Table 1", "ref_id": "TABREF2"}], "section": "Dataset Description and Implementation Details"}, {"text": "We compare DSbM with four state-of-the-art methods namely mDSM, JMI, JMI with COBRA search (JC) and RelaxMRMR. Here, DSbM, mDSM and JC are feature selection method, however, JMI and RelaxMRMR are feature ranking method. Hence, the number of selected feature obtained in DSbM are used to generate the results for these two methods. For JMI and RelaxMRMR, we use forward selection whereas, JC performs COBRA search and mDSM uses \u03c7 2 based search. For comparing the methods we use three metrics namely accuracy, Score (defined in Eq. 10) and Pareto Optimality(PO). PO returns a set of non-dominant candidate solutions.", "cite_spans": [], "ref_spans": [], "section": "Dataset Description and Implementation Details"}, {"text": "here, \u03b1 i and w i indicates the performance evaluation criteria and weights respectively. For our method \u03b1 1 and \u03b1 2 indicates the percentage accuracy, and \u03b1 2 = (N t \u2212 N s )/N t is the percentage of reduction features. Here, N t is the total number of features in a dataset and N s is the number of selected features. We use equal weights. To calculate PO, we use \u03b1 1 and \u03b1 2 and to perform Friedman test we use Score to incorporate the joint impact of number of selected features and the corresponding accuracy. We also calculate Win/Tie/Loss which indicates the number of datasets for which comparing method performs better/equallywell/worse than other methods unless otherwise stated. To determine whether the wins are statistically significant we perform t-test at 0.05 significance level.", "cite_spans": [], "ref_spans": [], "section": "Dataset Description and Implementation Details"}, {"text": "Here, we first discuss how DSbM performs compared to other methods and then we compare the performance of DSbM with DSbM fb .", "cite_spans": [], "ref_spans": [], "section": "Results and Discussion"}, {"text": "To investigate the impact of high-order term for approximating joint MI in DSbM, let us first consider Table 2 . For this table, win/tie/loss is calculated using the accuracies given in Table 3 . RelaxMRMR performs slightly better than JMI due to the incorporation of interaction term. Whereas, mDSM outperforms RelaxMRMR even though mDSM does not consider high-order term. It is due to the bias correction, dynamic discretization and \u03c7 2 based search. This indicates that mDSM with high-order term might perform well which is the proposed DSbM. mDSM also performs better than JC. Table 3 compares DSbM with mDSM, JC, JMI and RelaxMRMR. The number inside the parenthesis represents the number of selected feature. For example, DSbM achieves 96% accuracy using SVM with 2 selected features for Iris dataset.", "cite_spans": [], "ref_spans": [{"start": 103, "end": 110, "text": "Table 2", "ref_id": "TABREF3"}, {"start": 186, "end": 193, "text": "Table 3", "ref_id": "TABREF4"}, {"start": 581, "end": 588, "text": "Table 3", "ref_id": "TABREF4"}], "section": "Comparison of DSbM with Other Methods."}, {"text": "It is evident from Table 3 that DSbM outperforms all the four state-of-theart methods. The second last and the last row of Table 3 represent the pair wise win/tie/loss and significant win/loss of DSbM with the existing methods respectively. Even though DSbM wins in thirteen datasets among the twenty compared to mDSM for SVM classifier, the differences in accuracies are not To understand the joint impact of accuracy and number of selected features, let us consider Table 4 , where the ranking of the above mentioned methods is shown according to their frequency in the PO set and Friedman test. In both cases, DSbM achieves the highest rank. In Friedman test, after rejecting the null hypothesis that all the methods perform equivalently, a post-hoc test called Nemenyi test [8] is used to determine the which method performs significantly better than the others. The test indicates that DSbM significantly (at 95% confidence level) outperforms the four other methods both for SVM and KNN.", "cite_spans": [{"start": 778, "end": 781, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [{"start": 19, "end": 26, "text": "Table 3", "ref_id": "TABREF4"}, {"start": 123, "end": 130, "text": "Table 3", "ref_id": "TABREF4"}, {"start": 468, "end": 475, "text": "Table 4", "ref_id": "TABREF5"}], "section": "Comparison of DSbM with Other Methods."}, {"text": "To understand the impact of simultaneous forward selection and backward elimination using DSbM fb , let us consider Fig. 1a and Fig. 1b . We observe, in most of the cases DSbM fb selects less features than DSbM (number of selected features is given on the top of each bar and on the x-axis the index of datasets are given according to their order in Table 1 ). These figures also illustrate that when the total number of features for a dataset is comparatively small then the performance of both DSbM and DSbM fb are similar in terms of number of selected features and accuracy (e.g., Iris, Yeast, Glass etc.). Note that in some cases such as in Cardio, Arrhythmia etc., DSbM fb selects fewer features with higher accuracy. Furthermore, a limitation of mDSM is that, the set of selected features might contain a subset for which better accuracy can be found. DSbM also has similar problem which can be observed in Fig. 2a . This issue is resolved to some extent in DSbM fb . Here, we get 74.19% accuracy with 84 selected features (see Fig. 2b ) while DSbM obtains an accuracy of 72.79% with 107 features. ", "cite_spans": [], "ref_spans": [{"start": 116, "end": 123, "text": "Fig. 1a", "ref_id": null}, {"start": 128, "end": 135, "text": "Fig. 1b", "ref_id": null}, {"start": 350, "end": 357, "text": "Table 1", "ref_id": "TABREF2"}, {"start": 914, "end": 921, "text": "Fig. 2a", "ref_id": null}, {"start": 1035, "end": 1042, "text": "Fig. 2b", "ref_id": null}], "section": "Impact of DSbM fb over DSbM."}, {"text": "In this paper, we propose a method DSbM which includes bias correction for high-order dependencies among features and use \u03c7 2 based search that also consider high-order dependencies. Results over a large amount of dataset demonstrate that DSbM outperforms current state-of-the-art methods. Beside this, a \u03c7 2 based simultaneous forward and backward search is also proposed here that shows similar performances with DSbM with less number of features. This method can be applied for different applications such as activity recognition and cancer classification for gene expression data. Incorporation of further highorder terms might improve the overall performance which require further theoretical analysis and experimentation with global feature selection which will be addressed in future work.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Using mutual information for selecting features in supervised neural net learning", "authors": [{"first": "R", "middle": [], "last": "Battiti", "suffix": ""}], "year": 1994, "venue": "IEEE Trans. Neural Netw", "volume": "5", "issn": "4", "pages": "537--550", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "authors": [{"first": "G", "middle": [], "last": "Brown", "suffix": ""}, {"first": "A", "middle": [], "last": "Pocock", "suffix": ""}, {"first": "M", "middle": ["J"], "last": "Zhao", "suffix": ""}, {"first": "M", "middle": [], "last": "Luj\u00e1n", "suffix": ""}], "year": 2012, "venue": "J. Mach. Learn. Res", "volume": "13", "issn": "1", "pages": "27--66", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "UCI machine learning repository", "authors": [{"first": "D", "middle": [], "last": "Dua", "suffix": ""}, {"first": "C", "middle": [], "last": "Graff", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Mutual information-based multi-label feature selection using interaction information", "authors": [{"first": "J", "middle": [], "last": "Lee", "suffix": ""}, {"first": "D", "middle": ["W"], "last": "Kim", "suffix": ""}], "year": 2015, "venue": "Exp. Syst. Appl", "volume": "42", "issn": "4", "pages": "2013--2025", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Feature selection and feature extraction for text categorization", "authors": [{"first": "D", "middle": ["D"], "last": "Lewis", "suffix": ""}], "year": 1992, "venue": "Proceedings of the Workshop on Speech and Natural Language", "volume": "", "issn": "", "pages": "212--217", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Orthogonal forward selection and backward elimination algorithms for feature subset selection", "authors": [{"first": "K", "middle": ["Z"], "last": "Mao", "suffix": ""}], "year": 2004, "venue": "IEEE Trans. Syst. Man Cybern. Part B", "volume": "34", "issn": "1", "pages": "629--634", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "A semidefinite programming based search strategy for feature selection with mutual information measure", "authors": [{"first": "T", "middle": [], "last": "Naghibi", "suffix": ""}, {"first": "S", "middle": [], "last": "Hoffmann", "suffix": ""}, {"first": "B", "middle": [], "last": "Pfister", "suffix": ""}], "year": 2014, "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "volume": "37", "issn": "8", "pages": "1529--1541", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Distribution-free multiple comparisons", "authors": [{"first": "P", "middle": [], "last": "Nemenyi", "suffix": ""}], "year": 1963, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Effective global approaches for mutual information based feature selection", "authors": [{"first": "X", "middle": ["V"], "last": "Nguyen", "suffix": ""}, {"first": "J", "middle": [], "last": "Chan", "suffix": ""}, {"first": "S", "middle": [], "last": "Romano", "suffix": ""}, {"first": "J", "middle": [], "last": "Bailey", "suffix": ""}], "year": 2014, "venue": "ACM SIGKDD", "volume": "", "issn": "", "pages": "512--521", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy", "authors": [{"first": "H", "middle": [], "last": "Peng", "suffix": ""}, {"first": "F", "middle": [], "last": "Long", "suffix": ""}, {"first": "C", "middle": [], "last": "Ding", "suffix": ""}], "year": 2005, "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "volume": "8", "issn": "", "pages": "1226--1238", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Hybrid feature selection method based on the genetic algorithm and pearson correlation coefficient", "authors": [{"first": "R", "middle": [], "last": "Saidi", "suffix": ""}, {"first": "W", "middle": [], "last": "Bouaguel", "suffix": ""}, {"first": "N", "middle": [], "last": "Essoussi", "suffix": ""}], "year": null, "venue": "Machine Learning Paradigms: Theory and Application. SCI", "volume": "801", "issn": "", "pages": "3--24", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "A new maximum relevance-minimum multicollinearity (MRmMC) method for feature selection and ranking", "authors": [{"first": "A", "middle": [], "last": "Senawi", "suffix": ""}, {"first": "H", "middle": ["L"], "last": "Wei", "suffix": ""}, {"first": "S", "middle": ["A"], "last": "Billings", "suffix": ""}], "year": 2017, "venue": "Pattern Recogn", "volume": "67", "issn": "", "pages": "47--61", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "BFSp: a feature selection method for bug severity classification", "authors": [{"first": "S", "middle": [], "last": "Sharmin", "suffix": ""}, {"first": "F", "middle": [], "last": "Aktar", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Ali", "suffix": ""}, {"first": "M", "middle": ["A H"], "last": "Khan", "suffix": ""}, {"first": "M", "middle": [], "last": "Shoyaib", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "750--754", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "SAL: an effective method for software defect prediction", "authors": [{"first": "S", "middle": [], "last": "Sharmin", "suffix": ""}, {"first": "M", "middle": ["R"], "last": "Arefin", "suffix": ""}, {"first": "M", "middle": ["A"], "last": "Wadud", "suffix": ""}, {"first": "N", "middle": [], "last": "Nower", "suffix": ""}, {"first": "M", "middle": [], "last": "Shoyaib", "suffix": ""}], "year": 2015, "venue": "18th ICCIT", "volume": "", "issn": "", "pages": "184--189", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Feature selection and discretization based on mutual information", "authors": [{"first": "S", "middle": [], "last": "Sharmin", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Ali", "suffix": ""}, {"first": "M", "middle": ["A H"], "last": "Khan", "suffix": ""}, {"first": "M", "middle": [], "last": "Shoyaib", "suffix": ""}], "year": 2017, "venue": "icIVPR", "volume": "", "issn": "", "pages": "1--6", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Simultaneous feature selection and discretization based on mutual information", "authors": [{"first": "S", "middle": [], "last": "Sharmin", "suffix": ""}, {"first": "M", "middle": [], "last": "Shoyaib", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Ali", "suffix": ""}, {"first": "M", "middle": ["A H"], "last": "Khan", "suffix": ""}, {"first": "O", "middle": [], "last": "Chae", "suffix": ""}], "year": 2019, "venue": "Pattern Recogn", "volume": "91", "issn": "", "pages": "162--174", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Can high-order dependencies improve mutual information based feature selection? Pattern Recogn", "authors": [{"first": "N", "middle": ["X"], "last": "Vinh", "suffix": ""}, {"first": "S", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "J", "middle": [], "last": "Chan", "suffix": ""}, {"first": "J", "middle": [], "last": "Bailey", "suffix": ""}], "year": 2016, "venue": "", "volume": "53", "issn": "", "pages": "46--58", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "GA-KDE-Bayes: an evolutionary wrapper method based on non-parametric density estimation applied to bioinformatics problems", "authors": [{"first": "M", "middle": ["F B"], "last": "Wanderley", "suffix": ""}, {"first": "V", "middle": [], "last": "Gardeux", "suffix": ""}, {"first": "R", "middle": [], "last": "Natowicz", "suffix": ""}, {"first": "A", "middle": [], "last": "De P\u00e1dua Braga", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "155--160", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Feature selection based on joint mutual information", "authors": [{"first": "H", "middle": [], "last": "Yang", "suffix": ""}, {"first": "J", "middle": [], "last": "Moody", "suffix": ""}], "year": 1999, "venue": "Proceedings of International ICSC Symposium on Advances in Intelligent Data Analysis", "volume": "", "issn": "", "pages": "22--25", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification", "authors": [{"first": "G", "middle": ["X"], "last": "Yuan", "suffix": ""}, {"first": "K", "middle": ["W"], "last": "Chang", "suffix": ""}, {"first": "C", "middle": ["J"], "last": "Hsieh", "suffix": ""}, {"first": "C", "middle": ["J"], "last": "Lin", "suffix": ""}], "year": 2010, "venue": "J. Mach. Learn. Res", "volume": "11", "issn": "", "pages": "3183--3234", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "(S; C) = I(f 1 , f 2 , ....., f k ; C) = f1,f2,.....,f k C P (f 1 , f 2 , ....., f k ; C) log P (f 1 , f 2 , ....., f k ; C) P (f 1 , f 2 , ....., f k )P (C)", "latex": null, "type": "figure"}, "FIGREF1": {"text": "among the features f m and f j given feature f i , where I, J and M are the number of intervals in feature f i , f j and f m respectively.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "DSbM(black bar) vs. DSbM fb (white bar) Accuracy (SVM) vs. Number of features for Arrhythmia dataset", "latex": null, "type": "figure"}, "TABREF0": {"text": "Sort Fc with corresponding Dc in decreasing order based on their Jc values 13: select f1 with its' corresponding d1 14: S \u21d0 S \u222a f1; D \u21d0 D \u222a d1; Fc \u21d0 Fc \\ f1; 15: for each fm \u2208 Fc do 16:for all l = dm -\u03b4 to l = dm + \u03b4 do Return S and their respective D", "latex": null, "type": "table"}, "TABREF1": {"text": "Algorithm 2 : DSbM fb Input: Set of n features, F , Maximum discretization level dmax, Class C Output: Selected set of features, S = {f1, f2, \u00b7 \u00b7 \u00b7 , f k } with discretization, D = && J DSbM (fm) on S > J DSbM (fm) on S then 18: S \u21d0 S \u222a fm; S \u2190 S ; D \u21d0 D \u222a dm; D \u21d0 D \\ {di, dj};", "latex": null, "type": "table"}, "TABREF2": {"text": "Dataset description", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Index </td><td>Dataset </td><td>Dimension </td><td>Instance </td><td>Class </td><td>Index </td><td>Dataset </td><td>Dimension </td><td>Instance </td><td>Class\n</td></tr><tr><td>1 </td><td>Iris </td><td>4 </td><td>150 </td><td>3 </td><td>11 </td><td>Parkinson </td><td>22 </td><td>197 </td><td>2\n</td></tr><tr><td>2 </td><td>Pima </td><td>8 </td><td>768 </td><td>2 </td><td>12 </td><td>Steel </td><td>27 </td><td>1941 </td><td>7\n</td></tr><tr><td>3 </td><td>Yeast </td><td>8 </td><td>1484 </td><td>10 </td><td>13 </td><td>Breast </td><td>30 </td><td>569 </td><td>2\n</td></tr><tr><td>4 </td><td>Glass </td><td>9 </td><td>214 </td><td>6 </td><td>14 </td><td>Dermatology </td><td>34 </td><td>366 </td><td>6\n</td></tr><tr><td>5 </td><td>Wine </td><td>13 </td><td>178 </td><td>3 </td><td>15 </td><td>Spambase </td><td>57 </td><td>4601 </td><td>2\n</td></tr><tr><td>6 </td><td>Heart </td><td>13 </td><td>270 </td><td>2 </td><td>16 </td><td>Sonar </td><td>60 </td><td>208 </td><td>2\n</td></tr><tr><td>7 </td><td>Australian </td><td>14 </td><td>690 </td><td>2 </td><td>17 </td><td>Liver </td><td>6 </td><td>345 </td><td>2\n</td></tr><tr><td>8 </td><td>Segment </td><td>17 </td><td>2310 </td><td>7 </td><td>18 </td><td>Breast Tissue </td><td>9 </td><td>106 </td><td>6\n</td></tr><tr><td>9 </td><td>Cardio </td><td>21 </td><td>2126 </td><td>10 </td><td>19 </td><td>Arrhythmia </td><td>279 </td><td>452 </td><td>16\n</td></tr><tr><td>10 </td><td>Waveform </td><td>21 </td><td>5000 </td><td>3 </td><td>20 </td><td>Semeion </td><td>256 </td><td>1593 </td><td>10\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Comparison", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>RelaxMRMR vs. JMI </td><td>mDSM vs. RelaxMRMR </td><td>mDSM vs. JC\n</td></tr><tr><td>SVM </td><td>7/7/6 </td><td>13/1/6 </td><td>14/0/6\n</td></tr><tr><td>KNN </td><td>14/1/5 </td><td>14/0/6 </td><td>16/0/4\n</td></tr></table></body></html>"}, "TABREF4": {"text": "Comparison among different methods based on its accuracy. ( * ) and (\u2022) represents that DSbM wins and loses significantly from that method respectively and bold values represent the overall win among all methods. 74.51 * 74.71 * 93.32 92.62 67.30 * 68.31 * 68.31 * Sonar 81.36(36) 75.91(21) 72.70(60) * 70.45 * 73.64 * 85.00 83.64 88.60 87.27 84.55 57.14 57.14 46.29 46.27 52.00 \u2022 43.14 43.14 Arrhythmia 72.79(107)66.28(118) * 70.70(253) 72.56 74.19 65.12 65.35 58.60 * 64.65 69.30 Semeion 93.23(253)92.93(255) 93.20(254) 93.17 93.17 91.41 90.73 91.20 91.40 91.41", "latex": null, "type": "table"}, "TABREF5": {"text": "Ranking of existing feature selection criteria.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Frequency in Pareto optimal set\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>SVM </td><td>DSbM(12) </td><td>JC(8) </td><td>mDSM(6) </td><td>JMI(2) </td><td>RelaxMRMR(2)\n</td></tr><tr><td>KNN </td><td>DSbM(13) </td><td>JC(10) </td><td>mDSM(6) </td><td>JMI(4) </td><td>RelaxMRMR(1)\n</td></tr><tr><td>Average rank from Friedman test\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>SVM </td><td>DSbM(1.70) </td><td>RelaxMRMR(2.73) </td><td>JMI(2.85) </td><td>mDSM(3.78) </td><td>JC(3.95)\n</td></tr><tr><td>KNN </td><td>DSbM(1.68) </td><td>RelaxMRMR(2.75) </td><td>JMI(2.83) </td><td>mDSM(3.80) </td><td>JC(3.95)\n</td></tr></table></body></html>"}}, "back_matter": []}