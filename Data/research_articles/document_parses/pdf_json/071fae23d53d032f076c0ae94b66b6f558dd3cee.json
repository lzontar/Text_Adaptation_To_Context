{"paper_id": "071fae23d53d032f076c0ae94b66b6f558dd3cee", "metadata": {"title": "A Machine Learning Based Software Pipeline to Pick the Variable Ordering for Algorithms with Polynomial Inputs", "authors": [{"first": "Dorian", "middle": [], "last": "Florescu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Coventry University", "location": {"postCode": "CV1 5FB", "settlement": "Coventry", "country": "UK"}}, "email": "fdorian88@gmail.com"}, {"first": "Matthew", "middle": [], "last": "England", "suffix": "", "affiliation": {"laboratory": "", "institution": "Coventry University", "location": {"postCode": "CV1 5FB", "settlement": "Coventry", "country": "UK"}}, "email": "matthew.england@coventry.ac.uk"}]}, "abstract": [{"text": "We are interested in the application of Machine Learning (ML) technology to improve mathematical software. It may seem that the probabilistic nature of ML tools would invalidate the exact results prized by such software, however, the algorithms which underpin the software often come with a range of choices which are good candidates for ML application. We refer to choices which have no effect on the mathematical correctness of the software, but do impact its performance.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "In the past we experimented with one such choice: the variable ordering to use when building a Cylindrical Algebraic Decomposition (CAD). We used the Python library Scikit-Learn (sklearn) to experiment with different ML models, and developed new techniques for feature generation and hyper-parameter selection.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "These techniques could easily be adapted for making decisions other than our immediate application of CAD variable ordering. Hence in this paper we present a software pipeline to use sklearn to pick the variable ordering for an algorithm that acts on a polynomial system. The code described is freely available online.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Mathematical Software, i.e. tools for effectively computing mathematical objects, is a broad discipline: the objects in question may be expressions such as polynomials or logical formulae, algebraic structures such as groups, or even mathematical theorems and their proofs. In recent years there have been examples of software that acts on such objects being improved through the use of artificial intelligence techniques. For example, [21] uses a Monte-Carlo tree search to find the representation of polynomials that are most efficient to evaluate; [22] uses a machine learnt branching heuristic in a SAT-solver for formulae in Boolean logic; [18] uses pattern matching to determine whether a pair of elements from a specified group are conjugate; and [1] uses deep neural networks for premise selection in an automated theorem prover. See the survey article [12] in the proceedings of ICMS 2018 for more examples.", "cite_spans": [{"start": 436, "end": 440, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 551, "end": 555, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 645, "end": 649, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 754, "end": 757, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 861, "end": 865, "text": "[12]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Introduction and Context"}, {"text": "Machine Learning (ML), that is statistical techniques to give computer systems the ability to learn rules from data, may seem unsuitable for use in mathematical software since ML tools can only offer probabilistic guidance, when such software prizes exactness. However, none of the examples above risked the correctness of the end-result in their software. They all used ML techniques to make non-critical choices or guide searches: the decisions of the ML carried no risk to correctness, but did offer substantial increases in computational efficiency. All mathematical software, no matter the mathematical domain, will likely involve such choices, and our thesis is that in many cases an ML technique could make a better choice than a human user, so-called magic constants [6] , or a traditional human-designed heuristic.", "cite_spans": [{"start": 775, "end": 778, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Introduction and Context"}, {"text": "In Sect. 2 we briefly survey our recent work applying ML to improve an algorithm in a computer algebra system which acts on sets of polynomials. We describe how we proposed a more appropriate definition of model accuracy and used this to improve the selection of hyper-parameters for ML models; and a new technique for identifying features of the input polynomials suitable for ML.", "cite_spans": [], "ref_spans": [], "section": "Contribution and Outline"}, {"text": "These advances can be applied beyond our immediate application: the feature identification to any situation where the input is a set of polynomials, and the hyper-parameter selection to any situation where we are seeking to take a choice that minimises a computation time. Hence we saw value in packaging our techniques into a software pipeline so that they may be used more widely. Here, by pipeline we refer to a succession of computing tasks that can be run as one task. The software is freely available as a Zenodo repository here: https://doi. org/10.5281/zenodo. 3731703 We describe the software pipeline and its functionality in Sect. 3. Then in Sect. 4 we describe its application on a dataset we had not previously studied.", "cite_spans": [{"start": 569, "end": 576, "text": "3731703", "ref_id": null}], "ref_spans": [], "section": "Contribution and Outline"}, {"text": "Our recent work has been using ML to select the variable ordering to use for calculating a cylindrical algebraic decomposition relative to a set of polynomials.", "cite_spans": [], "ref_spans": [], "section": "Brief Survey of Our Recent Work"}, {"text": "A Cylindrical Algebraic Decomposition (CAD) is a decomposition of ordered R n space into cells arranged cylindrically, meaning the projections of cells all lie within cylinders over a CAD of a lower dimensional space. All these cells are (semi)-algebraic meaning each can be described with a finite sequence of polynomial constraints. A CAD is produced for either a set of polynomials, or a logical formula whose atoms are polynomial constraints. It may be used to analyse these objects by finding a finite sample of points to query and thus understand the behaviour over all R n . The most important application of CAD is to perform Quantifier Elimination (QE) over the reals. I.e. given a quantified formula, a CAD may be used to find an equivalent quantifier free formula 1 .", "cite_spans": [], "ref_spans": [], "section": "Cylindrical Algebraic Decomposition"}, {"text": "CAD was introduced in 1975 [10] and is still an active area of research. The collection [7] summarises the work up to the mid-90s while the background section of [13] , for example, includes a summary of progress since. QE has numerous applications in science [2] , engineering [25] , and even the social sciences [23] .", "cite_spans": [{"start": 27, "end": 31, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 88, "end": 91, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 162, "end": 166, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 260, "end": 263, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 278, "end": 282, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 314, "end": 318, "text": "[23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Cylindrical Algebraic Decomposition"}, {"text": "CAD requires an ordering of the variables. QE imposes that the ordering matches the quantification of variables, but variables in blocks of the same quantifier and the free variables can be swapped 2 . The ordering can have a great effect on the time/memory use of CAD, the number of cells, and even the underlying complexity [5] . Human designed heuristics have been developed to make the choice [3, 4, 11, 14] and are used in most implementations.", "cite_spans": [{"start": 326, "end": 329, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 397, "end": 400, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 401, "end": 403, "text": "4,", "ref_id": "BIBREF3"}, {"start": 404, "end": 407, "text": "11,", "ref_id": "BIBREF10"}, {"start": 408, "end": 411, "text": "14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Cylindrical Algebraic Decomposition"}, {"text": "The first application of ML to the problem was in 2014 when a support vector machine was trained to choose which of these heuristics to follow [19, 20] . The machine learned choice did significantly better than any one heuristic overall.", "cite_spans": [{"start": 143, "end": 147, "text": "[19,", "ref_id": "BIBREF18"}, {"start": 148, "end": 151, "text": "20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Cylindrical Algebraic Decomposition"}, {"text": "The present authors revisited these experiments in [15] but this time using ML to predict the ordering directly (because there were many problems where none of the human-made heuristics made good choices and although the number of orderings increases exponentially, the current scope of CAD application means this is not restrictive). We also explored a more diverse selection of ML methods available in the Python library scikit-learn (sklearn) [24] . All the models tested outperformed the human made heuristics.", "cite_spans": [{"start": 51, "end": 55, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 446, "end": 450, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Recent Work on ML for CAD Variable Ordering"}, {"text": "The ML models learn not from the polynomials directly, but from features: properties which evaluate to a floating point number for a specific polynomial set. In [20] and [15] only a handful of features were used (measures of degree and frequency of occurrence for variables). In [16] we developed a new feature generation procedure which used combinations of basic functions (average, sign, maximum) evaluated on the degrees of the variables in either one polynomial or the whole system. This allowed for substantially more features and improved the performance of all ML models. The new features could be used for any ML application where the input is a set of polynomials.", "cite_spans": [{"start": 161, "end": 165, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 170, "end": 174, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 279, "end": 283, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Recent Work on ML for CAD Variable Ordering"}, {"text": "The natural metric for judging a CAD variable ordering is the corresponding CAD runtime: in the work above models were trained to pick the ordering which minimises this for a given input. However, this meant the training did not distinguish between any non-optimal ordering even though the difference between these could be huge. This led us to a new definition of accuracy in [17] : to picking an ordering which leads to a runtime within x% of the minimum possible.", "cite_spans": [{"start": 377, "end": 381, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Recent Work on ML for CAD Variable Ordering"}, {"text": "We then wrote a new version of the sklearn procedure which uses crossvalidation to select model hyper-parameters to minimise the total CAD runtime of its choices, rather than maximise the number of times the minimal ordering is chosen. This also improved the performance of all ML models in the experiments of [17] . The new definition and procedure are suitable for any situation where we are seeking to take a choice that minimises a computation time.", "cite_spans": [{"start": 310, "end": 314, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Recent Work on ML for CAD Variable Ordering"}, {"text": "The input to our pipeline is given by two distinct datasets used for training and testing, respectively. An individual entry in the data set is a set of polynomials that represent an input to a symbolic computation algorithms, in our case CAD. The output is a corresponding sequence of variable ordering suggestions for each set of polynomials in the testing dataset.", "cite_spans": [], "ref_spans": [], "section": "Software Pipeline"}, {"text": "The pipeline is fully automated: it generates and uses the CAD runtimes for each set of polynomials under each admissible variable ordering; uses the runtimes from the training dataset to select the hyper-parameters with crossvalidation and tune the parameters of the model; and evaluates the performance of those classifiers (along with some other heuristics for the problem) for the sets of polynomials in the testing dataset.", "cite_spans": [], "ref_spans": [], "section": "Software Pipeline"}, {"text": "We describe these key steps in the pipeline below. Each of the numbered stages can be individually marked for execution or not in a run of the pipeline (avoiding duplication of existing computation). The code for this pipeline, written all in Python, is freely available at: https://doi.org/10.5281/zenodo.3731703.", "cite_spans": [], "ref_spans": [], "section": "Software Pipeline"}, {"text": "(a) Measuring the CAD Runtimes: The CAD routine is run for each set of polynomials in the training dataset. The runtimes for all possible variable orderings are stored in a different file for each set of polynomials. If the runtime exceeds a pre-defined timeout, the value of the timeout is stored instead.", "cite_spans": [], "ref_spans": [], "section": "I. Generating a Model Using the Training Dataset"}, {"text": "The training dataset is first converted to a format that is easier to process into features. For this purpose, we chose the format given by the terms() method from the Poly class located in the sympy package for symbolic computation in Python.", "cite_spans": [], "ref_spans": [], "section": "(b) Polynomial Data Parsing:"}, {"text": "Here, each monomial is defined by a tuple, containing another tuple with the degrees of each variable, and a value defining the monomial coefficient. The All the data points in the training dataset are then collected into a single file called terms train.txt after being placed into this format. Subsequently, the file y train.txt is created storing the index of the variable ordering with the minimum computing times for each set of polynomials, using the runtimes measured in Step I(a).", "cite_spans": [], "ref_spans": [], "section": "(b) Polynomial Data Parsing:"}, {"text": "(c) Feature Generation: Here each set of polynomials in the training dataset is processed into a fixed length sequence of floating point numbers, called features, which are the actual data used to train the ML models in sklearn. This is done with the following steps:", "cite_spans": [], "ref_spans": [], "section": "(b) Polynomial Data Parsing:"}, {"text": "i. Raw feature generation", "cite_spans": [], "ref_spans": [], "section": "(b) Polynomial Data Parsing:"}, {"text": "We systematically consider applying all meaningful combinations of the functions average, sign, maximum, and sum to polynomials with a given number of variables. This generates a large set of feature descriptions as proposed in [16] . The new format used to store the data described above allows for an easy evaluation of these features. An example of computing such features is given in Fig. 1 . In [16] we described how the method provides 1728 possible features for polynomials constructed with three variables for example. This step generates the full set of feature descriptions, saved in a file called features descriptions.txt, and the corresponding values of the features on the training dataset, saved in a file called features train raw.txt. ", "cite_spans": [{"start": 228, "end": 232, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 400, "end": 404, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [{"start": 388, "end": 394, "text": "Fig. 1", "ref_id": "FIGREF1"}], "section": "(b) Polynomial Data Parsing:"}, {"text": "After computing the numerical values of the features in Step I(c)i this step will remove those features that are constant or repetitive for the dataset in question, as described in [16] . The descriptions of the remaining features are saved in a new file called features descriptions final.txt.", "cite_spans": [{"start": 181, "end": 185, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "ii. Feature simplification"}, {"text": "The final set of features is computed by evaluating the descriptions in features descriptions final.txt for the training dataset. Even though these were already evaluated in Step I(c)i we repeat the evaluation for the final set of feature descriptions. This is to allow the possibility of users entering alternative features manually and skipping steps i and ii. As noted above, any of the named steps in the pipeline can be selected or skipped for execution in a given run. The final values of the features are saved in a new file called features train.txt.", "cite_spans": [], "ref_spans": [], "section": "iii. Final feature generation"}, {"text": "i.", "cite_spans": [], "ref_spans": [], "section": "(d) Machine Learning Classifier Training:"}, {"text": "The pipeline can apply four of the most commonly used deterministic ML models (see [15] for details), using the implementations in sklearn [24] .", "cite_spans": [{"start": 83, "end": 87, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 139, "end": 143, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Fitting the model hyperparameters by cross-validation"}, {"text": "-The K-Nearest Neighbors (KNN) classifier -The Multi-Layer Perceptron (MLP) classifier -The Decision Tree (DT) classifier -The Support Vector Machine (SVM) classifier Of course, additional models in sklearn and its extensions could be included with relative ease. The pipeline can use two different methods for fitting the hyperparameters via a cross-validation procedure on the training set, as described in [17] :", "cite_spans": [{"start": 409, "end": 413, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Fitting the model hyperparameters by cross-validation"}, {"text": "-Standard cross-validation: maximizing the prediction accuracy (i.e. the number of times the model picks the optimum variable ordering). -Time-based cross-validation: minimizing the CAD runtime (i.e. the time taken to compute CADs with the model's choices). Both methods tune the hyperparameters with cross-validation using the routine RandomizedSearchCV from the sklearn package in Python (the latter an adapted version we wrote). The cross-validation results (i.e. choice of hyperparameters) are saved in a file hyperpar D** ** T** **.txt, where D** ** is the date and T** ** denotes the time when the file was generated.", "cite_spans": [], "ref_spans": [], "section": "Fitting the model hyperparameters by cross-validation"}, {"text": "ii. Fitting the parameters", "cite_spans": [], "ref_spans": [], "section": "Fitting the model hyperparameters by cross-validation"}, {"text": "The parameters of each model are subsequently fitted using the standard sklearn algorithms for each chosen set of hyperparameters. These are saved in a file called par D** ** T** **.txt.", "cite_spans": [], "ref_spans": [], "section": "Fitting the model hyperparameters by cross-validation"}, {"text": "The models in Step I are then evaluated according to their choices of variable orderings for the sets of polynomials in the testing dataset. The steps below are listed without detailed description as they are performed similarly to Step I for the testing dataset. (c) Predictions Using ML: Predictions on the testing dataset are generated using the model computed in", "cite_spans": [], "ref_spans": [], "section": "II. Predicting the CAD Variable Orderings Using the Testing Dataset"}, {"text": "Step I(c). The model is run with the data in Step II(a)ii, and the predictions are stored in a file called y D** ** T** ** test.txt.", "cite_spans": [], "ref_spans": [], "section": "II. Predicting the CAD Variable Orderings Using the Testing Dataset"}, {"text": "In our prior papers [15] [16] [17] we compared the performance of the ML models with the human-designed heuristics in [4] and [11] . For details on how these are applied see [15] . Their choices are saved in two files entitled y brown test.txt and y sotd test.txt, respectively.", "cite_spans": [{"start": 20, "end": 24, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 25, "end": 29, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 30, "end": 34, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 118, "end": 121, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 126, "end": 130, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 174, "end": 178, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "(d) Predictions Using Human-Made Heuristics:"}, {"text": "(e) Comparative Results: Finally, in order to compare the performance of the proposed pipeline, we must measure the actual CAD runtimes on the testing dataset. The results of the comparison is saved in a file with the template: comparative results D** ** T** **.txt.", "cite_spans": [], "ref_spans": [], "section": "(d) Predictions Using Human-Made Heuristics:"}, {"text": "The pipeline above was developed for choosing the variable ordering for the CAD implementation in Maple's Regular Chains Library [8, 9] . But it could be used to pick the variable ordering for other procedures which take sets of polynomials as input by changing the calls to CAD in Steps I(a) and II(e) to that of another implementation/algorithm.", "cite_spans": [{"start": 129, "end": 132, "text": "[8,", "ref_id": "BIBREF7"}, {"start": 133, "end": 135, "text": "9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Adapting the Pipeline to Other Algorithms"}, {"text": "Step II(d) would also have to be edited to call an appropriate competing heuristic.", "cite_spans": [], "ref_spans": [], "section": "Adapting the Pipeline to Other Algorithms"}, {"text": "The pipeline described in the previous section makes it easy for us to repeat our past experiments (described in Sect. 2) for a new dataset. All that is needed to do is replace the files storing the polynomials and run the pipeline.", "cite_spans": [], "ref_spans": [], "section": "Application of Pipeline to New Dataset"}, {"text": "To demonstrate this we test the proposed pipeline on a new dataset of randomly generated polynomials. We are not suggesting that it is appropriate to test classifiers on random data: we simply mean to demonstrate the ease with which the experiments in [15] [16] [17] that originally took many man-hours can be repeated with just a single code execution.", "cite_spans": [{"start": 252, "end": 256, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 257, "end": 261, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 262, "end": 266, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Application of Pipeline to New Dataset"}, {"text": "The randomly generated parameters are: the degrees of the three variables in each polynomial term, the coefficient of each term, the number of terms in a polynomial and the number of polynomials in a set. The means and standard deviations of these parameters were extracted from the problems in the nlsat dataset 3 , which was used in our previous work [15] so that the dataset is of a comparable scale. We generated 7500 sets of random polynomials, where 5000 were used for training, and the remaining 2500 for testing. The results of the proposed processing pipeline, including the comparison with the existing human-made heuristics are given in Table 1 . The prediction time is the time taken for the classifier or heuristic to make its predictions for the problems in the training set. The total time adds to this the time for the actual CAD computations using the suggested orderings. We do not report the training time of the ML as this is a cost paid only once in advance. The virtual solvers are those which always make the best/worst choice for a problem (in zero prediction time) and are useful to show the range of possible outcomes. We note that further details on our experimental methodology are given in [15] [16] [17] .", "cite_spans": [{"start": 353, "end": 357, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1219, "end": 1223, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1224, "end": 1228, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 1229, "end": 1233, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [{"start": 648, "end": 655, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Application of Pipeline to New Dataset"}, {"text": "As with the tests on the original dataset [15, 16] the ML classifiers outperformed the human made heuristics, but for this dataset the difference compared to the Brown heuristic was marginal. We used a lower CAD timeout which may benefit the Brown heuristic as past analysis shows that when it makes suboptimal choices these tend to be much worse. We also note that the relative performance of the Brown heuristic fell significantly when used on problems with more than three variables in [17] . The results for the sotd heuristic are bad because it had a particularly long prediction time on this random dataset. We note that there is scope to parallelize sotd which may make it more competitive.", "cite_spans": [{"start": 42, "end": 46, "text": "[15,", "ref_id": "BIBREF14"}, {"start": 47, "end": 50, "text": "16]", "ref_id": "BIBREF15"}, {"start": 489, "end": 493, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Application of Pipeline to New Dataset"}, {"text": "We presented our software pipeline for training and testing ML classifiers that select the variable ordering to use for CAD, and described the results of an experiment applying it to a new dataset.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}, {"text": "The purpose of the experiment in Sect. 4 was to demonstrate that the pipeline can easily train classifiers that are competitive on a new dataset with almost no additional human effort, at least for a dataset of a similar scale (we note that the code is designed to work on higher degree polynomials but has only been tested on datasets of 3 and 4 variables so far). The pipeline makes it possible for a user to easily tune the CAD variable ordering choice classifiers to their particular application area.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}, {"text": "Further, with only a little modification, as noted at the end of Sect. 3, the pipeline could be used to select the variable ordering for alternative algorithms that act on sets of polynomials and require a variable ordering. We thus expect the pipeline to be a useful basis for future research and plan to experiment with its use on such alternative algorithms in the near future.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "DeepMathdeep sequence models for premise selection", "authors": [{"first": "A", "middle": [], "last": "Alemi", "suffix": ""}, {"first": "F", "middle": [], "last": "Chollet", "suffix": ""}, {"first": "N", "middle": [], "last": "Een", "suffix": ""}, {"first": "G", "middle": [], "last": "Irving", "suffix": ""}, {"first": "C", "middle": [], "last": "Szegedy", "suffix": ""}, {"first": "J", "middle": [], "last": "Urban", "suffix": ""}], "year": 2016, "venue": "Proceedings of the NIPS 2016", "volume": "", "issn": "", "pages": "2243--2251", "other_ids": {"DOI": ["10.5555/3157096.3157347"]}}, "BIBREF1": {"ref_id": "b1", "title": "Identifying the parametric occurrence of multiple steady states for some biological networks", "authors": [{"first": "R", "middle": [], "last": "Bradford", "suffix": ""}], "year": 2020, "venue": "J. Symb. Comput", "volume": "98", "issn": "", "pages": "84--119", "other_ids": {"DOI": ["10.1016/j.jsc.2019.07.008"]}}, "BIBREF2": {"ref_id": "b2", "title": "Optimising problem formulation for cylindrical algebraic decomposition", "authors": [{"first": "R", "middle": [], "last": "Bradford", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Davenport", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "D", "middle": [], "last": "Wilson", "suffix": ""}, {"first": "J", "middle": [], "last": "Carette", "suffix": ""}, {"first": "D", "middle": [], "last": "Aspinall", "suffix": ""}, {"first": "C", "middle": [], "last": "Lange", "suffix": ""}, {"first": "P", "middle": [], "last": "Sojka", "suffix": ""}], "year": 2013, "venue": "CICM 2013", "volume": "7961", "issn": "", "pages": "19--34", "other_ids": {"DOI": ["10.1007/978-3-642-39320-4_2"]}}, "BIBREF3": {"ref_id": "b3", "title": "Companion to the tutorial: cylindrical algebraic decomposition", "authors": [{"first": "C", "middle": [], "last": "Brown", "suffix": ""}], "year": 2004, "venue": "Presented at ISSAC 2004", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "The complexity of quantifier elimination and cylindrical algebraic decomposition", "authors": [{"first": "C", "middle": [], "last": "Brown", "suffix": ""}, {"first": "J", "middle": [], "last": "Davenport", "suffix": ""}], "year": 2007, "venue": "Proceedings of the ISSAC 2007", "volume": "", "issn": "", "pages": "54--60", "other_ids": {"DOI": ["10.1145/1277548.1277557"]}}, "BIBREF5": {"ref_id": "b5", "title": "Understanding expression simplification", "authors": [{"first": "J", "middle": [], "last": "Carette", "suffix": ""}], "year": 2004, "venue": "Proceedings of the ISSAC", "volume": "", "issn": "", "pages": "72--79", "other_ids": {"DOI": ["10.1145/1005285.1005298"]}}, "BIBREF6": {"ref_id": "b6", "title": "Quantifier Elimination and Cylindrical Algebraic Decomposition. Texts & Monographs in Symbolic Computation", "authors": [{"first": "B", "middle": [], "last": "Caviness", "suffix": ""}, {"first": "J", "middle": [], "last": "Johnson", "suffix": ""}], "year": 1998, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/978-3-7091-9459-1"]}}, "BIBREF7": {"ref_id": "b7", "title": "Computing cylindrical algebraic decomposition via triangular decomposition", "authors": [{"first": "C", "middle": [], "last": "Chen", "suffix": ""}, {"first": "M", "middle": [], "last": "Moreno Maza", "suffix": ""}, {"first": "B", "middle": [], "last": "Xia", "suffix": ""}, {"first": "L", "middle": [], "last": "Yang", "suffix": ""}], "year": 2009, "venue": "Proceedings of the ISSAC 2009", "volume": "", "issn": "", "pages": "95--102", "other_ids": {"DOI": ["10.1145/1576702.1576718"]}}, "BIBREF8": {"ref_id": "b8", "title": "Quantifier elimination by cylindrical algebraic decomposition based on regular chains", "authors": [{"first": "C", "middle": [], "last": "Chen", "suffix": ""}, {"first": "M", "middle": [], "last": "Moreno Maza", "suffix": ""}], "year": 2016, "venue": "J. Symb. Comput", "volume": "75", "issn": "", "pages": "74--93", "other_ids": {"DOI": ["10.1016/j.jsc.2015.11.008"]}}, "BIBREF9": {"ref_id": "b9", "title": "Quantifier elimination for real closed fields by cylindrical algebraic decompostion", "authors": [{"first": "G", "middle": ["E"], "last": "Collins", "suffix": ""}], "year": 1975, "venue": "GI-Fachtagung 1975", "volume": "33", "issn": "", "pages": "134--183", "other_ids": {"DOI": ["10.1007/3-540-07407-4_17"]}}, "BIBREF10": {"ref_id": "b10", "title": "Efficient projection orders for CAD", "authors": [{"first": "A", "middle": [], "last": "Dolzmann", "suffix": ""}, {"first": "A", "middle": [], "last": "Seidl", "suffix": ""}, {"first": "T", "middle": [], "last": "Sturm", "suffix": ""}], "year": 2004, "venue": "Proceedings of the ISSAC", "volume": "", "issn": "", "pages": "111--118", "other_ids": {"DOI": ["10.1145/1005285.1005303"]}}, "BIBREF11": {"ref_id": "b11", "title": "Machine learning for mathematical software", "authors": [{"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Davenport", "suffix": ""}, {"first": "M", "middle": [], "last": "Kauers", "suffix": ""}, {"first": "G", "middle": [], "last": "Labahn", "suffix": ""}], "year": 2018, "venue": "ICMS 2018", "volume": "10931", "issn": "", "pages": "165--174", "other_ids": {"DOI": ["10.1007/978-3-319-96418-8_20"]}}, "BIBREF12": {"ref_id": "b12", "title": "Cylindrical algebraic decomposition with equational constraints", "authors": [{"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "R", "middle": [], "last": "Bradford", "suffix": ""}, {"first": "J", "middle": [], "last": "Davenport", "suffix": ""}], "year": 2020, "venue": "J. Symb. Comput", "volume": "100", "issn": "", "pages": "38--71", "other_ids": {"DOI": ["10.1016/j.jsc.2019.07.019"]}}, "BIBREF13": {"ref_id": "b13", "title": "Choosing a variable ordering for truth-table invariant cylindrical algebraic decomposition by incremental triangular decomposition", "authors": [{"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "R", "middle": [], "last": "Bradford", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Davenport", "suffix": ""}, {"first": "D", "middle": [], "last": "Wilson", "suffix": ""}], "year": 2014, "venue": "ICMS 2014", "volume": "8592", "issn": "", "pages": "450--457", "other_ids": {"DOI": ["10.1007/978-3-662-44199-2_68"]}}, "BIBREF14": {"ref_id": "b14", "title": "Comparing machine learning models to choose the variable ordering for cylindrical algebraic decomposition", "authors": [{"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "D", "middle": [], "last": "Florescu", "suffix": ""}], "year": 2019, "venue": "CICM 2019", "volume": "11617", "issn": "", "pages": "93--108", "other_ids": {"DOI": ["10.1007/978-3-030-23250-4_7"]}}, "BIBREF15": {"ref_id": "b15", "title": "Algorithmically generating new algebraic features of polynomial systems for machine learning", "authors": [{"first": "D", "middle": [], "last": "Florescu", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}], "year": 2019, "venue": "Proceedings of the SC 2 2019. CEUR-WS", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Improved cross-validation for classifiers that make algorithmic choices to minimise runtime without compromising output correctness", "authors": [{"first": "D", "middle": [], "last": "Florescu", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}], "year": 2020, "venue": "MACIS 2019", "volume": "11989", "issn": "", "pages": "341--356", "other_ids": {"DOI": ["10.1007/978-3-030-43120-4_27"]}}, "BIBREF17": {"ref_id": "b17", "title": "Solving the conjugacy decision problem via machine learning", "authors": [{"first": "J", "middle": [], "last": "Gryak", "suffix": ""}, {"first": "R", "middle": [], "last": "Haralick", "suffix": ""}, {"first": "D", "middle": [], "last": "Kahrobaei", "suffix": ""}], "year": 2020, "venue": "Exp. Math", "volume": "29", "issn": "1", "pages": "66--78", "other_ids": {"DOI": ["10.1080/10586458.2018.1434704"]}}, "BIBREF18": {"ref_id": "b18", "title": "Using machine learning to improve cylindrical algebraic decomposition", "authors": [{"first": "Z", "middle": [], "last": "Huang", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "D", "middle": [], "last": "Wilson", "suffix": ""}, {"first": "J", "middle": [], "last": "Bridge", "suffix": ""}, {"first": "J", "middle": [], "last": "Davenport", "suffix": ""}, {"first": "L", "middle": [], "last": "Paulson", "suffix": ""}], "year": 2019, "venue": "Math. Comput. Sci", "volume": "13", "issn": "4", "pages": "461--488", "other_ids": {"DOI": ["10.1007/s11786-019-00394-8"]}}, "BIBREF19": {"ref_id": "b19", "title": "Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition", "authors": [{"first": "Z", "middle": [], "last": "Huang", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}, {"first": "D", "middle": [], "last": "Wilson", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Davenport", "suffix": ""}, {"first": "L", "middle": ["C"], "last": "Paulson", "suffix": ""}, {"first": "J", "middle": [], "last": "Bridge", "suffix": ""}], "year": 2014, "venue": "CICM 2014", "volume": "8543", "issn": "", "pages": "92--107", "other_ids": {"DOI": ["10.1007/978-3-319-08434-3_8"]}}, "BIBREF20": {"ref_id": "b20", "title": "Code optimization in FORM", "authors": [{"first": "J", "middle": [], "last": "Kuipers", "suffix": ""}, {"first": "T", "middle": [], "last": "Ueda", "suffix": ""}, {"first": "J", "middle": [], "last": "Vermaseren", "suffix": ""}], "year": 2015, "venue": "Comput. Phys. Commun", "volume": "189", "issn": "", "pages": "1--19", "other_ids": {"DOI": ["10.1016/j.cpc.2014.08.008"]}}, "BIBREF21": {"ref_id": "b21", "title": "Learning rate based branching heuristic for SAT solvers", "authors": [{"first": "J", "middle": ["H"], "last": "Liang", "suffix": ""}, {"first": "V", "middle": [], "last": "Ganesh", "suffix": ""}, {"first": "P", "middle": [], "last": "Poupart", "suffix": ""}, {"first": "K", "middle": [], "last": "Czarnecki", "suffix": ""}], "year": 2016, "venue": "SAT 2016", "volume": "9710", "issn": "", "pages": "123--140", "other_ids": {"DOI": ["10.1007/978-3-319-40970-2_9"]}}, "BIBREF22": {"ref_id": "b22", "title": "TheoryGuru: a mathematica package to apply quantifier elimination technology to economics", "authors": [{"first": "C", "middle": ["B"], "last": "Mulligan", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Davenport", "suffix": ""}, {"first": "M", "middle": [], "last": "England", "suffix": ""}], "year": 2018, "venue": "ICMS 2018", "volume": "10931", "issn": "", "pages": "369--378", "other_ids": {"DOI": ["10.1007/978-3-319-96418-8_44"]}}, "BIBREF23": {"ref_id": "b23", "title": "Scikit-learn: machine learning in Python", "authors": [{"first": "F", "middle": [], "last": "Pedregosa", "suffix": ""}], "year": 2011, "venue": "J. Mach. Learn. Res", "volume": "12", "issn": "", "pages": "2825--2830", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "New domains for applied quantifier elimination", "authors": [{"first": "T", "middle": [], "last": "Sturm", "suffix": ""}], "year": null, "venue": "CASC 2006", "volume": "4194", "issn": "", "pages": "295--301", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "polynomials are then defined by lists of monomials given in this format, and a point in the training dataset consists of a list of polynomials. For example, one entry in the dataset is the set {235x 1 + 42x 2 2 , 2x 2 1 x 3 \u2212 1} which is represented as [[((1, 0, 0), 235) , ((0, 2, 0), 42)] , [((2, 0, 1), 2) , ((0, 0, 0), \u22121)]] .", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Generating feature avp (maxm (d m,p 1 )) from data stored in the format of Section I(b). Here d m,p 1 denotes the degree of variable x1 in polynomial number p and monomial number m, and avp denotes the average function computed for all polynomials[16].", "latex": null, "type": "figure"}, "FIGREF2": {"text": "a) Polynomial Data Parsing: The values generated are saved in a new file called terms test.txt. (b) Feature Generation: The final set of features is computed by evaluating the descriptions in Step I(b)ii for the testing dataset. These values are saved in a new file called features test.txt.", "latex": null, "type": "figure"}, "TABREF0": {"text": "The comparative performance of DT, KNN, MLP, SVM, the Brown and sotd heuristics on the testing data for our randomly generated dataset. A random prediction, and the virtual best (VB) and virtual worst (VW) predictions are also included.", "latex": null, "type": "table"}}, "back_matter": [{"text": "Acknowledgements. This work is funded by EPSRC Project EP/R019622/1: Embedding Machine Learning within Quantifier Elimination Procedures. We thank the anonymous referees for their comments.", "cite_spans": [], "ref_spans": [], "section": "acknowledgement"}]}