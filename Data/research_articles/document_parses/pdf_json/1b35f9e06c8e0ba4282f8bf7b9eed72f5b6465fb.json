{"paper_id": "1b35f9e06c8e0ba4282f8bf7b9eed72f5b6465fb", "metadata": {"title": "Studying Attention Models in Sentiment Attitude Extraction Task", "authors": [{"first": "Nicolay", "middle": [], "last": "Rusnachenko", "suffix": "", "affiliation": {"laboratory": "", "institution": "Bauman Moscow State Technical University", "location": {"settlement": "Moscow", "country": "Russia"}}, "email": ""}, {"first": "(", "middle": ["B"], "last": "", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Natalia", "middle": [], "last": "Loukachevitch", "suffix": "", "affiliation": {"laboratory": "", "institution": "Bauman Moscow State Technical University", "location": {"settlement": "Moscow", "country": "Russia"}}, "email": ""}]}, "abstract": [{"text": "In the sentiment attitude extraction task, the aim is to identify \u00abattitudes\u00bb -sentiment relations between entities mentioned in text. In this paper, we provide a study on attention-based context encoders in the sentiment attitude extraction task. For this task, we adapt attentive context encoders of two types: (I) feature-based; (II) self-based. Our experiments (https://github.com/nicolay-r/attitu de-extraction-with-attention) with a corpus of Russian analytical texts RuSentRel illustrate that the models trained with attentive encoders outperform ones that were trained without them and achieve 1.5-5.9% increase by F 1. We also provide the analysis of attention weight distributions in dependence on the term type.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Classifying relations between entities mentioned in texts remains one of the popular tasks in natural language processing (NLP). The sentiment attitude extraction task aims to seek for positive/negative relations between objects expressed as named entities in texts [10] . Let us consider the following sentence as an example (named entities are underlined):", "cite_spans": [{"start": 266, "end": 270, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Introduction"}, {"text": "\"Meanwhile Moscow has repeatedly emphasized that its activity in the Baltic Sea is a response precisely to actions of NATO and the escalation of the hostile approach to Russia near its eastern borders\"", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In the example above, named entities \u00abRussia\u00bb and \u00abNATO\u00bb have the negative attitude towards each other with additional indication of other named entities. The complexity of the sentence structure is one of the greatest difficulties one encounters when dealing with the relation extraction task. Texts usually contain a lot of named entity mentions; a single opinion might comprise several sentences.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "This paper is devoted to study of models for targeted sentiment analysis with attention. The intuition exploited in the models with attentive encoders is that not all terms in the context are relevant for attitude indication. The interactions of words, not just their isolated presence, may reveal the specificity of contexts with attitudes of different polarities. The primary contribution of this work is an application of attentive encoders based on (I) sentiment frames and attitude participants (features); (II) context itself. We conduct the experiments on the RuSentRel [7] collection. The results demonstrate that attentive models with CNN-based and over LSTM-based encoders result in 1.5-5.9% by F 1 over models without attentive encoders.", "cite_spans": [{"start": 577, "end": 580, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Introduction"}, {"text": "In previous works, various neural network approaches for targeted sentiment analysis were proposed. In [10] the authors utilize convolutional neural networks (CNN). Considering relation extraction as a three-scale classification task of contexts with attitudes in it, the authors subdivide each context into outer and inner (relative to attitude participants) to apply Piecewise-CNN (PCNN) [16] . The latter architecture utilizes a specific idea of max-pooling operation. Initially, this is an operation, which extracts the maximal values within each convolution. However, for relation classification, it reduces information extremely rapid and blurs significant aspects of context parts. In case of PCNN, separate maxpooling operations are applied to outer and inner contexts. In the experiments, the authors revealed a fast training process and a slight improvement in the PCNN results in comparison to CNN.", "cite_spans": [{"start": 103, "end": 107, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 390, "end": 394, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Related Work"}, {"text": "In [12] , the authors proposed an attention-based CNN model for semantic relation classification [4] . The authors utilized the attention mechanism to select the most relevant context words with respect to participants of a semantic relation. The architecture of the attention model is a multilayer perceptron (MLP), which calculates the weight of a word in context with respect to the entity. The resulting AttCNN model outperformed several CNN and LSTM based approaches with 2.6-3.8% by F1-measure.", "cite_spans": [{"start": 3, "end": 7, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 97, "end": 100, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Related Work"}, {"text": "In [9] , the authors experimented with attentive models in aspect-based sentiment analysis. The models were aimed to identify sentiment polarity of specific targets in context, which are characteristics or parts of an entity. Both targets and the context were treated as sequences. The authors proposed an interactive attention network (IAN), which establishes element relevance of one sequence with the other in two directions: targets to context, context to targets. The effectiveness of IAN was demonstrated on the SemEval-2014 dataset [13] and several biomedical datasets [1] .", "cite_spans": [{"start": 3, "end": 6, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 539, "end": 543, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 576, "end": 579, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Related Work"}, {"text": "In [14, 17] , the authors experimented with self-based attention models, in which targets became adapted automatically during the training process. Comparing with IAN, the presence of targets might be unclear in terms of algorithms.", "cite_spans": [{"start": 3, "end": 7, "text": "[14,", "ref_id": "BIBREF13"}, {"start": 8, "end": 11, "text": "17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Related Work"}, {"text": "The authors considered the attention as context word quantification with respect to abstract targets. In [14] , the authors brought a similar idea also onto the sentence level. The obtained hierarchical model was called as HAN.", "cite_spans": [{"start": 105, "end": 109, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Related Work"}, {"text": "We consider sentiment analysis of Russian analytical articles collected in the RuSentRel corpus [8] . The corpus comprises texts in the international politics domain and contains a lot of opinions. The articles are labeled with annotations of two types: (I) the author's opinion on the subject matter of the article; (II) the attitudes between the participants of the described situations. The annotation of the latter type includes 2000 relations across 73 large analytical texts. Annotated sentiments can be only positive or negative. Additionally, each text is provided with annotation of mentioned named entities. Synonyms and variants of named entities are also given, which allows not to deal with the coreference of named entities.", "cite_spans": [{"start": 96, "end": 99, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Data and Lexicons"}, {"text": "In our study, we also use two Russian sentiment resources: the RuSentiLex lexicon [7] , which contains words and expressions of the Russian language with sentiment labels and the RuSentiFrames lexicon [11] , which provides several types of sentiment attitudes for situations associated with specific Russian predicates.", "cite_spans": [{"start": 82, "end": 85, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 201, "end": 205, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Data and Lexicons"}, {"text": "The RuSentiFrames 1 lexicon describes sentiments and connotations conveyed with a predicate in a verbal or nominal form [11] , such as \" \" (to condemn, to improve, to exaggerate), etc. The structure of the frames in RuSentFrames comprises: (I) the set of predicate-specific roles; (II) frames dimensions such as the attitude of the author towards participants of the situation, attitudes between the participants, effects for participants. Currently, RuSentiFrames contains frames for more than 6 thousand words and expressions.", "cite_spans": [{"start": 120, "end": 124, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Data and Lexicons"}, {"text": "In RuSentiFrames, individual semantic roles are numbered, beginning with zero. For a particular predicate entry, Arg0 is generally the argument exhibiting features of a Prototypical Agent, while Arg1 is a Prototypical Patient or Theme [2] . In the main part of the frame, the most applicable for the current study is the polarity of Arg0 with a respect to Arg1 (A0\u2192A1). For example, in case of Russian verb \" \" (to approve) the sentiment polarity A0\u2192A1 is positive.", "cite_spans": [{"start": 235, "end": 238, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Data and Lexicons"}, {"text": "In this paper, the task of sentiment attitude extraction is treated as follows: given a pair of named entities, we predict a sentiment label of a pair, which could be positive, negative, or neutral. As the RuSentRel corpus provides opinions with positive or negative sentiment labels only (Sect. 3), we automatically added neutral sentiments for all pairs not mentioned in the annotation and co-occurred in the same sentences of the collection texts. We consider a context as a text fragment that is limited by a single sentence and includes a pair of named entities. The general architecture is presented in Fig. 1 (left) , where the sentiment could be extracted from the context. To present a context, we treat the original text as a sequence of terms [t 1 , . . . , t n ] limited by n. Each term belongs to one of the following classes: entities, frames, tokens, and words (if none of the prior has not been matched). We use masked representation for attitude participants (E obj , E subj ) and mentioned named entities (E) to prevent models from capturing related information.", "cite_spans": [], "ref_spans": [{"start": 609, "end": 622, "text": "Fig. 1 (left)", "ref_id": "FIGREF0"}], "section": "Model"}, {"text": "To represent frames, we combine a frame entry with the corresponding A0\u2192A1 sentiment polarity value (and neutral if the latter is absent). We also invert sentiment polarity when an entry has \" \" (not) preposition. For example, in Fig. 1 (right) all entries are encoded with the negative polarity A0\u2192A1: \" \" (confrontation) has a negative polarity, and \" \" (not necessary) has a positive polarity of entry \"necessary\" which is inverted due to the \"not\" preposition.", "cite_spans": [], "ref_spans": [{"start": 230, "end": 236, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Model"}, {"text": "The tokens group includes: punctuation marks, numbers, url-links. Each term of words is considered in a lemmatized 2 form. Figure 1 (right) provides a context example with the corresponding representation (\u00abterms\u00bb block).", "cite_spans": [], "ref_spans": [{"start": 123, "end": 131, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Model"}, {"text": "To represent the context in a model, each term is embedded with a vector of fixed dimension. The sequence of embedded vectors X = [x 1 , . . . , x n ] is denoted as input embedding (x i \u2208 R m , i \u2208 1..n). Sections 4.1 and 4.2 provide an encoder implementation in details. In particular, each encoder relies on input embedding and generates output embedded context vector s. Fig. 2 . AttCNN neural network [6] In order to determine a sentiment class by the embedded context s, we apply: (I) the hyperbolic tangent activation function towards s and (II) transformation through the fully connected layer :", "cite_spans": [{"start": 405, "end": 408, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [{"start": 374, "end": 380, "text": "Fig. 2", "ref_id": null}], "section": "Model"}, {"text": "In Formula 1, W r , b r corresponds to hidden states; z correspond to the size of vector s, and c is a number of classes. Finally, to obtain an output vector of", "cite_spans": [], "ref_spans": [], "section": "Model"}, {"text": ", we use sof tmax operation:", "cite_spans": [], "ref_spans": [], "section": "Model"}, {"text": "In this section, we consider features as a significant for attitude identification context terms, towards which we would like to quantify the relevance of each term in the context. For a particular context, we select embedded values of the (I) attitude participants (E obj , E subj ) and (II) terms of the frames group and create a set of features F = [f 1 , . . . , f k ] limited by k.", "cite_spans": [], "ref_spans": [], "section": "Feature Attentive Context Encoders"}, {"text": "MLP-Attention. Figure 2 illustrates a feature-attentive encoder with the quantification approach called Multi-Layer Perceptron [6] . In formulas 3-5, we describe the quantification process of a context embedding X with respect to a particular feature f \u2208 F . Given an i'th embedded term x i , we concatenate its representation with f:", "cite_spans": [{"start": 127, "end": 130, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [{"start": 15, "end": 23, "text": "Figure 2", "ref_id": null}], "section": "Feature Attentive Context Encoders"}, {"text": "The quantification of the relevance of x i with respect to f is denoted as u i \u2208 R and calculated as follows (see Fig. 2a ):", "cite_spans": [], "ref_spans": [{"start": 114, "end": 121, "text": "Fig. 2a", "ref_id": null}], "section": "Feature Attentive Context Encoders"}, {"text": "In Formula 4, W we and W a correspond to the weight and attention matrices respectively, and h mlp corresponds to the size of the hidden representation in the weight matrix. To deal with normalized weights within a context, we transform quantified values u i into probabilities \u03b1 i using sof tmax operation (Formula 2). We utilize Formula 5 to obtain attention-based context embedding\u015d of a context with respect to feature f:\u015d", "cite_spans": [], "ref_spans": [], "section": "Feature Attentive Context Encoders"}, {"text": "Applying Formula 5 towards each feature f j \u2208 F , j \u2208 1..k results in vector {\u015d j } k j=1 . We use average-pooling to transform the latter sequence into single averaged vector", "cite_spans": [], "ref_spans": [], "section": "Feature Attentive Context Encoders"}, {"text": "We also utilize a CNN-based encoder (Fig. 2b) to compete the context representation s cnn \u2208 R c , where c is related to convolutional filters count [10] . The resulting context embedding vector s (size of z = m + c) is a concatenation of s f and s cnn .", "cite_spans": [{"start": 148, "end": 152, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [{"start": 36, "end": 45, "text": "(Fig. 2b)", "ref_id": null}], "section": "Feature Attentive Context Encoders"}, {"text": "IAN. As a context encoder, a Recurrent Neural Network (RNN) model allows treating the context [t 1 , . . . , t n ] as a sequence of terms to generate a hidden representation, enriched with features of previously appeared terms. In comparison with CNN, the application of rnn allows keeping a history of the whole sequence while CNN-based encoders remain limited by the window size. The application of RNN towards a context and certain features appeared in it -is another way how the correlation of these both factors could be quantitatively measured [9] . Figure 3a illustrates the IAN architecture attention encoder. The input assumes separated sequences of embedded terms X and embedded features F . To learn the hidden term semantics for each input, we utilize the LSTM [5] recurrent neural network architecture, which addresses learning long-term dependencies by avoiding gradient vanishing and expansion problems. The calculation h t of t'th embedded term x t based on prior state h t\u22121 , where the latter acts as a parameter of auxiliary functions [5] . ", "cite_spans": [{"start": 550, "end": 553, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 1054, "end": 1057, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [{"start": 556, "end": 565, "text": "Figure 3a", "ref_id": "FIGREF1"}], "section": "Feature Attentive Context Encoders"}, {"text": ".n, j \u2208 1..k) and h is the size of the hidden representation. The quantification of input sequences is carried out in the following directions: (I) feature representation with respect to context, and (II) context representation with respect to features. To obtain the representation of a hidden sequence, we utilize average-pooling. In Fig. 3a , p f and p c denote a hidden representation of features and context respectively. Figure 3b illustrates the quantification computation of a hidden state h t with respect to p: ", "cite_spans": [], "ref_spans": [{"start": 336, "end": 343, "text": "Fig. 3a", "ref_id": "FIGREF1"}, {"start": 427, "end": 436, "text": "Figure 3b", "ref_id": "FIGREF1"}], "section": "Feature Attentive Context Encoders"}, {"text": "In order to deal with normalized weight vectors \u03b1 f i and \u03b1 c j , we utilize the sof tmax operation for u f and u c respectively (Formula 2). The resulting context vector s (size of z = 2 \u00b7 h) is a concatenation of weighted context s c and features s f representations:", "cite_spans": [], "ref_spans": [], "section": "Feature Attentive Context Encoders"}, {"text": "In Sect. 4.1 the application of attention in context embedding fully relies on the sequence of predefined features. The quantification of context terms is performed towards each feature. In turn, the self-attentive approach assumes to quantify a context with respect to an abstract parameter. Unlike quantification methods in feature-attentive embedding models, here the latter is replaced with a hidden state (parameter w, see Fig. 4b ), which modified during the training process. Figure 4a illustrates the bi-directional RNN-based self-attentive context encoder architecture. We utilize bi-directional LSTM (BiLSTM) to obtain a pair of sequences ", "cite_spans": [], "ref_spans": [{"start": 428, "end": 435, "text": "Fig. 4b", "ref_id": "FIGREF2"}, {"start": 483, "end": 492, "text": "Figure 4a", "ref_id": "FIGREF2"}], "section": "Self Attentive Context Encoders"}, {"text": "The quantification of hidden term representation h i \u2208 R 2\u00b7h with respect to w \u2208 R 2\u00b7h is described in formulas 8-9 and illustrated in Fig. 4b . We apply the sof tmax operation towards u i to obtain vector of normalized weights \u03b1 \u2208 R n . The resulting context embedding vector s (size of z = 2 \u00b7 h) is an activated weighted sum of each parameter of context hidden states:", "cite_spans": [], "ref_spans": [{"start": 135, "end": 142, "text": "Fig. 4b", "ref_id": "FIGREF2"}], "section": "Self Attentive Context Encoders"}, {"text": "Input Embedding Details. We provide embedding details of context term groups described in Sect. 4. For words and frames, we look up for vectors in precomputed and publicly available model 3 M word based on news articles with window size of 20, and vector size of 1000. Each term that is not presented in the model we treat as a sequence of parts (n-grams) and look up for related vectors in M word to complete an averaged vector. For a particular part, we start with a trigram (n = 3) and decrease n until the related n-gram is found. For masked entities (E, E obj , E subj ) and tokens, each element embedded with a randomly initialized vector with size of 1000. Each context term has been additionally expanded with the following parameters: -Distance embedding [10] (v d-obj , v d-subj ) -is vectorized distance in terms from attitude participants of entry pair (E obj and E subj respectively) to a given term; -Closest to synonym distance embedding (v sd-obj , v sd-subj ) is a vectorized absolute distance in terms from a given term towards the nearest entity, synonymous to E obj and E subj respectively; -Part-of-speech embedding (v pos ) is a vectorized tag for words (for terms of other groups considering \u00abunknown\u00bb tag); -A0\u2192A1 polarity embedding (v A0\u2192A1 ) is a vectorized \u00abpositive\u00bb or \u00abnegative\u00bb value for frame entries whose description in RuSentiFrames provides the corresponding polarity (otherwise considering \u00abneutral\u00bb value); polarity is inverted when an entry has \" \" (not) preposition.", "cite_spans": [{"start": 764, "end": 768, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Model Details"}, {"text": "Training. This process assumes hidden parameter optimization of a given model. We utilize an algorithm described in [10] . The input is organized in minibatches, where minibatch yields of l bags. Each bag has a set of t pairs X j , y j In order to evaluate and assess attention-based models, we provide a list of baseline models. These are independent encoders described in Sects. 4.1 and 4.2: PCNN [10] , LSTM, BiLSTM. In case of models with feature-based attentive encoders (IAN * , PCNN * ) we experiment with following feature sets: attitude participants only (att-ends), and frames with attitude participants (att-ef ). For self-based attentive encoders we experiment with Att-BLSTM (Sect. 4 ", "cite_spans": [{"start": 116, "end": 120, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 399, "end": 403, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 695, "end": 696, "text": "4", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Model Details"}, {"text": "Att-BLSTM z-yang -is a bi-directional LSTM model with word-based attentive encoder of HAN model [14] . Table 1 provides related results. For evaluating models in this task, we adopt macroaveraged F1-score (F 1) over documents. F1-score is considered averaging of the positive and negative class. We measure F 1 on train part every 10 epochs. The number of epochs was limited by 150. The training process terminates when F 1 on train part becomes greater than 0.85. Analyzing F 1 test results it is quite difficult to demarcate attention-based models from baselines except Att-BLSTM and PCNN att-ends . In turn, average results by F 1 in the case of CV-3 experiments illustrate the effectiveness of attention application. The average increase in the performance of such models over related baselines is as follows: 1.4% (PCNN * ), 1.2% (IAN * ), and 5.9% (Att-BLSTM, Att-BLSTM z-yang ) by F 1. The greatest increase in 9.8% by F 1 is achieved by Att-BLSTM model.", "cite_spans": [{"start": 96, "end": 100, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [{"start": 103, "end": 110, "text": "Table 1", "ref_id": "TABREF2"}], "section": ".2) and"}, {"text": "According to Sects. 4.1 and 4.2, attentive embedding models perform the quantification of terms in the context. The latter results in the probability distribution of weights 6 across the terms mentioned in a context. We utilize the test part of the RuSentRel dataset (Sect. 6) for analysis of weight distribution of frames group, declared in Sect. 4, across all input contexts. We also introduce two extra groups utilized in the analysis by separating the subset of words into prepositions (prep) and terms appeared in RuSentiLex lexicon (sentiment) described in Sect. 3.", "cite_spans": [], "ref_spans": [], "section": "Analysis of Attention Weights"}, {"text": "The context-level weight of a group is a weighted sum of terms which both appear in the context and belong the corresponding term group. Figure 5 illustrates the weight distribution plots, where the models are organized in rows, and the columns correspond to the term groups. Each plot combines distributions of context-levels weights across:", "cite_spans": [], "ref_spans": [{"start": 137, "end": 145, "text": "Figure 5", "ref_id": "FIGREF3"}], "section": "Analysis of Attention Weights"}, {"text": "-Neutral contexts -contexts, labeled as neutral; -Sentiment contexts -contexts, labeled with positive or negative labels.", "cite_spans": [], "ref_spans": [], "section": "Analysis of Attention Weights"}, {"text": "In Fig. 5 and further, the distribution of context-level weights across neutral (\u00abN\u00bb in legends) and sentiment contexts (\u00abS\u00bb in legends) denoted as \u03c1 g N and \u03c1 g S respectively. The rows in Fig. 5 correspond to the following models: (1) PCNN att-ef , (2) IAN ef , (3) Att-BLSTM. Analyzing prepositions (column 1) it is possible to see the lack of differences in quantification between the \u03c1 prep N and \u03c1 prep S contexts in the case of the models (1) and (2) . Another situation is in case of the model (3), where related terms in sentiment contexts are higher quantified than in neutral ones. frames and sentiment groups are slightly higher quantified in sentiment contexts than in neutral one in the case of models (1) and (2), while (3) illustrates a significant discrepancy.", "cite_spans": [{"start": 454, "end": 457, "text": "(2)", "ref_id": "BIBREF1"}], "ref_spans": [{"start": 3, "end": 9, "text": "Fig. 5", "ref_id": "FIGREF3"}, {"start": 190, "end": 196, "text": "Fig. 5", "ref_id": "FIGREF3"}], "section": "Analysis of Attention Weights"}, {"text": "Overall, model Att-BLSTM stands out among others both in terms of results (Sect. 6) and it illustrates the greatest discrepancy between \u03c1 N and \u03c1 S across all the groups presented in the analysis (Fig. 5) . We assume that the latter is achieved due to the following factors: (I) application of bi-directional LSTM encoder; (II) utilization of a single trainable vector (w) in the quantification process (Fig. 4b) while the models of other approaches (AttCNN, IAN, and Att-BLSTM z-yang ) depend on fully-connected layers. Figure 6 shows examples of those sentiment contexts in which the weight distribution is the largest among the frames group. These examples are the case when both frame and attention masks convey context meaning. Fig. 6 . Weight distribution visualization for model Att-BLSTM on sentiment contexts; for visualization purposes, weight of each term is normalized by maximum in context", "cite_spans": [], "ref_spans": [{"start": 196, "end": 204, "text": "(Fig. 5)", "ref_id": "FIGREF3"}, {"start": 403, "end": 412, "text": "(Fig. 4b)", "ref_id": "FIGREF2"}, {"start": 450, "end": 463, "text": "(AttCNN, IAN,", "ref_id": null}, {"start": 521, "end": 529, "text": "Figure 6", "ref_id": null}, {"start": 733, "end": 739, "text": "Fig. 6", "ref_id": null}], "section": "Analysis of Attention Weights"}, {"text": "In this paper, we study the attention-based models, aimed to extract sentiment attitudes from analytical articles. The described models should classify a context with an attitude mentioned in it onto the following classes: positive, negative, neutral. We investigated two types of attention embedding approaches: (I) feature-based, (II) self-based. We conducted experiments on Russian analytical texts of the RuSentRel corpus and provide the analysis of the results. According to the latter, the advantage of attention-based encoders over non-attentive was shown by the variety in weight distribution of certain term groups between sentiment and non-sentiment contexts. The application of attentive context encoders illustrates the classification improvement in 1.5-5.9% range by F 1.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Interactive attention network for adverse drug reaction classification", "authors": [{"first": "I", "middle": [], "last": "Alimova", "suffix": ""}, {"first": "V", "middle": [], "last": "Solovyev", "suffix": ""}, {"first": "D", "middle": [], "last": "Ustalov", "suffix": ""}, {"first": "A", "middle": [], "last": "Filchenkov", "suffix": ""}, {"first": "L", "middle": [], "last": "Pivovarova", "suffix": ""}], "year": 2018, "venue": "AINL 2018. CCIS", "volume": "930", "issn": "", "pages": "185--196", "other_ids": {"DOI": ["10.1007/978-3-030-01204-5_18"]}}, "BIBREF1": {"ref_id": "b1", "title": "Thematic proto-roles and argument selection", "authors": [{"first": "D", "middle": [], "last": "Dowty", "suffix": ""}], "year": 1991, "venue": "Language", "volume": "67", "issn": "3", "pages": "547--619", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Understanding the difficulty of training deep feedforward neural networks", "authors": [{"first": "X", "middle": [], "last": "Glorot", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2010, "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics", "volume": "", "issn": "", "pages": "249--256", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Semeval-2010 task 8: multi-way classification of semantic relations between pairs of nominals", "authors": [{"first": "I", "middle": [], "last": "Hendrickx", "suffix": ""}], "year": 2009, "venue": "Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions", "volume": "", "issn": "", "pages": "94--99", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Long short-term memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Attention-based convolutional neural network for semantic relation extraction", "authors": [{"first": "X", "middle": [], "last": "Huang", "suffix": ""}], "year": 2016, "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers", "volume": "", "issn": "", "pages": "2526--2536", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Creating a general Russian sentiment lexicon", "authors": [{"first": "N", "middle": [], "last": "Loukachevitch", "suffix": ""}, {"first": "A", "middle": [], "last": "Levchik", "suffix": ""}], "year": 2016, "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)", "volume": "", "issn": "", "pages": "1171--1176", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Extracting sentiment attitudes from analytical texts", "authors": [{"first": "N", "middle": [], "last": "Loukachevitch", "suffix": ""}, {"first": "N", "middle": [], "last": "Rusnachenko", "suffix": ""}], "year": 2018, "venue": "Proceedings of International Conference on Computational Linguistics and Intellectual Technologies Dialogue-2018", "volume": "", "issn": "", "pages": "459--468", "other_ids": {"arXiv": ["arXiv:1808.08932"]}}, "BIBREF8": {"ref_id": "b8", "title": "Interactive attention networks for aspect-level sentiment classification", "authors": [{"first": "D", "middle": [], "last": "Ma", "suffix": ""}, {"first": "S", "middle": [], "last": "Li", "suffix": ""}, {"first": "X", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "H", "middle": [], "last": "Wang", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1709.00893"]}}, "BIBREF9": {"ref_id": "b9", "title": "Neural network approach for extracting aggregated opinions from analytical articles", "authors": [{"first": "N", "middle": [], "last": "Rusnachenko", "suffix": ""}, {"first": "N", "middle": [], "last": "Loukachevitch", "suffix": ""}], "year": 2019, "venue": "DAMDID/RCDL 2018. CCIS", "volume": "1003", "issn": "", "pages": "167--179", "other_ids": {"DOI": ["10.1007/978-3-030-23584-0_10"]}}, "BIBREF10": {"ref_id": "b10", "title": "Distant supervision for sentiment attitude extraction", "authors": [{"first": "N", "middle": [], "last": "Rusnachenko", "suffix": ""}, {"first": "N", "middle": [], "last": "Loukachevitch", "suffix": ""}, {"first": "E", "middle": [], "last": "Tutubalina", "suffix": ""}], "year": 2019, "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Attention-based convolutional neural network for semantic relation extraction", "authors": [{"first": "Y", "middle": [], "last": "Shen", "suffix": ""}, {"first": "X", "middle": [], "last": "Huang", "suffix": ""}], "year": 2016, "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers", "volume": "", "issn": "", "pages": "2526--2536", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "DCU: aspect-based polarity classification for SemEval task", "authors": [{"first": "J", "middle": [], "last": "Wagner", "suffix": ""}], "year": 2014, "venue": "", "volume": "4", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Hierarchical attention networks for document classification", "authors": [{"first": "Z", "middle": [], "last": "Yang", "suffix": ""}, {"first": "D", "middle": [], "last": "Yang", "suffix": ""}, {"first": "C", "middle": [], "last": "Dyer", "suffix": ""}, {"first": "X", "middle": [], "last": "He", "suffix": ""}, {"first": "A", "middle": [], "last": "Smola", "suffix": ""}, {"first": "E", "middle": [], "last": "Hovy", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "1480--1489", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "ADADELTA: an adaptive learning rate method", "authors": [{"first": "M", "middle": ["D"], "last": "Zeiler", "suffix": ""}], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1212.5701"]}}, "BIBREF15": {"ref_id": "b15", "title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "authors": [{"first": "D", "middle": [], "last": "Zeng", "suffix": ""}, {"first": "K", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Chen", "suffix": ""}, {"first": "J", "middle": [], "last": "Zhao", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1753--1762", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Attention-based bidirectional long short-term memory networks for relation classification", "authors": [{"first": "P", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "2", "issn": "", "pages": "207--212", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "(left) General, context-based 3-scale (positive, negative, neutral) classification model, with details on \u00abAttention-Based Context Encoder\u00bb block in Sect. 4.1 and 4.2;(right) An example of a context processing into a sequence of terms; attitude participants (\u00abRussia\u00bb, \u00abTurkey\u00bb) and other mentioned entities become masked; frames are bolded and optionally colored corresponding to the sentiment value of A0\u2192A1 polarity.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Interactive Attention Network (IAN)[9]", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Attention-based bi-directional LSTM neural network (Att-BLSTM)[17]", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Kernel density estimations (KDE) of context-level weight distributions of term groups (from left to right: prep, frames, sentiment) across neutral (N) and sentiment (S) context sets for models: PCNN att-ef , IAN ef , Att-BLSTM; the probability range (x-axis) scaled to [0, 0.2]; vertical lines indicate expected values of distributions", "latex": null, "type": "figure"}, "TABREF0": {"text": "The application of LSTM towards the input sequences results in [h c", "latex": null, "type": "table"}, "TABREF1": {"text": "). The resulting context representation H = [h 1 , . . . , h n ] is composed as the concatenation of bi-directional sequences elementwise:", "latex": null, "type": "table"}, "TABREF2": {"text": "Three", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>F1avg </td><td>F11cv </td><td>F12cv </td><td>F13cv </td><td>F1TEST\n</td></tr><tr><td>Att-BLSTM </td><td>0.314 </td><td>0.35 </td><td>0.27 </td><td>0.32 </td><td>0.35\n</td></tr><tr><td>Att-BLSTMz-yang </td><td>0.292 </td><td>0.33 </td><td>0.25 </td><td>0.30 </td><td>0.33\n</td></tr><tr><td>BiLSTM </td><td>0.286 </td><td>0.32 </td><td>0.26 </td><td>0.28 </td><td>0.34\n</td></tr><tr><td>IANef </td><td>0.289 </td><td>0.31 </td><td>0.28 </td><td>0.27 </td><td>0.32\n</td></tr><tr><td>IANends </td><td>0.286 </td><td>0.31 </td><td>0.26 </td><td>0.29 </td><td>0.32\n</td></tr><tr><td>LSTM </td><td>0.284 </td><td>0.28 </td><td>0.27 </td><td>0.29 </td><td>0.32\n</td></tr><tr><td>PCNNatt-ends </td><td>0.297 </td><td>0.32 </td><td>0.29 </td><td>0.28 </td><td>0.35\n</td></tr><tr><td>PCNNatt-ef </td><td>0.289 </td><td>0.31 </td><td>0.25 </td><td>0.31 </td><td>0.31\n</td></tr><tr><td>PCNN </td><td>0.285 </td><td>0.29 </td><td>0.27 </td><td>0.30 </td><td>0.32\n</td></tr></table></body></html>"}}, "back_matter": [{"text": "t j=1 , where each pair is described by an input embedding X j with the related label y j \u2208 R c . The training process is iterative, and each iteration includes the following steps:1. Composing a minibatch of l bags of size t; 2. Performing forward propagation through the network which results in a vector (size of q = l \u00b7 t) of outputs o k \u2208 R c ; 3. Computing cross entropy loss for output:. L i\u00b7t to update hidden variables set; cost i is a maximal loss within i 'th bag;Parameters Settings. The minibatch size (l) is set to 2, where contexts count per bag t is set to 3. All the sentences were limited by 50 terms. For embedding parameters (v d-obj , v d-subj , v sd-obj , v sd-subj , v pos , v A0\u2192A1 ), we use randomly initialized vectors with size of 5. For CNN and PCNN context encoders, the size of convolutional window and filters count (c) were set to 3 and 300 respectively. As for parameters related to sizes of hidden states in Sect. 4: h mlp = 10, h = 128. For feature attentive encoders, we keep frames in order of their appearance in context and limit k by 5. We utilize the AdaDelta optimizer with parameters \u03c1 = 0.95 and = 10 \u22126 [15] . To prevent models from overfitting, we apply dropout towards the output with keep probability set to 0.8. We use Xavier weight initialization to setup initial values for hidden states [3] .", "cite_spans": [{"start": 1148, "end": 1152, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1339, "end": 1342, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "annex"}, {"text": "We conduct experiments with the RuSentRel 4 corpus in following formats:1. Using 3-fold cross-validation (CV), where all folds are equal in terms of the number of sentences; 2. Using predefined train/test separation 5 .", "cite_spans": [], "ref_spans": [], "section": "Experiments"}]}