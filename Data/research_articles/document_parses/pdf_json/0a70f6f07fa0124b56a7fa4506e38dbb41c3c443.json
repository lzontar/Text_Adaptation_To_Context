{"paper_id": "0a70f6f07fa0124b56a7fa4506e38dbb41c3c443", "metadata": {"title": "Dynamic Decision Boundary for One-class Classifiers applied to non-uniformly Sampled Data", "authors": [{"first": "Riccardo", "middle": ["La"], "last": "Grassa", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Insubria", "location": {"settlement": "Varese", "country": "Italy"}}, "email": ""}, {"first": "Ignazio", "middle": [], "last": "Gallo", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Insubria", "location": {"settlement": "Varese", "country": "Italy"}}, "email": ""}, {"first": "Nicola", "middle": [], "last": "Landro", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Insubria", "location": {"settlement": "Varese", "country": "Italy"}}, "email": ""}]}, "abstract": [{"text": "A typical issue in Pattern Recognition is the non-uniformly sampled data, which modifies the general performance and capability of machine learning algorithms to make accurate predictions. Generally, the data is considered non-uniformly sampled when in a specific area of data space, they are not enough, leading us to misclassification problems. This issue cut down the goal of the one-class classifiers decreasing their performance. In this paper, we propose a one-class classifier based on the minimum spanning tree with a dynamic decision boundary (OCdmst) to make good prediction also in the case we have non-uniformly sampled data. To prove the effectiveness and robustness of our approach we compare with the most recent one-class classifier reaching the state-of-the-art in most of them.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "In the deep learning era, the best models need to have many hundreds or thousands of instances to make good predictions and to reduce as little as possible misclassification. However, in the scenario where we have few samples of a specific class without knowing anything about other classes, the neural network models can not be used. In this case, different approaches exist that uses neural networks for this task (Perera and Patel (2019) ). They train the model using a dataset of a different context than the one in which the model will then be used. Although it offers good results in terms of classification, it does not be applied to a specific context, for instance, in case the source of the test set is very different from the data of the pre-trained model and consequently, the discrimination power model will be very low to generalize these test instances.", "cite_spans": [{"start": 416, "end": 440, "text": "(Perera and Patel (2019)", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Introduction"}, {"text": "In conjunction with few objects available and time required to train a model with a huge dataset, deep models might not be the right choice for this specific task. In La Grassa et al. (2020) , we used a neural network pre-trained on a large dataset as Imagenet to extract deep features and use them as input for a one-class classifier (OCC) based on MST (OCmst). They introduce another level of boundary used to label all uncertain This paper is under review to \"Pattern Recognition Letters\" Code https://gitlab.com/artelabsuper/ocdmst images and construct another one-class MST using all negative samples classified. The final step will classify all uncertain value starting from a couple of MST. Although the idea is interesting they do not handle dynamic boundaries but only different levels of static boundaries are treated. In literature, various approaches exist for one-class classification. For example, Tax and Duin (2002) and Krawczyk et al. (2018) use ensemble methods to find best partitions or to handle under/over-sampling in an unbalanced scenario. A classical SVM revised algorithm (OCSVM) (Sch\u00f6lkopf et al. (2001) ) offers good results with different kinds of data. Other interesting models are graph-based (Juszczak et al. (2009) , La Grassa et al. (2019b) , La Grassa et al. (2019a) , Liu et al. (2016a) ) and occur on a different domain as computer vision (Perera and Patel (2019) , Ruff et al. (2018) ), bioinformatics (Juszczak et al. (2009 ), Zhang et al. (2014 ), clustering (Liu et al. (2016a) ). Furthermore, in the literature other well-known models exist, like hybrid models. In this scenario, neural network models are used as deep features extractors jointly to classical machine learning classification algorithms like a one-class support vector machine. Sch\u00f6lkopf et al. (2001) , autoencoder+ocsvm Andrews et al. (2016) , autoencoder plus knn Song et al. (2017) , autoencoder plus svdd Kim et al. (2015) . Intuitively, the main advantages of using hybrid models are the reduced number of instances and to exploit the power of neural network models to provide the best discriminative features. In Chalapathy et al. (2018) and Ruff et al. (2018), the authors propose an approach to combine Convolutional Neural Networks (CNN) with SVDD, while optimizing the class boundary in the output features space. Some disadvantages of this methodology are the computational time required for the training step and in the scenario which we have few data this approach might not have good results. In this paper, we propose an extension of our previous works La Grassa et al. (2019b) and La Grassa et al. (2019a) , using a new approach based on inverse sigmoid function to handle the decision boundary in a dynamic methodology. We conduct a wide variety of experiments on different datasets to demonstrate the rightness of our model and compare it with the most recent oneclass classifiers, overcoming them in many tasks. Today, finding good models in particular situations such as in medicine is a necessity. We tested our model on Covid-19 disease using clinical images from radiopaedia and sirm, obtaining interesting results. In the following sections we describe the main approach using a minimum spanning tree as class descriptor 2, we introduce our approach 4 and finally we will discuss our results compared with other known models 5.", "cite_spans": [{"start": 170, "end": 190, "text": "Grassa et al. (2020)", "ref_id": "BIBREF9"}, {"start": 912, "end": 931, "text": "Tax and Duin (2002)", "ref_id": "BIBREF18"}, {"start": 936, "end": 958, "text": "Krawczyk et al. (2018)", "ref_id": "BIBREF6"}, {"start": 1106, "end": 1130, "text": "(Sch\u00f6lkopf et al. (2001)", "ref_id": "BIBREF16"}, {"start": 1224, "end": 1247, "text": "(Juszczak et al. (2009)", "ref_id": "BIBREF4"}, {"start": 1253, "end": 1274, "text": "Grassa et al. (2019b)", "ref_id": "BIBREF8"}, {"start": 1280, "end": 1301, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}, {"start": 1304, "end": 1322, "text": "Liu et al. (2016a)", "ref_id": "BIBREF10"}, {"start": 1376, "end": 1400, "text": "(Perera and Patel (2019)", "ref_id": "BIBREF13"}, {"start": 1403, "end": 1421, "text": "Ruff et al. (2018)", "ref_id": "BIBREF15"}, {"start": 1440, "end": 1462, "text": "(Juszczak et al. (2009", "ref_id": "BIBREF4"}, {"start": 1463, "end": 1484, "text": "), Zhang et al. (2014", "ref_id": "BIBREF22"}, {"start": 1499, "end": 1518, "text": "(Liu et al. (2016a)", "ref_id": "BIBREF10"}, {"start": 1786, "end": 1809, "text": "Sch\u00f6lkopf et al. (2001)", "ref_id": "BIBREF16"}, {"start": 1830, "end": 1851, "text": "Andrews et al. (2016)", "ref_id": "BIBREF0"}, {"start": 1875, "end": 1893, "text": "Song et al. (2017)", "ref_id": "BIBREF17"}, {"start": 1918, "end": 1935, "text": "Kim et al. (2015)", "ref_id": "BIBREF5"}, {"start": 2128, "end": 2152, "text": "Chalapathy et al. (2018)", "ref_id": "BIBREF1"}, {"start": 2580, "end": 2601, "text": "Grassa et al. (2019b)", "ref_id": "BIBREF8"}, {"start": 2609, "end": 2630, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "One-class novelty detection refers to the recognition of abnormal patterns on data recognized as normal. Abnormal data, also know as outliers are patters who belong to the different classes than a normal class. The goal of the novelty detection field is to distinguish anomaly patterns that are different from normal and classify them. The capability of many machine learning technique, in the field of one-class classification is to decide whether a new instance belongs to the same class of data or if it has different behavior such as to be considered as an outlier. Designing an OCC classifier is not a simple task due to the nature of data available and from the quality of data sampled. In Tab. 1 we show the general capabilities of one-class classifiers.", "cite_spans": [], "ref_spans": [], "section": "One-class Classifiers"}, {"text": "In Tab. 1 the data has multi-modality and exists more than one distribution in it Hovelynck and Chidlovskii (2010) . The general approach of multi-density is not a simple task. An object might have many levels of significance in the proximity of decision boundary showing that a single density level in some cases can be reductive in terms of classification. The noise in the data can lower the general performance of classifiers lead it to misclassification. Arbitrarily shaped distribution of the target class is a common feature of the MST, SVDD or MoG models, that are able to cover the data according to their methodology.", "cite_spans": [{"start": 82, "end": 114, "text": "Hovelynck and Chidlovskii (2010)", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "One-class Classifiers"}, {"text": "In Liu et al. (2016b) , the authors propose an interesting approach based on density modular ensemble classifiers for the one-class classification task. They applied a pruning method to split the original structure of an MST, built from the target class while creating different clusters. In this work, no cluster algorithms are applied. They used a Gaussian estimator algorithm to build a decision border for each cluster created in the previous step. Parameters considered are the level of the pruning step and all parameters behind the one-class chosen for each partition built. The authors describe the small density or isolate nodes as noises and remove them in the pruning step including only all samples of the dense region extracted by local dense subset method. The local dense subset can capture dense small regions of the target sample excluding all data far away from it. In Liu et al. (2016a) , authors use the k-means clustering algorithm in the given target class. Then, a minimum spanning tree is build considering all the clusters centroids computed before. Furthermore, they use the second partitioning step to smooth the boundary because gaps through the clusters exist. Finally, a Gaussian estimator or an MST Class Descriptor (MST CD) is applied to build a decision boundary for each cluster. Both works show us optimal results in terms of accuracy and overcome the state-of-the-arts in many datasets from the UCI repository. However, the following issues exist: -a bad choice of several clusters can be a problem that can lead to misclassification; -isolates nodes or medium-large density of nodes are avoided because considered as outliers. This solution could cause problems in the decision boundary since in the case of non-uniformly sampled data the density of the region can be varied by noisy samples removal.", "cite_spans": [{"start": 3, "end": 21, "text": "Liu et al. (2016b)", "ref_id": "BIBREF11"}, {"start": 887, "end": 905, "text": "Liu et al. (2016a)", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "One-class Classifiers"}, {"text": "In Tax and Duin (2004) , using SVDD (Support Vector Data Description), the authors describe some models related to OCSVM, where a hypersphere is considered to separate the data instead to use a hyperplane. The goal is to find the smallest hypersphere with center c \u2208 F k and radius R > 0 that encloses as much as possible data in feature space F k Arbitrarily shaped distributions are showed in our previously works (La Grassa et al. (2019b) , La Grassa et al. (2019a) ). Here the main idea is to cut down the computation of the minimum spanning tree and to avoid data noise using only a small partition of the target class as a class descriptor. Therefore, the method is iterated for each test to classify. The decision boundary built is different for each run, because we construct an MST starting from the neighbors of each test sample to be classified. However, the approach offers good results, the decision border remains the same in all the neighbors, also in regions with a small density. This problem can lead us to expand the boundary too much in all regions due to the low density of non-uniformly sampled data. Starting from La Grassa et al. (2019a) , we elaborated our model with a dynamic approach to choose the boundary based on the density of the nearest region to the test instance. In the following section, we discuss the main approach using an MST as a class descriptor and then introduce our proposed model.", "cite_spans": [{"start": 3, "end": 22, "text": "Tax and Duin (2004)", "ref_id": "BIBREF19"}, {"start": 420, "end": 441, "text": "Grassa et al. (2019b)", "ref_id": "BIBREF8"}, {"start": 447, "end": 468, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}, {"start": 1140, "end": 1161, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "One-class Classifiers"}, {"text": "Many task predictions have obtained successful results using specific structure in Graph Theory as spanning trees and have 0.828 0 found many application contexts in social network (Wang et al. (2019b) ), biology (Isaza et al. (2018) ), companies and political events (Memon et al. (2020) ). In this section, we focus on the minimum spanning tree used as a class descriptor and highlight the pro and cons of this model. This brief overview is useful to understand our approach and a possible solution to increase the performance of this classifier. In general, MST CD model uses the following two alternatives to measure the distance of a point x from the graph:", "cite_spans": [{"start": 181, "end": 201, "text": "(Wang et al. (2019b)", "ref_id": "BIBREF21"}, {"start": 213, "end": 233, "text": "(Isaza et al. (2018)", "ref_id": "BIBREF3"}, {"start": 268, "end": 288, "text": "(Memon et al. (2020)", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "The main approach using a Minimum Spanning Tree"}, {"text": "If p e i, j (x) lies on the edge e i, j = (x i ,x j ), we compute p e i, j (x) and the Euclidean distance between x and p e i, j (x), more formally:", "cite_spans": [], "ref_spans": [], "section": "The main approach using a Minimum Spanning Tree"}, {"text": "Otherwise, we compute the Euclidean distance of x and the two points x i and x j . More precisely:", "cite_spans": [], "ref_spans": [], "section": "The main approach using a Minimum Spanning Tree"}, {"text": "Then, a new instance x is recognized from a MST if it lies within the boundary, otherwise, the object is considered an outlier. In Juszczak et al. (2009) , La Grassa et al. (2019a) , La Grassa et al. (2019b) the decision whether an object is recognized by classifier or not is based on a threshold \u03b8 applied to the shape created in this phase. More formally:", "cite_spans": [{"start": 131, "end": 153, "text": "Juszczak et al. (2009)", "ref_id": "BIBREF4"}, {"start": 159, "end": 180, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}, {"start": 186, "end": 207, "text": "Grassa et al. (2019b)", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "The main approach using a Minimum Spanning Tree"}, {"text": "The threshold \u03b8 is a parameter used to assign the boundary dimension of a spanning tree. Then, given\u00ea = (||e 1 ||, ||e 2 ||, ..., ||e n ||) as an ordered set of edges weights, we define \u03b8 as For instance, with \u03b1 = 0.5, we assign to \u03b8 the median value of all edges weights of the spanning tree. The curse of the number of instances is a possible weakness that is reflected in an increase in terms of computational time, it also does not handle non-uniformly sampled data. This lack results in a too-large decision border in the whole area with few data.", "cite_spans": [], "ref_spans": [], "section": "The main approach using a Minimum Spanning Tree"}, {"text": "Starting with our previous works La Grassa et al. (2019a) and La Grassa et al. (2019b) we use the same baseline called MST CD gp. This last approach is different from Juszczak et al. (2009) because it uses a couple of small MST built for each test instance to make a prediction and it handles the case in which both classifiers has right or wrong. To remain in the context of a one-class classifier, we use only a target class of size \u03b3 to make predictions. In previous papers, we catch the structure of the target class using MST and built a decision boundary \u03b8 using Eq. 6. However, authors do not consider the problem of non-uniformly sampled data. In specifics regions of the euclidean space, it is possible to found the sparse zone and consequently to obtain a good boundary from the target class (see red line in Fig.4) can become a problem. To remedy this problem we need to build a general methodology to create a dynamic decision boundary and avoid the wrong threshold due to sparse data. For each test instance, we search the first nearest \u03b3 nodes and use the Kruskal algorithm to build an MST CD gp, then we simply apply a Breadth-first search starting to the nearest object to test instance. In this way we preserve the structure around a target class created by an MST of size \u03b3 and takes N nodes by BFS search, more formally, given\u00ea = (||e 1 ||, ||e 2 ||, ..., ||e n ||) be a set of weighted edges extracted by a BFS search algorithm applied on the MST CD gp, we introduce a dynamics threshold\u03b8 as:\u03b8 = ||BFS e[\u03b1n] || * 1 1 + e K(\u03c3\u2212\u03c3 rg * \u03b2)", "cite_spans": [{"start": 36, "end": 57, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}, {"start": 65, "end": 86, "text": "Grassa et al. (2019b)", "ref_id": "BIBREF8"}, {"start": 167, "end": 189, "text": "Juszczak et al. (2009)", "ref_id": "BIBREF4"}], "ref_spans": [{"start": 819, "end": 825, "text": "Fig.4)", "ref_id": "FIGREF3"}], "section": "Our approach"}, {"text": "where \u03b1 = 0.5 in order to take the median of all the edges weights,\u03c3 is the normalized standard deviation of all nodes selected by BFS with variance computed with his data instances distribution (see blue line in Fig.4) and K is the logistic growth rate or steepness of the curve (we used K = 5).\u03c3 rg represents the median value of all standard deviation computed taking N random group having the size returned by the BFS with depth d. This last is an important value that defines the inflection point of our inverse sigmoid. Furthermore, we use a variable \u03b2 useful to shift the inflection point and thus we slow the fall to 0 (see Fig. 2 ). Note that our mean value of standard deviation will be the centroid computed considering all target class instances. From Popoviciu's inequality (Popoviciu (1935) ), it is well-known that the upper bound of the variance of any random variable X on a range [x min , x max ] is equal to:", "cite_spans": [{"start": 787, "end": 804, "text": "(Popoviciu (1935)", "ref_id": "BIBREF14"}], "ref_spans": [{"start": 213, "end": 219, "text": "Fig.4)", "ref_id": "FIGREF3"}, {"start": 632, "end": 638, "text": "Fig. 2", "ref_id": "FIGREF1"}], "section": "Our approach"}, {"text": "Since we handle instances x i with dimension d > 1, the deviation standard is:", "cite_spans": [], "ref_spans": [], "section": "Our approach"}, {"text": "Keep in mind the relationship in Eq. 8, the upper bound of \u03c3 will be:", "cite_spans": [], "ref_spans": [], "section": "Our approach"}, {"text": "Therefore in Eq. 7 we normalize \u03c3 a\u015d", "cite_spans": [], "ref_spans": [], "section": "Our approach"}, {"text": "where x max and x min are the maximum/minimum range of our data. Our approach leverages the distribution around the neighbors of test samples and uses metrics that are well-known in the literature. So, the main effect will be to deflate the decision boundary with high power in a non-uniformly sampled area of data and with low power in a uniformly sampled zone. We remember that we normalize \u03c3 in Eq. 11 using the upper bound from Popoviciu's inequality. This simple approach allows us to shrink the decision boundary in the sparse data zone and to decrease with less power his area near the data with a low standard deviation. Our approach can be summarized as below:", "cite_spans": [], "ref_spans": [], "section": "Our approach"}, {"text": "1. Build a small MST using the first \u03b3 instances nearest to the test sample. 2. Apply BFS with parameter depth d on the small MST constructed before and return \u03c3 and the median of the weighted edges obtained from BFS search. 3. Take the median of N random nodes of small MST of size d. This value will be the inflection point of our inverse sigmoid function (all standard deviation are normalized 11). 4. Using threshold extracted by step 2 shifted by \u03b2 value and build a dynamic decision boundary for each instance, then apply the classical MST CD gp model described in La Grassa et al. (2019a) .", "cite_spans": [{"start": 574, "end": 595, "text": "Grassa et al. (2019a)", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Our approach"}, {"text": "1: G 0 =Target class 2: x=All test instances 3: for v \u2208 G 0 do 4:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 OCdmst"}, {"text": "all euclidean distances \u2190 ||x \u2212 v|| 5: end for 6: Create a small mst from \u03b3 neighbors from test x 7: NodeX = Take min(all euclidean distances) and return node v 8: EdgesNodeX \u2190 Search inc/out edge nodeX and return (x i , x j ) 9: for x i , x j \u2208 EdgesNodeX do 10:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 OCdmst"}, {"text": "<= 1 then 11: Fig. 5 : In (a) the BFS path results starting from the node closest to the X test using depth = 2 and \u03b3 = 9. In (b), the structure created without the use of BFS. The difference is that in (a) we capture the neighbor's relations in terms of connectivity instead in (b) we use the Euclidean distance.", "cite_spans": [], "ref_spans": [{"start": 14, "end": 20, "text": "Fig. 5", "ref_id": null}], "section": "Algorithm 1 OCdmst"}, {"text": "To prove the effectiveness and robustness of our OCdmst, we use the same experimental setting used in Wang et al. (2019a) and compute Matthews's Coefficient Correlation (MCC) and in some experiments we reported total accuracy as performance metrics. In the binary classification problem, a model can obtain high accuracy on outliers class but very low performance to recognize target class or vice versa. In such a situation, using a global metric as the classical accuracy is not the right way to compute the classification score. To avoid this issue we use MCC as a metric that can be used even if we have classes with different sizes. MCC returns a value between -1 and +1 where a coefficient of +1 represents a perfect prediction, 0 random predictions and -1 indicates total disagreement between prediction and observation.", "cite_spans": [{"start": 102, "end": 121, "text": "Wang et al. (2019a)", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Experiments"}, {"text": "All our experiments use five-fold cross-validation to split target class. To compose the complete test set we use the fold used as a test, together with all the outliers samples. Finally, we repeat the approach twenty times taking means and variance to compare our results with other models. In our experiments we use six numerical datasets from UCI repository (see. Table. 5) and a novel dataset of Covid-19 that contains chest X-ray or CT images of positive/negative patients (see Fig. 1 ). In order to have the right comparison criterion in Covid-19 dataset, we extract confusion matrix as reported in Tab. 4 and compute the precision metric described in Eqs. 13 and 14. These last measures are needed to compare the class accuracy of the proposed OCdmst and the Resnet18 used in our experiments.", "cite_spans": [], "ref_spans": [{"start": 367, "end": 373, "text": "Table.", "ref_id": null}, {"start": 483, "end": 489, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Experiments"}, {"text": "Like for OCSVM and many others models, we need to do fine-tuning to find good parameters and obtain good results. Considering the lack of MST due to his high computational time, it is not easy to find optimal parameters. Therefore, we use BFS with depth d \u2208 {1, ..., 7} for all UCI dataset (see details in Tab. 5), d \u2208 {2, ..., 11} only for Covid-19 dataset, a growth rate k \u2208 {5, 6, 7, 8, 9, 10, 20} and a variable \u03b2 \u2208 {1.05, 1.1, 1.5} to shift the logistic function (see Fig. 2 ). Furthermore, in finetuning parameters, we set the size of small MST from the length target class 4 to length target class 2 .", "cite_spans": [], "ref_spans": [{"start": 473, "end": 479, "text": "Fig. 2", "ref_id": "FIGREF1"}], "section": "Experiments"}, {"text": "In this section, we compare our OCdmst with others oneclass classifiers more recent in the literature showing the results in Tab. 3. For clarity, in Liver dataset we use the last field for classification. Also if it does not represent the presence/absence of a liver disorder, we have to use it to obtain the comparison with the other papers. In the Heart dataset (Cleveland) we distinguish values 1,2,3,4 as the presence of disease, 0 otherwise and removed instances with missing values in according to the experiments in the literature. Results demonstrate the rightness of our model and as reported in Tab. 3 we outperform the stateof-the-art in terms of global mean for many classifiers (except FS-EOCC) and for many datasets.", "cite_spans": [], "ref_spans": [], "section": "First experiment"}, {"text": "In the second experiment, we analyze our model with MCC accuracies in y-axis and the depth from 1 to 7 for each size of small MST (x-axis) (see Fig. 6 , 3). Depth of BFS and size of MST are useful parameters to set up our model making as less as possible wrong classifications.", "cite_spans": [], "ref_spans": [{"start": 144, "end": 150, "text": "Fig. 6", "ref_id": "FIGREF4"}], "section": "Second experiment"}, {"text": "In the third experiment, we demonstrate by experimental results the effectiveness of our model with few data and compare it with a neural network (Resnet18). We simulate the scenario in which we have too few data to deny common neural networks to make right discrimination of different concepts (two classes in our case). Therefore we apply a 2-Fold cross-validation considering all data available from Covid-19 dataset and we extract the deep features for each model trained (Resnet18). All images have been re-scaled to 256x256 pixels and transformed to gray-scale. Learning rate and batch-size are set to 0.001 and 10 respectively. As evaluation metric we do not consider the total accuracy but we evaluate the accuracy for each class (Precision). Then, we use the deep features as input to our OCdmst using the dataset split in the previous step. The results reported in Tab. 2 show the flexibility of our model to operate also with Resnet18 learned features using cross-entropy loss. As showed in Tab. 2, Resnet18 does not offer right prediction to recognize instances from negative class. This results is reasonable considering we have too few data with negative labels. Even when we used features extracted by Resnet18 to our model, but OCdmst is able to recognize value from both classes and overcome the right prediction also on positive class. The confusion matrix showed in Fig. 7 and accuracies for each class demonstrate the capability of OCdmst to operate also in a scenario with few data, as well as overcome deep neural networks. This last result is plausible because it represents one of the weaknesses of deep models.", "cite_spans": [], "ref_spans": [{"start": 1385, "end": 1391, "text": "Fig. 7", "ref_id": null}], "section": "Third experiment"}, {"text": "In machine learning, non-uniformly sampled data is a problem that normally leads the predictive models to make misclassifications. To contrast this issue, in this paper, we propose a dynamic boundary approach for one-class classification able to handle their own boundary also in a non-uniformly sampled data. The effectiveness and solidity of our approach have been compared with many one-class classifiers and for many of them, we have passed the state of the art. We believe that our model can be improved in terms of computational time, in order to be applied also in computer vision.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Detecting anomalous data using auto-encoders", "authors": [{"first": "J", "middle": ["T"], "last": "Andrews", "suffix": ""}, {"first": "E", "middle": ["J"], "last": "Morton", "suffix": ""}, {"first": "L", "middle": ["D"], "last": "Griffin", "suffix": ""}], "year": 2016, "venue": "International Journal of Machine Learning and Computing", "volume": "6", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Anomaly detection using oneclass neural networks", "authors": [{"first": "R", "middle": [], "last": "Chalapathy", "suffix": ""}, {"first": "A", "middle": ["K"], "last": "Menon", "suffix": ""}, {"first": "S", "middle": [], "last": "Chawla", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1802.06360"]}}, "BIBREF2": {"ref_id": "b2", "title": "Multi-modality in one-class classification", "authors": [{"first": "M", "middle": [], "last": "Hovelynck", "suffix": ""}, {"first": "B", "middle": [], "last": "Chidlovskii", "suffix": ""}], "year": 2010, "venue": "", "volume": "", "issn": "", "pages": "441--450", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Biological signaling pathways and potential mathematical network representations: biological discovery through optimization", "authors": [{"first": "C", "middle": [], "last": "Isaza", "suffix": ""}, {"first": "J", "middle": ["F"], "last": "Rosas", "suffix": ""}, {"first": "E", "middle": [], "last": "Lorenzo", "suffix": ""}, {"first": "A", "middle": [], "last": "Marrero", "suffix": ""}, {"first": "C", "middle": [], "last": "Ortiz", "suffix": ""}, {"first": "M", "middle": ["R"], "last": "Ortiz", "suffix": ""}, {"first": "L", "middle": [], "last": "Perez", "suffix": ""}, {"first": "M", "middle": [], "last": "Cabrera-R\u00edos", "suffix": ""}], "year": 2018, "venue": "Cancer medicine", "volume": "7", "issn": "", "pages": "1875--1895", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Minimum spanning tree based one-class classifier", "authors": [{"first": "P", "middle": [], "last": "Juszczak", "suffix": ""}, {"first": "D", "middle": ["M"], "last": "Tax", "suffix": ""}, {"first": "E", "middle": [], "last": "Pe", "suffix": ""}, {"first": "R", "middle": ["P"], "last": "Duin", "suffix": ""}], "year": 2009, "venue": "Neurocomputing", "volume": "72", "issn": "", "pages": "1859--1869", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Deep learning with support vector data description", "authors": [{"first": "S", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Y", "middle": [], "last": "Choi", "suffix": ""}, {"first": "M", "middle": [], "last": "Lee", "suffix": ""}], "year": 2015, "venue": "Neurocomputing", "volume": "165", "issn": "", "pages": "111--117", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Dynamic ensemble selection for multi-class classification with one-class classifiers", "authors": [{"first": "B", "middle": [], "last": "Krawczyk", "suffix": ""}, {"first": "M", "middle": [], "last": "Galar", "suffix": ""}, {"first": "M", "middle": [], "last": "Wo\u017aniak", "suffix": ""}, {"first": "H", "middle": [], "last": "Bustince", "suffix": ""}, {"first": "F", "middle": [], "last": "Herrera", "suffix": ""}], "year": 2018, "venue": "Pattern Recognition", "volume": "83", "issn": "", "pages": "34--51", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Binary classification using pairs of minimum spanning trees or n-ary trees", "authors": [{"first": "La", "middle": [], "last": "Grassa", "suffix": ""}, {"first": "R", "middle": [], "last": "Gallo", "suffix": ""}, {"first": "I", "middle": [], "last": "Calefati", "suffix": ""}, {"first": "A", "middle": [], "last": "Ognibene", "suffix": ""}, {"first": "D", "middle": [], "last": "", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "365--376", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "A classification methodology based on subspace graphs learning", "authors": [{"first": "La", "middle": [], "last": "Grassa", "suffix": ""}, {"first": "R", "middle": [], "last": "Gallo", "suffix": ""}, {"first": "I", "middle": [], "last": "Calefati", "suffix": ""}, {"first": "A", "middle": [], "last": "Ognibene", "suffix": ""}, {"first": "D", "middle": [], "last": "", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "1--8", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Ocmst: One-class novelty detection using convolutional neural network and minimum spanning trees", "authors": [{"first": "La", "middle": [], "last": "Grassa", "suffix": ""}, {"first": "R", "middle": [], "last": "Gallo", "suffix": ""}, {"first": "I", "middle": [], "last": "Landro", "suffix": ""}, {"first": "N", "middle": [], "last": "", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Fast structural ensemble for one-class classification", "authors": [{"first": "J", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Q", "middle": [], "last": "Miao", "suffix": ""}, {"first": "Y", "middle": [], "last": "Sun", "suffix": ""}, {"first": "J", "middle": [], "last": "Song", "suffix": ""}, {"first": "Y", "middle": [], "last": "Quan", "suffix": ""}], "year": 2016, "venue": "Pattern Recognition Letters", "volume": "80", "issn": "", "pages": "179--187", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Modular ensembles for one-class classification based on density analysis", "authors": [{"first": "J", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Q", "middle": [], "last": "Miao", "suffix": ""}, {"first": "Y", "middle": [], "last": "Sun", "suffix": ""}, {"first": "J", "middle": [], "last": "Song", "suffix": ""}, {"first": "Y", "middle": [], "last": "Quan", "suffix": ""}], "year": 2016, "venue": "Neurocomputing", "volume": "171", "issn": "", "pages": "262--276", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "General election effect on the network topology of pakistans stock market: network-based study of a political event", "authors": [{"first": "B", "middle": ["A"], "last": "Memon", "suffix": ""}, {"first": "H", "middle": [], "last": "Yao", "suffix": ""}, {"first": "R", "middle": [], "last": "Tahir", "suffix": ""}], "year": 2020, "venue": "Financial Innovation", "volume": "6", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Learning deep features for one-class classification", "authors": [{"first": "P", "middle": [], "last": "Perera", "suffix": ""}, {"first": "V", "middle": ["M"], "last": "Patel", "suffix": ""}], "year": 2019, "venue": "IEEE Transactions on Image Processing", "volume": "28", "issn": "", "pages": "5450--5463", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Sur les\u00e9quations alg\u00e9briques ayant toutes leurs racines r\u00e9elles", "authors": [{"first": "T", "middle": [], "last": "Popoviciu", "suffix": ""}], "year": 1935, "venue": "Mathematica", "volume": "9", "issn": "", "pages": "129--145", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Deep one-class classification", "authors": [{"first": "L", "middle": [], "last": "Ruff", "suffix": ""}, {"first": "R", "middle": [], "last": "Vandermeulen", "suffix": ""}, {"first": "N", "middle": [], "last": "Goernitz", "suffix": ""}, {"first": "L", "middle": [], "last": "Deecke", "suffix": ""}, {"first": "S", "middle": ["A"], "last": "Siddiqui", "suffix": ""}, {"first": "A", "middle": [], "last": "Binder", "suffix": ""}, {"first": "E", "middle": [], "last": "M\u00fcller", "suffix": ""}, {"first": "M", "middle": [], "last": "Kloft", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "4393--4402", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Estimating the support of a high-dimensional distribution", "authors": [{"first": "B", "middle": [], "last": "Sch\u00f6lkopf", "suffix": ""}, {"first": "J", "middle": ["C"], "last": "Platt", "suffix": ""}, {"first": "J", "middle": [], "last": "Shawe-Taylor", "suffix": ""}, {"first": "A", "middle": ["J"], "last": "Smola", "suffix": ""}, {"first": "R", "middle": ["C"], "last": "Williamson", "suffix": ""}], "year": 2001, "venue": "Neural computation", "volume": "13", "issn": "", "pages": "1443--1471", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "A hybrid semi-supervised anomaly detection model for high-dimensional data", "authors": [{"first": "H", "middle": [], "last": "Song", "suffix": ""}, {"first": "Z", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "A", "middle": [], "last": "Men", "suffix": ""}, {"first": "B", "middle": [], "last": "Yang", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Using two-class classifiers for multiclass classification 2", "authors": [{"first": "D", "middle": ["M"], "last": "Tax", "suffix": ""}, {"first": "R", "middle": ["P"], "last": "Duin", "suffix": ""}], "year": 2002, "venue": "", "volume": "", "issn": "", "pages": "124--127", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Support vector data description", "authors": [{"first": "D", "middle": ["M"], "last": "Tax", "suffix": ""}, {"first": "R", "middle": ["P"], "last": "Duin", "suffix": ""}], "year": 2004, "venue": "Machine learning", "volume": "54", "issn": "", "pages": "45--66", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Dynamic hypersphere svdd without describing boundary for one-class classification", "authors": [{"first": "J", "middle": [], "last": "Wang", "suffix": ""}, {"first": "W", "middle": [], "last": "Liu", "suffix": ""}, {"first": "K", "middle": [], "last": "Qiu", "suffix": ""}, {"first": "H", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "L", "middle": [], "last": "Zhao", "suffix": ""}], "year": 2019, "venue": "Neural Computing and Applications", "volume": "31", "issn": "", "pages": "3295--3305", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Hierarchical community detection in social networks based on micro-community and minimum spanning tree", "authors": [{"first": "Z", "middle": [], "last": "Wang", "suffix": ""}, {"first": "M", "middle": [], "last": "Hou", "suffix": ""}, {"first": "G", "middle": [], "last": "Yuan", "suffix": ""}, {"first": "J", "middle": [], "last": "He", "suffix": ""}, {"first": "J", "middle": [], "last": "Cui", "suffix": ""}, {"first": "M", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2019, "venue": "IEICE TRANSACTIONS on Information and Systems", "volume": "102", "issn": "", "pages": "1773--1783", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "One-class kernel subspace ensemble for medical image classification", "authors": [{"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "B", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "F", "middle": [], "last": "Coenen", "suffix": ""}, {"first": "J", "middle": [], "last": "Xiao", "suffix": ""}, {"first": "W", "middle": [], "last": "Lu", "suffix": ""}], "year": 2014, "venue": "EURASIP Journal on Advances in Signal Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Four images extracted from Covid-19 dataset. Figs (a) and (b) represents negative cases with pneumonia and SARS respectively, Figs (c) and (d) are positive cases of Covid-19. Due to different variety of images we convert all images to gray scale and re-scaled to 256 \u00d7 256 pixels.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Logistic function with grow rate k \u2208{6, 7, 8, 9, 10, 20}. For all experiments we chose \u03b2 = 1.5 to move the inflection point of the curve with respect to the median value of all the standard deviations starting from N random samples in small MST built.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Analysis of OCdmst using different size of \u03b3 on different levels of depth d on Covid-19 dataset. In most of the datasets used we obtain better results using BFS with low depth. In this case the target class used is the positive label.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Overview of OCdmst algorithm. This figure emphasizes the dynamic border relationship with the nearest instances.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Analysis using different parameters on UCI datasets. In x-axis the size of small mst and y-axis the depth used in BFS search. Red line and blue line represent the related max and standard deviation of MCC accuracies.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Capabilities of state-of-the-art OCC algorithms on target class features. Our model uses a dynamic boundary for the target class shape, differing from other models that make use of a static boundary around the class.", "latex": null, "type": "table"}, "TABREF1": {"text": "Performance comparison of OCdmst and Resnet18 on Covid-19 dataset. We highlight the capability of OCdmst to make prediction even when the neural network does not recognize values from negative class. Target class used is Positive label.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>K-Fold </td><td>Positive class </td><td>Negative class\n</td></tr><tr><td>OCdmst\n</td></tr><tr><td>1 </td><td>0.853 </td><td>0.347\n</td></tr><tr><td>2 </td><td>0.843 </td><td>0.187\n</td></tr><tr><td>\u00a0</td><td>Resnet18\n</td></tr><tr><td>1 </td><td>0.781 </td><td>0\n</td></tr><tr><td>2 </td><td>0.828 </td><td>0\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Performance comparison of OCdmst and state-of-the-art OCC methodology on UCI datasets using MCC metric. In bold our average result.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Dataset </td><td>FS-EOCC (Gauss) </td><td>FS-EOCC (MST CD) </td><td>TOCC </td><td>OCClustE (Gauss) </td><td>OCClustE (MST CD)\n</td><td>Gauss </td><td>MST CD </td><td>OC-SVM </td><td>OCdmst\n</td></tr><tr><td>Breast (Benign) </td><td>0.765 (0.006) </td><td>0.864 (0.003) </td><td>0.847 (0.004) </td><td>0.899 (0.001) </td><td>0.905 (0.003) </td><td>0.895 (0.0) </td><td>0.894 (0.002) </td><td>0.473 (0.002) </td><td>0.774\n</td></tr><tr><td>Breast (Malignant) </td><td>0.471 (0.022) </td><td>0.218 (0.009) </td><td>0.215 (0.012) </td><td>0.208 (0.027) </td><td>0.313 (0.009) </td><td>0.19 (0.007) </td><td>-0.015 (0.011) </td><td>0.234 (0.005) </td><td>0.204\n</td></tr><tr><td>Diabetes (Absent) </td><td>0.096 (0.008) </td><td>0.005 (0.002) </td><td>0.05 (0.003) </td><td>-0.01 (0.005) </td><td>0.051 (0.004) </td><td>0.009 (0.005) </td><td>-0.015 (0.004) </td><td>0.010 (0.007) </td><td>0.066\n</td></tr><tr><td>Diabetes (Present) </td><td>0.186 (0.004) </td><td>0.233 (0.008) </td><td>0.209 (0.003) </td><td>0.228 (0.004) </td><td>0.21 (0.012) </td><td>0.161 (0.016) </td><td>0.206 (0.003) </td><td>0.203 (0.016) </td><td>0.178\n</td></tr><tr><td>Glass (Float) </td><td>0.505 (0.017) </td><td>0.567 (0.016) </td><td>0.471 (0.012) </td><td>0.452 (0.006) </td><td>0.507 (0.005) </td><td>0.43 (0.011) </td><td>0.422 (0.005) </td><td>0.389 (0.005) </td><td>0.535\n</td></tr><tr><td>Glass (NoFloat) </td><td>0.375 (0.022) </td><td>0.32 (0.003) </td><td>0.314 (0.015) </td><td>0.339 (0.024) </td><td>0.336 (0.002) </td><td>0.251 (0.015) </td><td>0.267 (0.017) </td><td>-0.149 (0.035) </td><td>0.238\n</td></tr><tr><td>Heart (Present) </td><td>0.062 (0.029) </td><td>0.095 (0.026) </td><td>0.004 (0.017) </td><td>0.015 (0.010) </td><td>0.03 (0.011) </td><td>0.01 (0.021) </td><td>0.071 (0.005) </td><td>0.069 (0.005) </td><td>0.037\n</td></tr><tr><td>Heart (Absent) </td><td>0.082 (0.037) </td><td>0.231 (0.006) </td><td>0.115 (0.011) </td><td>0.095 (0.024) </td><td>0.249 (0.004) </td><td>0.328 (0.008) </td><td>0.161 (0.025) </td><td>-0.005 (0.020) </td><td>0.117\n</td></tr><tr><td>Liver (Disorder) </td><td>0.151 (0.001) </td><td>0.016 (0.03) </td><td>0.01 (0.006) </td><td>-0.012 (0.01) </td><td>0.124 (0.006) </td><td>-0.084 (0.009) </td><td>-0.06 (0.022) </td><td>0.056 (0.005) </td><td>0.099\n</td></tr><tr><td>Liver (Healthy) </td><td>0.94 (0.006) </td><td>0.033 (0.022) </td><td>0.053 (0.004) </td><td>0.07 (0.014) </td><td>0.112 (0.01) </td><td>0.062 (0.018) </td><td>0.047 (0.01) </td><td>0.065 (0.002) </td><td>0.073\n</td></tr><tr><td>Sonar (Mines) </td><td>0.42 (0.011) </td><td>0.563 (0.006) </td><td>0.511 (0.009) </td><td>0.396 (0.039) </td><td>0.351 (0.008) </td><td>0.356 (0.031) </td><td>0.255 (0.008) </td><td>0.133 (0.019) </td><td>0.672\n</td></tr><tr><td>Sonar (Rocks) </td><td>0.215 (0.005) </td><td>0.147 (0.02) </td><td>0.045 (0.015) </td><td>0.11 (0.009) </td><td>0.108 (0.014) </td><td>0.151 (0.027) </td><td>0.166 (0.032) </td><td>0.054 (0.023) </td><td>0.336\n</td></tr><tr><td>Average </td><td>0.285 </td><td>0.274 </td><td>0.237 </td><td>0.233 </td><td>0.275 </td><td>0.23 </td><td>0.2 </td><td>0.128 </td><td>0.277\n</td></tr></table></body></html>"}, "TABREF4": {"text": "Confusion", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>True labels\n</td></tr><tr><td>\u00a0</td><td>Positive </td><td>Negative\n</td></tr><tr><td>Predicted Positive Negative </td><td>TP FN </td><td>FP \u00d6 PPVTN \u00d6 NPV\n</td></tr></table></body></html>"}, "TABREF5": {"text": "Number of features, instances for all the datasets used in our experiments.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Datasets </td><td>Features </td><td>Instances\n</td></tr><tr><td>Sonar </td><td>60 </td><td>208\n</td></tr><tr><td>Liver </td><td>6 </td><td>345\n</td></tr><tr><td>Breast cancer (Wisconsin) </td><td>9 </td><td>699\n</td></tr><tr><td>Diabetes </td><td>8 </td><td>768\n</td></tr><tr><td>Glass </td><td>9 </td><td>214\n</td></tr><tr><td>Heart </td><td>13 </td><td>297\n</td></tr></table></body></html>"}, "TABREF6": {"text": "Performance comparison using classical MST CD and MST CD gp with our OCdmst. Results obtained show a clear advantages using a dynamic decision boundary. In bold our average result.Fig. 7: Confusion matrix of OCdmst using deep features extracted by Resnet18. We use k-fold cross-validation using k = 1 (image on the left) and k = 2 (image on the right). Numerical results are showed in Tab. 2.", "latex": null, "type": "table"}}, "back_matter": []}