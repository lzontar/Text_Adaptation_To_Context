{
    "paper_id": "0d55a450f1fc696a73ac5d3477617c1c9b9a259f",
    "metadata": {
        "title": "Synthetic Interventions",
        "authors": [
            {
                "first": "Anish",
                "middle": [],
                "last": "Agarwal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": "anish90@mit.edu"
            },
            {
                "first": "Abdullah",
                "middle": [],
                "last": "Alomar",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": "aalomar@mit.edu"
            },
            {
                "first": "Romain",
                "middle": [],
                "last": "Cosson",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": "cosson@mit.edu"
            },
            {
                "first": "Devavrat",
                "middle": [],
                "last": "Shah",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": "devavrat@mit.edu"
            },
            {
                "first": "Dennis",
                "middle": [],
                "last": "Shen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": "deshen@mit.edu"
            },
            {
                "first": "C",
                "middle": [
                    "A"
                ],
                "last": "Agarwal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "A",
                "middle": [],
                "last": "Alomar",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "R",
                "middle": [],
                "last": "Cosson",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "D",
                "middle": [],
                "last": "Shah",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "&amp;",
                "middle": [
                    "D"
                ],
                "last": "Shen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Alomar",
                "middle": [],
                "last": "Agarwal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Shah",
                "middle": [],
                "last": "Cosson",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "",
                "middle": [],
                "last": "Shen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Massachusetts Institute of Technology",
                    "location": {
                        "addrLine": "77 Massachusetts Avenue",
                        "postCode": "02139",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We develop a method to help quantify the impact that different levels of mobility restrictions have had on COVID-19 related deaths across various countries. Synthetic control (SC), regarded as the \"most important innovation in the policy evaluation in the last 15 years\" (8), has emerged as a standard tool to produce counterfactual estimates if a particular intervention had not occurred, using just observational data. However, extending SC to obtain counterfactual estimates if a particular intervention had occurred remains an important open problem (4) -this is precisely the question that arises when assessing the impacts of varying mobility restrictions as stated above. As the main contribution of this work, we introduce synthetic interventions (SI), which helps resolve this open problem by providing counterfactual estimates for multiple interventions of interest. We introduce a tensor factor model, a natural generalization of matrix factor models used to analyze SC, and prove that SI produces consistent counterfactual estimates under this setting. Our finite sample analyses show the test (out-of-sample) error decays as 1/T 0 , where T 0 is the amount of observed pre-intervention (training) data. As a special case of our result, this improves upon the 1/ \u221a T 0 bound on the test error for SC in prior works. We prove that our test error bound holds under a certain \"subspace inclusion\" condition, and furnish a data-driven hypothesis test, with provable guarantees, to check for this condition. Again, as a special case, this provides a quantitative hypothesis test for the validity of when to apply SC, which has been absent in the literature. As a technical contribution, we establish that both the parameter estimation and test error for Principal Component Regression (a key subroutine of SI and several SC variants) decay as 1/T 0 under the high-dimensional error-in-variable regression setting; this improves upon the best prior test error bound of 1/ \u221a T 0 . In addition to the COVID-19 case study, we show how SI can be used to perform dataefficient, personalized randomized control trials (or A/B tests) using real-data from a large e-commerce website and large developmental economics study, thereby establishing its widespread applicability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Motivation. As the COVID-19 pandemic began to rapidly spread within the United States (U.S.), the U.S. government responded by implementing policies to enforce social distancing. Unfortunately, these policies only led to a less than 5% reduction in mobility (1) , and many lives were tragically lost. This begs the questions: Would greater reductions in mobility, say 30% or 60%, have led to significantly better societal health outcomes? And moving forward, what trade-offs between health outcomes and economic impact can be achieved through different policies? Although it is infeasible to answer either question through actual experimentation, it is possible to leverage information from across the globe. Given that different regions and/or countries have implemented various policies, valuable observation data is readily available and can be used to answer these questions.",
            "cite_spans": [
                {
                    "start": 258,
                    "end": 261,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Problem Setup. We are interested in outcomes (e.g., COVID-19 death counts) associated with N \u2265 1 units (e.g., countries) across T tn denote the observed outcome for unit n at time t under intervention d. We denote the \"no-intervention\" state with d = 1 (e.g., no mobility restriction enacted).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Pre-and Post-Intervention. Let 1 \u2264 T 0 < T denote the intervention point, which partitions the time horizon into two distinct segments: (i) the \"pre-intervention\" period (e.g., pre-COVID), t \u2264 T 0 , when all units are assumed to be in the no intervention state; and (ii) the \"post-intervention\" period, t > T 0 , when each unit receives some intervention (or remains unaffected). We group the units by the intervention they receive during the post-intervention period. Let I (d) = {n : unit n experiences intervention d} denote the subgroup of units that experience intervention d, and N (d) = |I (d) |\u2265 1. (1) here, \u03c0 tn is a Bernoulli random variable with parameter \u03c1 \u2208 (0,1] to model randomly missing data. Note, this observation model includes the standard consistency assumption made in the potential outcomes literature (see (15) ) with the generalization that there may still be randomly missing observations.",
            "cite_spans": [
                {
                    "start": 831,
                    "end": 835,
                    "text": "(15)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Objective. We aim to infer potential outcomes under all interventions d for every unit n across t > T 0 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "SC (2; 3; 4) has emerged as a standard tool that uses observational data to produce counterfactual estimates if a particular intervention had not occurred. Indeed, it has been regarded as \"arguably the most important innovation in the policy evaluation literature in the last 15 years\" (8) . In our context, it provides a solution for a restricted setting: D = 2 with I (1) = [N ]\\{1}, I (2) = {1}, i.e., only unit 1 experiences intervention 2 after T 0 while all other N \u22121 units remain under the no-intervention state. The goal in SC is to infer outcomes for unit 1 under no-intervention only, i.e., Y (1) t1 for t > T 0 . In our COVID example, this corresponds to the situation where the U.S. (i.e., unit 1 in this case) implemented a mobility restriction of less than 5% while all other countries did nothing. Using such observations, we aim to infer the number of deaths in the U.S. if it had done nothing to combat COVID-19. SC (and its variants) provide an answer to this via a remarkably simple, but powerful solution. Specifically, using pre-intervention data, a \"synthetic\" version of unit 1 (i.e., the \"target\" unit) is created as a weighted combination of the remaining N \u22121 units (i.e., the \"donor\" units). The learnt model is then used to produce counterfactual predictions for the target unit under the no-intervention state during the post-intervention period. That is, SC learns \u03b2 sc \u2208 R N \u22121 as",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 289,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 604,
                    "end": 607,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic Control (SC), A Partial Solution"
        },
        {
            "text": "where the constraint set SC \u2286 R N \u22121 differs across variants of the method, but is classically taken to be a convex set (cf. (2; 3; 9; 6; 7)). Subsequently, Y (1),sc t1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Control (SC), A Partial Solution"
        },
        {
            "text": "tn is the estimate for the target unit under no-intervention for t > T 0 . Comparing Y (1) ,sc t1 with Y (2) t1 for t > T 0 allows us to evaluate the impact of intervention 2 on the target unit compared to the no-intervention effect.",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 90,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 105,
                    "end": 108,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic Control (SC), A Partial Solution"
        },
        {
            "text": "Given that the literature on SC is large, we focus on a recent line of work, (6; 7; 5) , studying the finite-sample properties of SC estimators. The authors in (6; 7; 5) argue that a remarkably simple, (approximate) linear relationship between a target and donors holds under a generalized latent factor model. This model (also considered in (3) ) states that the observations under no-intervention (d = 1), Y (1) tn , are random variables with mean g(u t ,v n ), where u t \u2208 R m 1 and v n \u2208 R m 2 are latent factors associated with time and unit, respectively, and g : R m 1 \u00d7R m 2 \u2192 R is a \"smooth\" mapping (e.g., H\u00f6lder continuous). Under this setting, they show that the finite-sample test (out-of-sample) error scales as 1/ \u221a T 0 in the large N regime. Additionally, given SC's widespread use across a plethora of domains, there are excellent heuristic hypothesis tests proposed in the literature (see (4) ) that serve as robustness checks for whether SC is valid to use; however, to the best of our knowledge, none of them come with rigorous theoretical guarantees.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 86,
                    "text": "(6; 7; 5)",
                    "ref_id": null
                },
                {
                    "start": 160,
                    "end": 169,
                    "text": "(6; 7; 5)",
                    "ref_id": null
                },
                {
                    "start": 342,
                    "end": 345,
                    "text": "(3)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 410,
                    "end": 413,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 907,
                    "end": 910,
                    "text": "(4)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic Control (SC), A Partial Solution"
        },
        {
            "text": "In summary, SC, though a powerful method, provides an incomplete answer to the COVID-19 question laid out above -it only allows one to produce counterfactual estimates if no intervention occurred, while we are interested in making counterfactual estimates if any one of multiple interventions had occurred. Further, the SC literature is missing a theoretically motivated hypothesis test and a tighter theoretical analysis with better test error rates. Indeed, extending SC to handle multiple interventions, as required in our setting, is an important open problem (see (4)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Control (SC), A Partial Solution"
        },
        {
            "text": "As the main contribution of this work, we introduce SI, a method that overcomes the limitations of SC laid out above; i.e., SI provides a counterfactual estimate for each unit under every intervention. Methodologically, SI pleasingly turns out to be straightforward extension of SC, making it easy to implement. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "\u223ci |. We represent the pre-and post-intervention observation matrices associated with the donors that receive d as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "SI produces the counterfactual estimates under intervention d for unit i as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "1. De-noise: Impute missing values in Y (1) pre and Y ",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 43,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "\u223ci \u2227(T \u2212T 0 )] are hyper-parameters. 2. Learn synthetic target model: apply linear regression on the pre-intervention data to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "3. Predict counterfactual potential outcomes: for every t > T 0 , define the estimate of the potential outcome of unit i under intervention d as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "COVID-19: Impact of Mobility Restrictions. We apply SI to estimate the counterfactual death tolls for multiple countries under three different mobility reductions: less than 5%, between 5-35%, and more than 35%. Figure 1 shows such counterfactual estimates for U.S., Brazil, and India. Pleasingly, the predicted outcomes match the actual outcomes (i.e., the death toll under the country's enacted intervention) quite closely. For details on the COVID-19 case study, please see Section 4.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 212,
                    "end": 220,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "Explaining Success of SI using a Tensor Factor Model. Though SI is methodologically similar to SC in terms of learning a model to estimate counterfactual outcomes, it is conceptually significantly different. Specifically, as in SC, the model in SI is learnt using pre-intervention data under the nointervention (d = 1) setting; however, to produce post-intervention counterfactual estimates, SI now applies the learnt model to any intervention d. A priori, it is not clear why the model can be transferred between interventions. We prove the validity of SI under a proposed tensor factor model, a natural generalization of the matrix factor model used to analyze SC. Specifically, let Y",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "tn be a random variable whose mean is g(u t ,v n ,w d ), where u t ,v n are defined as before, w d \u2208 R m 3 is the latent factor associated with intervention d, and g is again a \"smooth\" function. A canonical example of such a g is the low-rank tensor factor model given by g(u t ,v n ,w d ) = r =1 u t v n w d ; in this paper, we focus on this model. Indeed, in (6; 7; 5), the authors argue that any H\u00f6lder continuous g is well-approximated by a low-rank tensor factor model, where the approximation error vanishes as more data is collected. Under this setting, we establish that there exists an invariant linear model that persists across pre-and post-intervention periods (see Proposition 2.1). We list some of the key comparisons between SC and SI in Table 1 . In particular, we note that (2) establishes the efficacy of the SC estimator by proving it is asymptotically unbiased; their analysis assumes the existence of both a low-rank factor model and convex weights that explain the relationship between donors and units. On the other hand, this work (and (6; 5)) analyzes the finite sample error and does not require the existence of convex weights. Under the low-rank tensor factor model, we show SI produces consistent post-intervention counterfactual estimates for all units under all interventions of interest. Our finite sample analyses indicates that the post-intervention (test) prediction error scales as 1/T 0 (see Cor. 33, Appendix H), in expectation. As a special case, this improves upon the best known test error guarantee for SC of 1/ \u221a T 0 . The statement in high-probability, with explicit dependence on the noise parameters and model complexity, is given in Theorem 3.2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 754,
                    "end": 761,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "Data-driven Hypothesis Test. Our test error relies on a \"subspace inclusion\" property, which can be verified through a simple, quantitative hypothesis described in Section 3.3. Indeed, we argue that for any given significance level \u03b1 \u2208 (0,1), the test statistic is smaller than an explicit critical value \u03c4 \u03b1 with probability at least 1\u2212\u03b1 (see Theorem 3.3). As a special case, this test also serves as a robustness check for the validity of when to use some of the recent SC estimators (6; 7; 5), which has been thus far absent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "Error-in-variable and Principal Component Regression (PCR). As a key technical contribution, we prove that PCR (a key subroutine of SI; see Proposition 3.1 of (5)) consistently learns the model parameters (linear model across interventions) even in a high-dimensional error-invariable setting (i.e., sparse, noisy covariates). In a recent work, (5) shows the test prediction error, under a restricted transductive learning setting, decays as 1/ \u221a T 0 ; here, T 0 is the number of training samples. We improve upon this in multiple ways: we (i) achieve a faster test error decay rate of 1/T 0 ; (ii) analyze a classical supervised learning setting (also includes the transductive setting); (iii) establish that PCR consistently estimates the underlying linear model (not proved in (5)) with an error rate of 1/T 0 . Compared to the rich literature of high-dimensional error-in-variable regression (cf. (18; 14; 20) ), our method achieves a similar error rate (with respect to T 0 ) for model identification without explicit knowledge of the underlying covariate noise model or a sparsity assumption on the model parameter; instead, we require the covariates to be low-rank. Moreover, the literature in error-in-variable regression often assumes a restricted eigenvalue condition on the covariate matrix, while we require the non-zero singular values of the covariate matrix to be well-balanced. Finally, we highlight that the error-in-variable regression literature does not provide generalization bounds for the prediction error since the existing algorithms do not provide a method to \"de-noise\" corrupted test (out-of-sample) covariates; PCR, on the other hand, does provide a de-noising approach. A list of comparisons is summarized in Table 2 .",
            "cite_spans": [
                {
                    "start": 345,
                    "end": 348,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 901,
                    "end": 913,
                    "text": "(18; 14; 20)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 1739,
                    "end": 1746,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "Data Efficient, Personalized Randomized Control Trials (RCTs). Consider the setting with N > 1 types of customers coming to an e-commerce website, which has D > 1 types of promotions to offer. The goal is to find which of the D promotions is best suited for each of the N different customer types. Traditionally, this is be achieved by running N \u00d7D RCTs (i.e., A/B tests). As detailed in Section 4.4, using actual e-commerce A/B testing data, we show that SI can infer these N \u00d7D outcomes by running only 2N experiments 2 (assuming D \u2264 N ); crucially, this does not depend on D. Indeed, the potential application of SI, especially in the context of personalized drug design or clinical trials (customer type = patient type, promotions = drug therapies), if successful, can have a large impact.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic Interventions (SI), A Complete Solution"
        },
        {
            "text": "Synthetic Control (SC). The concept of SC was originally proposed in (3; 2). The robust SC (RSC) and multi-dimensional robust SC (mRSC) methods were recently presented as generalizations of the SC method for the settings of missing and noisy data, and multiple metrics, see (6; 5) and (7), respectively. We refer the reader to (4) and references therein for a detailed overview of SC-like methods. As stated earlier, the works described above evaluate the impact on a target unit if a single intervention had not occurred; this work, on the other hand, aims to predict the counterfactual outcome on a target unit if any one of multiple interventions had occurred. Refer to Table 1 and the related discussion in Section 1 for a comparison of our results with previous SC works.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 673,
                    "end": 680,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Related Works"
        },
        {
            "text": "Heterogeneous Treatment Effects. Randomized control trials (RCTs) are popular methods to study the average treatment effects (ATEs) when the units under consideration are approximately homogeneous. However, RCTs suffer when the units are highly heterogeneous, i.e., when each unit of interest might react very differently to each intervention. A complementary and exciting line of work to tackle this problem has been on estimating heterogeneous treatment effects (see (16) for a textbook style reference); here, the goal is to estimate the effect of a single intervention (or treatment), conditioned on a sufficiently rich set of covariates about a unit. The setting of SI differs from these works in two important ways: (i) it does not require covariate information regarding the units (this is in line with the work of (9)), yet can estimate the heterogeneous treatment effect; (ii) it leverages the latent structure 2. If one has or is already collecting pre-intervention data, then SI only requires N experiments.",
            "cite_spans": [
                {
                    "start": 469,
                    "end": 473,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "across interventions (via a tensor factor model) to estimate the optimal intervention per unit. An interesting line of future work would be to combine the literature on heterogeneous treatment effects with SI to exploit covariate information about units.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Principal Component Regression (PCR). The SI algorithm can alternatively be viewed through the context of PCR (see Proposition 3.1 of (5)). Despite the ubiquity of PCR in practice, the formal literature on PCR is surprisingly sparse. Notable works include (10; 17; 13) . Recently, (5) presented a rigorous analysis on the finite-sample properties of PCR even when the regressors (other donors in our context) are noisily observed. This is known in the statistics literature as error-in-variable regression and is exactly the setting we consider (see Section 3). Thus, we utilize results from (5) to study the theoretical properties of our SI framework. However, as stated earlier, we add to the results of (5) by not only improving the test (out-of-sample) prediction error rate by a factor of 1/ \u221a T 0 , but also showing that PCR consistently estimates the latent model parameter. This model identification result may be of interest to the high-dimensional error-in-variable regression literature.",
            "cite_spans": [
                {
                    "start": 256,
                    "end": 268,
                    "text": "(10; 17; 13)",
                    "ref_id": null
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Additionally, we note that proving SI's post-intervention counterfactual prediction error vanishes to zero is equivalent to showing that PCR generalizes from training data (pre-intervention setting) to testing data (post-intervention setting). However, unlike standard generalization error analyses, we cannot utilize the standard techniques of Rademacher complexity (as done in (5)) as the usual i.i.d assumption is no longer appropriate. Specifically, the data generating process pre-and postintervention may not be identically distributed. The model identification of PCR, which we establish, enables us to show that PCR generalizes well from the training to testing data, without making any distributional assumptions on the data generating process. Please refer to Table 2 and the related discussion in Section 1, or Theorem 3.1, for a comparison of our results with previous work.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 770,
                    "end": 777,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Related Works"
        },
        {
            "text": "Connection to Transfer Learning. Since the effects of interventions can vary, the latent potential outcomes associated with the pre-and post-intervention periods may come from different domains. Additionally, we are only given access to the target unit's pre-intervention labels, as its post-intervention labels are precisely the unobservable counterfactuals we wish to estimate. Therefore, our problem of interest also places us within the transductive transfer learning setting. That is, using the language of transfer learning, the source (pre-intervention) and target (post-intervention) domains may be different yet related, and only the source domain labels are available. Formally connecting our results to the transfer learning literature is interesting future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "In Section 2, we state our assumptions on the underlying model and observed outcomes. We present our primary results in Section 3, which provide bounds on the parameter estimation and post-intervention prediction errors, as well as a description of our data-driven hypothesis test (with theoretical guarantees). Then, in Section 4, we detail applications of SI on real-world data, i.e., COVID-19, A/B testing, and development economics case studies. Finally, we relegate all proofs to the Appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Organization"
        },
        {
            "text": "For any matrix X \u2208 R m\u00d7n , we denote its operator (spectral), Frobenius, and max element-wise norms as X , X F , and X max , respectively. Further, for any vector X \u2208 R n , let X p denote its (d) Potential outcomes tensor M .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tensor Factor Model"
        },
        {
            "text": "(e) Observed outcomes matrix Y . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tensor Factor Model"
        },
        {
            "text": "Let M \u2208 R T \u00d7N \u00d7D be an order-three tensor where its (t,n,d)-th element, M ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Potential Outcomes"
        },
        {
            "text": "Interpretation. Property 2.1 implies that every frontal slice M (d) can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Potential Outcomes"
        },
        {
            "text": "where U (d) \u2208 R T \u00d7r , and V \u2208 R N \u00d7r has (without loss of generality) orthonormal columns. Hence, the low rank tensor model implies there exists an r-dimensional linear subspace of R N , denoted by V , that describes a latent relationship between units that is invariant across interventions. Each intervention can then be interpreted as some linear transformation, denoted by U (d) , applied to this subspace. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Potential Outcomes"
        },
        {
            "text": "We begin by stating a natural property, which we require to theoretically justify SI. Let v n denote the n-th row of V , given in (4) , which is the latent factor associated with unit n. ",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 133,
                    "text": "(4)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Existence of Synthetic Interventions"
        },
        {
            "text": "Interpretation. Under a low-rank tensor factor model, Proposition 2.1 states that the target unit can be expressed as a linear combination of every donor subgroup across all time and interventions. Indeed, this is the key result that enables SI to \"transfer\" the learned linear model from the pre-to postintervention period. In Proposition 2.2 below, we show that Property 2.4 holds with high-probability. Interpretation. By the union bound, \u03b2 (d) exists for all d simultaneously w.p. at least 1\u2212 D d=1 r/N (d) . Proposition 2.2 circumvents the \"pathological\" case where v i is orthogonal to all other rows in V . Since there are at most r\u22121 such rows in any rank r matrix, Proposition 2.2 establishes that, with respect to the unit indexing randomness, this pathological case will not occur w.h.p. (details in Appendix C).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Property 2.4 Given intervention d and unit"
        },
        {
            "text": "We assume every observed outcome is corrupted by noise, i.e., every entry of Y , given by (1), satisfies Y",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "tn represents measurement noise. For ease of notation, let \u03b5 t \u2208 R N denote the noise vector associated with the t-th row of Y . We assume the noise satisfies Properties 2.5 and 2.6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "tn be a sequence of independent mean zero sub-gaussian random variables with Var(\u03b5",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "Interpretation. Since \u03b5 (d) tn are independent, K and \u03b3 2 are constants. However, our analysis goes through for the more general case where the noise is dependent across the donor units for a given t; here, K and \u03b3 2 quantify the level of dependence in the noise between the donors at a given time. As such, we will state Theorems 3.1 and 3.2 in this general setting and show their explicit dependencies on K and \u03b3 2 . Property 2.6 (Missing at random) The entries of Y are independently observed with probability \u03c1 \u2208 (0,1], i.e., \u03c0 tn , given by (1), are a sequence of i.i.d. Bernoulli(\u03c1) random variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "Connection to Error-in-Variable Regression. Without loss of generality, let i = 1 in Proposition 2.1. Then, the observed outcome for the target unit follows a linear model:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "where M",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "pre , for t \u2264 T 0 . Since we observe a noisy version of M (1) pre , namely Y (1) pre , this is exactly the setting of error-in-variable regression.",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 61,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 77,
                    "end": 80,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "Parameter Estimation. To estimate the post-intervention counterfactual potential outcomes, we require a good estimate of \u03b2 (d) , given in (5) . It is well known, however, that recovering the latent model parameter without any additional assumptions is ill-defined since infinitely many solutions to (5) exist. Thus, for the purposes of model identification, it is standard within the error-in-variable regression literature to assume, for instance, \u03b2 (d) is sparse and M (1) pre satisfies the restricted eigenvalue condition; see (18) and references therein. However, for the purposes of prediction, we argue only the component of \u03b2 (d) within the row space of M (1) pre matters since any component within the null space is mapped to zero. This particular \u03b2 (d) is unique and has minimum 2 -norm; we show PCR accurately estimates this vector, which may be of independent interest in the error-in-variable regression literature. For details, see Appendix G, which also contains a useful synthetic simulation ( Figure 9 ) of this phenomenon.",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 141,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 471,
                    "end": 474,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 530,
                    "end": 534,
                    "text": "(18)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 663,
                    "end": 666,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 1009,
                    "end": 1017,
                    "text": "Figure 9",
                    "ref_id": "FIGREF39"
                }
            ],
            "section": "Perturbed Observations: Error-In-Variable Regression Model"
        },
        {
            "text": "We state Theorems 3.1 and 3.2, which hold for all interventions d and units n. Thus, for simplicity and ease of notation, we restrict our attention to estimating the post-intervention counterfactual potential outcomes under a specific intervention d and for unit 1, i.e., given Y , we aim to recover M",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Results"
        },
        {
            "text": "Notation. Throughout, we suppress dependencies on d. Instead, to distinguish between the pre-and post-intervention data (corresponding to no-intervention and intervention d, respectively), we make explicit their dependencies through appropriate subscripts, e.g., (2)) and \u03b2 * = \u03b2 (d) (given in (5)). To avoid confusion, we do not alter N (d) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Results"
        },
        {
            "text": "Evaluation Metric. We evaluate SI based on the post-intervention squared prediction error. Specifically, we define the post-intervention (or test) error for unit 1 under intervention d as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Results"
        },
        {
            "text": "Remark. Although Property 2.5 assumes independent noise entries, our results are stated when \u03b5 tn can be dependent across donors for a given t, i.e., only the target and donor noise must remain independent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Results"
        },
        {
            "text": "Theorem 3.1 Assume Properties 2.1, 2.2, 2.3, 2.4, 2.5, 2.6 hold. Let rank( M pre ) = rank(M pre ) = r pre , and \u03b2 * be the unique vector that satisfies (5) with minimum 2 -norm. For any \u03b4 > 0 and some",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Estimation"
        },
        {
            "text": ", then the following holds w.p. at least 1\u2212\u03b4:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Estimation"
        },
        {
            "text": "Interpretation. The first term within the parentheses on the RHS is exactly the in-sample prediction error for PCR. The second term is the additional cost paid for directly estimating \u03b2 * . Now, consider the special case where the following two conditions hold: (i) M pre is directly observed, i.e., there are no missing values or measurement errors in the donor data; and (ii) the columns of M pre are linearly independent (i.e., \u03b2 * lies within its row space). Then, the bound above agrees with classical parameter estimation results for ordinary least squares (e.g., Remark 2.3 of (19) under Property 2.3.)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Estimation"
        },
        {
            "text": ", the parameter estimation error scales as O( \u03b2 * 2 2 / T 0 ) with high probability; this is with respect to the minimum 2 -norm \u03b2 * . We note that this rate is in line with previous works (c.f. (18; 14; 20) ), where the error also grows as O( \u03b2 * 2 2 / T 0 ). Note, however, these previous works make a key sparsity assumption that \u03b2 * 0 \u2264 r pre , which we do not require for our results. Instead, we make a low-rank assumption on the covariate (donor) matrix. Further, the estimators proposed in (18; 14; 20) explicitly require knowledge of the noise distribution (i.e., its second moment matrix). PCR, on the other hand, does not require this. See Table 2 for a summary of the key points of comparison.",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 207,
                    "text": "(18; 14; 20)",
                    "ref_id": null
                },
                {
                    "start": 498,
                    "end": 510,
                    "text": "(18; 14; 20)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 651,
                    "end": 658,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Parameter Estimation"
        },
        {
            "text": "We denote the right singular vectors of M pre and M post as V pre and V post , respectively, which describe the latent relationship between units during the pre-and post-intervention periods, respectively. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Post-Intervention (Test) Prediction Error"
        },
        {
            "text": "w.p. at least 1\u2212\u03b4 where C 1 ,C 2 ,\u2206 are given in (7).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Post-Intervention (Test) Prediction Error"
        },
        {
            "text": "Interpretation. For simplicity, we let T 0 = \u0398(N (d) ) = \u0398(T ), and suppress dependencies on \u03b2 * and logN (d) . The error bound in Theorem 3.2 is quantified by four terms: (i) the first term, scaling as O(r pre /T 0 ), corresponds to the minmax in-sample prediction error for low-dimensional ordinary least squares with noiseless covariates; (ii) the second term, scaling as O(r pre /(\u03c1 4 T 0 )) is the additional error due to the sparsity and noise in the covariates; (iii) the third term, scaling as O(r pre /(\u03c1 4 T 0 )) is the generalization error; (iv) the fourth term,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Post-Intervention (Test) Prediction Error"
        },
        {
            "text": ", disappears if the error is taken in expectation (see Corollary 34), which is consistent with Theorem 3.1; finally, the scaling, r pre /r post \u2265 1, is the ratio of the pre-and post-intervention donor matrix ranks, which may be a remnant of our proof technique.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Post-Intervention (Test) Prediction Error"
        },
        {
            "text": "Comparison with Literature. Under the setting above, we highlight that in expectation, the test error decays linearly with T 0 , which is a factor of \u221a T 0 improvement over the bound in (5) . Additionally, it is worth mentioning that Theorem 3.2 does not make any distributional assumptions of having i.i.d. covariates as was done in (5) . Such an assumption can be unrealistic for SI as potential outcomes from different interventions are likely to come from different distributions. Instead, the key assumption that enables Theorem 3.2 is a linear algebraic condition: span(V post ) \u2286 span(V pre ). This allows SI to \"generalize\" from the pre-to post-intervention period and estimate potential outcomes under any d using a model learned under a no-intervention state. A more general test error result when this condition fails to hold can be found in Lemma 32 of Appendix H. See Table 1 for a comparison summary.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 189,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 334,
                    "end": 337,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 881,
                    "end": 888,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Post-Intervention (Test) Prediction Error"
        },
        {
            "text": "As shown in Theorem 3.2, a key assumption that enables SI to generalize is span(V post ) \u2286 span(V pre ). This condition gives rise to a natural hypothesis test:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Natural, Data-Driven Hypothesis Test"
        },
        {
            "text": "pre . However, given that the right singular vectors are never observable, we use the top right singular vectors of Y pre ,Y post , denoted as V pre , V post , as proxies. Hence, a natural test statistic is the gap between V post and P Vpre V post . In particular,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Natural, Data-Driven Hypothesis Test"
        },
        {
            "text": "where \u03c4 is our test statistic, \u03c4 \u03b1 is the critical value, and \u03b1 \u2208 (0,1) is the significance level. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Natural, Data-Driven Hypothesis Test"
        },
        {
            "text": "Interpretation. We note that (8) also functions as a quantitative hypothesis test for the validity of the SC method as well, something which, to the best of our knowledge, is missing from the literature. As seen in Section 4, the post intervention prediction error corresponds closely to whether this hypothesis test passes or fails, as desired.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Natural, Data-Driven Hypothesis Test"
        },
        {
            "text": "We extensively test the validity and widespread applicability of SI on real-world data. In particular, we consider three case studies: (i) analyzing the impact of mobility-restricting interventions in mitigating the COVID-19 pandemic; (ii) exploring the effect of different discount strategies to increase user engagement in an A/B testing framework for a large e-commerce company; (iii) studying how 20 different interventions affected immunization rates in Haryana, India as part of a large developmental economics study (11) . Our results indicate that SI can not only be useful in guiding policy-makers as they weigh the trade-offs of different policy interventions, but also in performing personalized, data-efficient randomized control trials.",
            "cite_spans": [
                {
                    "start": 523,
                    "end": 527,
                    "text": "(11)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Experiments Overview"
        },
        {
            "text": "To quantify the accuracy of the counterfactual predictions produced by SI, we need meaningful baselines to compare against. To that end, we define R 2 rct = 1\u2212 SSres SS rct , where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Quantifying Counterfactual Prediction Accuracy"
        },
        {
            "text": "t,rct is the average outcome across all donor units that experienced intervention d at time t. If the units were indeed homogeneous (i.e., they all reacted identically to each intervention), then Y (d) t,rct will be a good predictor of the counterfactual outcome for the target unit, i.e. Y (d)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Quantifying Counterfactual Prediction Accuracy"
        },
        {
            "text": ", and R 2 rct will be correspondingly small. In other words, the R 2 rct -score captures the gain by \"personalizing\" the prediction to the target unit using the SI method over the natural baseline of taking the average outcome of all units who receive that particular intervention. Thus, R 2 rct > 0 indicates the success of SI. Of course, for each unit, we can only evaluate the R 2 rct -score for the actual intervention it received.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Quantifying Counterfactual Prediction Accuracy"
        },
        {
            "text": "Aim, Setup and Key Modeling Choices. We apply the SI methodology to study the impact of mobility restriction policies on COVID-19 related health outcomes at a national level. Below, we list our key modeling decisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Choosing Metric of Interest: Daily Death Counts. Due to its relative reliability and availability, we use daily COVID-19 related death counts as our outcome variable of interest. Another standard metric, number of daily infections, is much less reliable due to the inconsistencies in testing and reporting across regions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Choosing Interventions of Interest: Daily Mobility Rates. Each country has implemented numerous policies to combat the spread of COVID-19. This makes it difficult to analyze any particular policy (e.g., stay-at-home orders vs. schools shutting down) in isolation. However, almost all such policies have been directed towards restricting how individuals move and interact. Thus, we adopt mobility as our notion of intervention, and investigate how a country's change in mobility level translates to the number of potential COVID-19 related deaths. To that end, we use Google's excellent mobility reports (1) to study the change in a country's mobility compared to their respective national baseline from January 2020.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Categorizing Countries by Intervention Received: Average (Lagged) Mobility Score. Studies have shown that there is a median lag of 20 days from the onset of infection to the day of death (e.g., see (25)). Thus, a country's death count on a particular day is a result of the infection levels from approximately 20 days prior. In order to analyze the effect of a mobility restricting intervention from \"Day 0\" (this will denote our intervention point, T 0 ) onwards, we consider a country's mobility score from Day -20 to Day -1. Given that the mobility score in (1) is changing every day, we take the average mobility score of a country from Day -20 to Day -1 and then bucket it into the three distinct, mutually exclusive intervention groups defined as follows (see Figure 3 for a visual depiction of this clustering):",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 766,
                    "end": 774,
                    "text": "Figure 3",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "(a) Low Mobility Restricting Intervention -reduction in mobility is below 5% compared to national baseline from January 2020;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "(b) Moderate Mobility Restricting Intervention -reduction in mobility is between 5% to 35% compared to national baseline from January 2020;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "(c) Severe Mobility Restricting Intervention -reduction in mobility is greater than 35% compared to national baseline from January 2020.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Choosing Pre-and Post-Intervention Periods: Number of Deaths in Country. To apply SI, it is crucial to have well-defined pre-and post-intervention period; in particular, the effects of each country's enacted interventions should only be observed during the post-intervention period. Using Google's mobility reports, we verify that 20 days prior to cumulative 80 deaths in a country (and any time before), none of the selected countries enacted a mobility restricting intervention. Thus, we choose the day a country has cumulative 80 deaths as Day 0, and the pre-and post-intervention periods refer to the days before and after Day 0, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Empirical Results and Key Takeaways. We apply SI using the setup above to produce counterfactual predictions of the daily death counts for 15 days following Day 0 under the three different mobility interventions of interest. This analysis is carried out for 27 countries selected as follows: we (i) only include countries whose mobility changes are tracked by Google mobility reports; (ii) remove countries that have enacted a mobility restricting intervention 20 days prior to Day 0; (iii) remove countries with not enough data in the pre-intervention period of interest. That is, countries that had less than 80 cumulative COVID-19 related deaths in the pre-intervention period. We then group the 27 countries into the three buckets defined above based on their average mobility score, as shown in Figure 3 . Intervention low mobility restriction moderate mobility restriction severe mobility restriction Hypo. Test (\u03b1 = 0.05) Pass Pass Pass R 2 rct -score 0.74 0.14 0.12 Table 3 : Hypothesis test and prediction accuracy results for SI in the context of COVID-19.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 800,
                    "end": 808,
                    "text": "Figure 3",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 974,
                    "end": 981,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Empirical Results. In Table 3 , we show the results of the hypothesis test (see Section 3.3) for the three mobility restricting interventions and the median R 2 rct -score for all 27 countries. The hypothesis test passes for all three interventions at a significance of \u03b1 = 0.05. A median R 2 rct -score of [0.74,0.14,0.12] across the three interventions indicates there is indeed significant heterogeneity amongst the countries on how mobility interventions affect the national death trajectories. Thus, there is significant gains to be had by using SI over naively averaging the outcome across countries that experienced a particular level of mobility reduction. For every mobility restriction level, we display the counterfactual predictions associated with two representative countries that enacted that intervention. We note similar results hold generally across all countries. For the low mobility restricting regime, we show results for the United States and the United Kingdom in Figures 4a and 4c , respectively. The dashed lines on Days 0 -15 are the predicted values under all possible mobility restriction levels and the solid line represents the true national death trajectory. Pleasingly, the predictions produced by SI closely matches the observed death rates in the post-intervention period. Similarly, for the moderate and severe mobility restricting regimes, we display results for Turkey, Brazil, India, and Ireland in Figures 5c, 5a , 6a, and 6c, respectively. Again, the predictions produced from SI closely matches the observed death rates under all different interventions, i.e., mobility restrictions. For each of the countries listed above, we display their top four donor countries (under each intervention) that most closely resemble them. These are shown in Figures 4b, 4d , 5d, 5b, 6b, and 6d respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 29,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 988,
                    "end": 1005,
                    "text": "Figures 4a and 4c",
                    "ref_id": "FIGREF22"
                },
                {
                    "start": 1438,
                    "end": 1452,
                    "text": "Figures 5c, 5a",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 1786,
                    "end": 1800,
                    "text": "Figures 4b, 4d",
                    "ref_id": "FIGREF22"
                }
            ],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Key Takeaways. Importantly, the SI model of the target country is fit in the pre-intervention period, when no intervention has yet occurred. Still, the learnt model transfers to an intervention setting, i.e., when the interventions take effect within the donor countries. This helps validate the SI framework. An \"optimistic\" conclusion one can draw from the figures above is that, uniformly across all countries, there is a significant drop in the number of deaths with even a \"moderate\" drop in mobility (i.e, a 5-35% drop compared to the national baseline). After this point, gains by further restricting mobility seem to be diminishing. We hope this case study shows how SI can be used to guide important policy decisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19: What-if Policy Evaluation via SI"
        },
        {
            "text": "Aim, Setup, and Key Modeling Choices. We consider an A/B testing dataset from a large e-commerce company 4 that issued different discount strategies (interventions) to engage its customer base: 10%, 30%, and 50% discounts over the regular subscription cost. Users were segmented into 25 groups (\u223c 10,000 users per group) based on the historical time and money spent on the platform. The aim of the e-commerce company was to find how these different levels of discounts affected user engagement for each of the 25 user groups. The A/B test was performed by randomly partitioning users in each of the 25 user groups into 4 sub-groups; these sub-groups corresponded to either one of the 3 discount strategies or a control group that received a 0% discount. User engagement in each of these 100 sub-groups (25 user groups multiplied by 4 discount strategies) was measured daily over 8 days.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "Suitability of Case Study to Validate SI. This web A/B testing case study is particularly suited to validate SI as we get to observe the engagement levels of each customer group under each of the three discount strategies, i.e., we observe every \"counterfactual\" trajectory. This is in contrast to the COVID-19 case study in Appendix 4.3 where we only observe the death trajectory for a country for the particular intervention it enacted during the post-intervention period.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "Choosing Pre-and Post-Intervention Periods. For each of the 25 user groups, we denote the daily user engagement trajectories of the sub-groups associated with the control -those who do not receive a discount on their regular subscription -as the pre-intervention period. Correspondingly, for each of the 25 user groups, we denote the daily user trajectories associated with the 10%, 30%, and 50% discount coupons as the post-intervention period.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "Choosing Donor Groups for Post-Intervention Period. We randomly partition the 25 user groups into three clusters, denoted as user groups 1-8, 9-16, and 17-25. For the 10% discount coupon strategy, we choose user groups 1-8 as our donor pool, i.e., we use their post-intervention data under a 10% discount to create the synthetic trajectories of user engagement for user groups 9-25 under a 10% discount. We do the same with the 30% and 50% discount coupon strategies, and user groups 9-16 and 17-25, respectively. See Figure 7a for a visual depiction of the set of experiments/observations the SI algorithm uses to make predictions. Empirical Results and Key Takeaways. We apply SI using the setup above to produce the \"counterfactual\" trajectories for each of the 25 user groups under the three discount strategies. We evaluate the accuracy under the 10% discount coupon strategy using only the estimated trajectories of user groups 9-25 (as we use user groups 1-8 as our donors). Similarly, we use the estimated trajectories of user groups 1-8 and 17-25 for the 30% discount coupon strategy, and user groups 1-16 for the 50% discount coupon strategy. Empirical Results. In Table 4 , we show the hypothesis test results for the three discount strategies and the median R 2 rct -score of the 25 user groups. The hypothesis test passes for all three interventions at a significance of \u03b1 = 0.05. Additionally, SI achieves a median R 2 rct -score of 0.98 across the three discount strategies. This indicates significant heterogeneity amongst the user groups in how they respond to discounts, and thus warrants having to run separate A/B tests for each of the 25 groups.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 518,
                    "end": 527,
                    "text": "Figure 7a",
                    "ref_id": "FIGREF26"
                },
                {
                    "start": 1175,
                    "end": 1182,
                    "text": "Table 4",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "Key Takeaways. Recall that there were a total of 100 distinct experiments run in the A/B testing framework as there were 25 user groups and 4 interventions (0%, 10%, 30%, and 50% discount coupons). However, the SI framework only required observations from 50 experiments. That is, two experiments for each of the 25 user user groups: one in the pre-intervention period (under 0% discount rate) and one in the post-intervention period (under exactly one of the three discount coupon strategies). See Figure 7b for a visual depiction of the experiments conducted by the e-commerce company in comparison to what is required by SI, as shown in Figure 7a .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 499,
                    "end": 508,
                    "text": "Figure 7b",
                    "ref_id": "FIGREF26"
                },
                {
                    "start": 640,
                    "end": 649,
                    "text": "Figure 7a",
                    "ref_id": "FIGREF26"
                }
            ],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "More generally, as described in Section 1, if there are N user groups and D interventions, an ideal RCT performs N \u00d7 D experiments to estimate the best \"personalized\" intervention for every user group. With SI, assuming the tensor factor model (Section 2) holds and D \u2264 N , one only needs to perform 2N experiments. Crucially, the number of required experiments does not scale with D, which becomes significant as the number of interventions, i.e, the level of personalization, grows. Also, if pre-intervention data has been or is being collected, then SI only requires N experiments. This can be significant when experimentation is costly (e.g., clinical trials).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Web A/B Testing: Towards Data Efficient RCTs via SI"
        },
        {
            "text": "Aim, Setup, and Key Modeling Choices. We use data from a large real-world development economics case study, which aimed to increase vaccination rates in seven districts in the state of Haryana, India. This study, carried out by the authors of (11) in collaboration with the Haryana state government, is the first large scale evaluation of the effects of different types of interventions on childhood immunization rates. The Haryana immunization trials were conducted with 2523 villages, with data collected monthly over 13 months, and included a total of 74 different interventions. Each intervention can be encoded by a 3-dimensional discrete-valued vector where its entries represent different levels of (1) financial incentives, (2) social network influence, and (3) information campaigns to encourage vaccinations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Development Economics: Towards \"Personalized\" RCTs via SI"
        },
        {
            "text": "\"Personalized\" RCTs via SI. As is standard in RCTs, the authors in (11) randomly partitioned the 2523 villages into 74 groups, corresponding to the 74 different interventions they aimed to study. They then measured the average increase in immunization rates for each of these 74 groups over the 13 month trial period. Subsequently, they made a single policy recommendation to the Haryana state government, corresponding to the intervention that yielded the highest average increase in immunization rates.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Development Economics: Towards \"Personalized\" RCTs via SI"
        },
        {
            "text": "The aim of this case study is to estimate whether there would have been a greater uptake in immunization amongst the villages if, instead of a single policy recommendation for all villages, a tailored intervention recommendation was made for each village.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Development Economics: Towards \"Personalized\" RCTs via SI"
        },
        {
            "text": "Data Pre-Processing to Run SI. We restrict our attention to the 20 most frequent interventions, where the frequency of an intervention is measured by the number of villages who experienced said intervention, e.g., 175 villages experienced the most frequent intervention while 18 villages experienced the twentieth most frequent intervention. Let D denote the collection of these 20 interventions. There were N = 1302 villages which received one of the top 20 most frequent interventions. Based on conversations with the authors of (11), it was appropriate to choose T 0 = 4 months, i.e., the first four months (of the total 13 months of data collected) was considered the pre-intervention period.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Development Economics: Towards \"Personalized\" RCTs via SI"
        },
        {
            "text": "Empirical Results and Key Takeaways. We follow the same setup as in the COVID-19 case study. That is, we iterate over the 1302 villages such that each village is designated to be the target village for some iteration. In the pre-intervention period, we build a model of the target village under each of the twenty interventions using the appropriate donor village sub-groups. Then in the post-intervention phase, we estimate the counterfactual immunization rates of the target village under each intervention using data from the appropriate donor village sub-group and fitted linear model. Tables 5 and 6 , we show the results of the hypothesis test for the twenty interventions considered and the median R 2 rct -scores. The hypothesis test passes for all but four interventions at a significance of \u03b1 = 0.05. Indeed, the corresponding median R rct -scores are among the lowest, with three of four being the minimum achieved scores. This highlights the use of the hypothesis test as a helpful robustness check for when to trust the counterfactual predictions produced by SI. For the remaining 17 interventions that do pass the hypothesis test, we generally see significantly higher R 2 rct -scores, indicating again that there is significant heterogeneity amongst villages. Key Takeaways. The question we set out to answer was whether providing \"personalized\" intervention recommendations to each village would have led to significant increases in the immunization rates for that village over the single intervention recommendation made by the authors of (11), as is standard practice in a RCT. Using the counterfactual estimates produced by SI, we define the average utility of intervention d for village n as",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 590,
                    "end": 604,
                    "text": "Tables 5 and 6",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Development Economics: Towards \"Personalized\" RCTs via SI"
        },
        {
            "text": "In words, for a particular intervention d, this is the average increase in immunization rates over the post-intervention phase. Further, let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Empirical Results. In"
        },
        {
            "text": "where d n \u223c Uniform(D). In words, U rand represents the estimated average utility across all villages if a randomly sampled intervention had been administered. U rct represents the estimated best single intervention across all villages in hindsight (i.e., the RCT policy). Lastly, U tailored is the estimated average utility for each village under its optimal intervention. Normalizing U rand as 1.0, we find U rct and U tailored are 1.3x and 2.8x higher, respectively, compared to U rand . Thus, if the units of interest are heterogeneous, then using SI to produce tailored interventions can lead to large gains. We stress these are only estimated utilities as we, of course, never observe each village under all interventions. Lastly, we note the estimated single best policy that maximized U rct matched the policy recommendation made in (11) .",
            "cite_spans": [
                {
                    "start": 841,
                    "end": 845,
                    "text": "(11)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Empirical Results. In"
        },
        {
            "text": "Average Utility U rand : Random Assignment 1.0 U rct : Single-best RCT Policy 1.3 U tailored : Personalized Recommendation 2.8 Table 7 : Average utilities associated with three types of intervention interventions per village: random assignment, single-best RCT policy, and personalized intervention recommendation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 127,
                    "end": 134,
                    "text": "Table 7",
                    "ref_id": null
                }
            ],
            "section": "Recommendation Type"
        },
        {
            "text": "Without loss of generality, we restrict our attention to the setting where we are interested in predicting the \"de-noised\" post-intervention counterfactual trajectory of our target unit of interest under intervention d; therefore, it suffices to restrict our attention to the target unit and donor units within I (d) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "For ease of notation, we will suppress all dependencies on d. In particular, throughout the rest of our proofs (unless otherwise specified), we adopt the following notation: Further, we assume their SVDs take the following form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "where, rank(M ) = k = r Observed Outcomes. We denote the observed pre-and post-intervention donor matrices as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "which admit the following SVDs:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Due to the perturbations of H and H , Y and Y may be full-rank. respectively, with the following SVDs: Linear Model. Let \u03b2 = \u03b2 (d) be the estimator of \u03b2 * = \u03b2 (d) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Projection Matrices. For any matrix Q with orthonormal columns, let P Q := QQ T denote the projection matrix onto the subspace spanned by the columns of Q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Constants & Model Parameters. Throughout these proofs, we will let C > 0 denote an absolute constant that is independent of any model parameters. For ease of notation, we will allow the value of C to change from line to line. The dependencies on the model parameters (e.g., \u03c3,\u03b3,K) will be made explicit.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Remark 1 We note that our notation changes are also meant to reflect their applicability towards (high-dimensional) error-in-variable regression and transductive transfer learning settings. As such, the pre-and post-intervention donor matrices can be viewed as the training and testing covariates, respectively, which are perturbed by measurement noise and missingness. The pre-intervention target unit observations then correspond to the observed labels or responses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Remark 2 Although Property 2.5 states that the entries of H and H are independent, our analyses allow for independent rows (as opposed to entries). For the proofs to follow through, we only need the target (response) unit noise to be independent from the donor (covariate) noise. Thus, for the remainder of these proofs, we operate in the more general setting where we have row-wise independence of H and H , instead of just restricted entry-wise independence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Proof Notations"
        },
        {
            "text": "Lemma 3 Let X be a mean zero, sub-gaussian random variable. Then for any \u03bb \u2208 R, Eexp(\u03bbX) \u2264 exp C\u03bb 2 X 2 \u03c8 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Sub-Gaussian Properties"
        },
        {
            "text": "Lemma 4 Let X 1 ,...,X n be independent, mean zero, sub-gaussian random variables. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Sub-Gaussian Properties"
        },
        {
            "text": "Theorem B.1 (Bernstein's inequality) Let X 1 ,...,X n be independent, mean zero, sub-exponential random variables. Then, for every t \u2265 0, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Concentration Inequalities"
        },
        {
            "text": "where c > 0 is an absolute constant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Concentration Inequalities"
        },
        {
            "text": "Theorem B.2 (Hanson-Wright inequality) Let X = (X 1 ,...,X n ) \u2208 R n be a random vector with independent, mean zero, sub-gaussian coordinates. Let A be an n\u00d7n matrix. Then, for every t \u2265 0, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Concentration Inequalities"
        },
        {
            "text": "Notation. Let A \u2208 R m\u00d7n be a low-rank matrix, and let H \u2208 R m\u00d7n be a perturbation matrix. We decompose the SVD of A as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Perturbation Theory"
        },
        {
            "text": "where U \u2208 R m\u00d7r ,V \u2208 R n\u00d7r ,\u03a3 1 \u2208 R r\u00d7r , and \u03a3 2 \u2208 R (m\u2212r)\u00d7(n\u2212r) . Let Z = A+H with a similar SVD partition:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Perturbation Theory"
        },
        {
            "text": "where U , U \u22a5 ,\u03a3 1 ,\u03a3 2 , V , and V \u22a5 have the same structures as U ,U \u22a5 ,\u03a3 1 ,\u03a3 2 ,V , and V \u22a5 , respectively. (9) and (10), respectively. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Perturbation Theory"
        },
        {
            "text": "where \u03c3 i denotes the i-th singular value of A. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Perturbation Theory"
        },
        {
            "text": "tn .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Perturbation Theory"
        },
        {
            "text": "Proof Fix an intervention d, and recall I (d) is the corresponding, randomly sampled donor group. Under Property 2.1, it follows that rank(V ) = r; hence, it must be that dim(span{",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Proof of Proposition 2.2"
        },
        {
            "text": "where v n denotes the n-th row of V . Since I (d) and the target unit (n = 1) are sampled randomly from [N ], the probability that v 1 / \u2208 span{(v n ) n\u2208I (d) } is at most r/N (d) (since among the I (d) \u222a{1} units, there can be at most r linearly independent vectors). Thus, for any d, defining",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Proof of Proposition 2.2"
        },
        {
            "text": "In this section, we study the impact of measurement noise and sparsity in the donor (covariate) observations through the matrix Y \u2212 \u03c1M . Specifically, we analyze the impact of perturbations through the operator (spectral) norm and 2,\u221e -norm, and state the primary results in Lemmas 9 and 11, respectively. These results will be critical as we bound the prediction and parameter estimation errors in high probability. Importantly, we highlight that the following results hold for any M that satisfies Property 2.2, H that satisfies Property 2.5, and Y that satisfies Property 2.6. As such, we will let M ,H, and Y be matrices of size m\u00d7n (only within this section) for generality. Further, for any m\u00d7n matrix Q, let Q i \u2208 R n and Q j \u2208 R m denote the i-th row and j-th column of Q, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix D. Impact of Measurement Noise and Sparsity"
        },
        {
            "text": "Lemma 6 Suppose that X \u2208 R n and P \u2208 {0,1} n are random vectors. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Proof Given a deterministic binary vector P 0 \u2208 {0,1} n , let I P 0 = {i \u2208 [n] : P 0i = 1}. Observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Here, \u2022 denotes the Hadamard product (entrywise product) of two matrices. By definition of the \u03c8 2 -norm,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Let u 0 \u2208 S n\u22121 denote the maximum-achieving unit vector (such u 0 exists because inf{\u00b7\u00b7\u00b7} is continuous with respect to u and S n\u22121 is compact). Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "For any u \u2208 S n\u22121 and P 0 \u2208 {0,1} n , observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Therefore, taking supremum over u \u2208 S n\u22121 , we obtain ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Proof Let P \u2208 {0,1} m\u00d7n denote a random matrix of independent Bernoulli random variables with parameter \u03c1. Further, let X = M +H. Note that Y i = X i \u2022P i when Property 2.6 is assumed and is identified with 0. By triangle inequality,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "By the definition of X, Property 2.5, and Lemma 6, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Moreover, Property 2.2 and the i.i.d. property of P ij for different j gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "The first inequality follows from Lemma 4, the second inequality is immediate, and the last inequality follows from Property 2.2. Lastly,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Lemma 8 Suppose Property 2.6 holds. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Proof We follow the proof of Lemma A.2 of (21) and state it here for completeness. Throughout, for any matrix Q \u2208 R m\u00d7n , let Q \u2208 R n denote the -th row of Q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "To begin, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Importantly, we highlight the following relations: for any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Operator Norm"
        },
        {
            "text": "Using the linearity of expectations, the expected value of the (i,j)-th entry of Z ( ) can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Suppose i = j, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "On the other hand, if i = j,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Therefore, we can express Z ( ) as the sum of two matrices where the diagonal components are generated from (11) and the off-diagonal components are generated from (12) . That is,",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 112,
                    "text": "(11)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 164,
                    "end": 168,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Taking the sum over all rows \u2208 [m] yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "To complete the proof, we apply triangle inequality to (13) to obtain",
            "cite_spans": [
                {
                    "start": 55,
                    "end": 59,
                    "text": "(13)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Since H is zero mean, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Collecting terms completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Lemma 9 Assume Properties 2.2, 2.5, 2.6 hold. Then for any s \u2265 0, the following holds w.p. at least 1\u22122exp \u2212s 2 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Applying triangle inequality gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "By Lemma 7, we establish that the rows ofH are sub-gaussian with",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "which are also independent by assumption; hence, we can apply Lemma 38 to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "with probability at least 1\u2212exp \u2212s 2 . Next, we apply Lemma 8, which gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Let C = C(1+\u03b3 2 )(1+K 2 )(1+\u03c3 2 ). Combining the above results yields,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "Putting everything together, we conclude",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, let us fix a row \u2208 [m] and denote"
        },
        {
            "text": "To prove Lemma 11, we will establish that the columns of Y \u2212\u03c1M are also sub-gaussian.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "Lemma 10 Assume Properties 2.2, 2.5, 2.6 hold. Then for every j \u2208 [n],",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "Proof Let e j denote the j-th canonical vector. Observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "where (a) follows from Lemma 4. The conclusion then follows from Lemma 7. Y j \u2212\u03c1M j 2 2 \u2264 m(\u03c3 2 \u03c1+\u03c1(1\u2212\u03c1))+s.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "Proof By Lemma 10, we have that the columns of Y \u2212\u03c1M are sub-gaussian random vectors in R m satisfying Y j \u2212\u03c1M j \u03c8 2 \u2264 CK for all j \u2208 [n]. Further, since the rows of Y \u2212\u03c1M are assumed to be independent, it follows that for every column j \u2208 [n], the coordinates of Y j \u2212\u03c1M j are independent, mean zero, sub-gaussian random variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "To that end, let us fix j \u2208 [n] and define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "is a sum of independent, mean zero, sub-exponential random variables with",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "Moreover, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "As a result, using Bernstein's inequality (Theorem B.1), we have that . We now unfix j by applying a union bound. Thus, for any s \u2265 0",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": ". This completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. 2,\u221e -norm"
        },
        {
            "text": "In this section, we will bound the estimation error between M and M through the 2,\u221e -norm error. As such, we will return to our original setting with the dimensions (e.g., N,T 0 ) defined as in Appendix A. Throughout the rest of these proofs, we define the HSVT estimation error as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E. Hard Singular Value Thresholding (HSVT) Estimation Error"
        },
        {
            "text": "We first state our primary result in Lemma 19, which holds with high probability, followed by its proof. In order to establish Lemma 19, we state its deterministic counterpart in Lemma 18, which expresses the estimation error in terms of the operator and 2,\u221e -norms of Y \u2212\u03c1M . Lemmas 9 and 11 of Appendix D are then utilized to analyze our particular setting of interest, i.e., when Properties 2.2, 2.5, and 2.6 hold. The remainder of the subsection is dedicated to proving the helper lemmas of these results. We begin, however, with notation and a useful observation of the HSVT operation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E. Hard Singular Value Thresholding (HSVT) Estimation Error"
        },
        {
            "text": "Notation. Throughout this section, let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E. Hard Singular Value Thresholding (HSVT) Estimation Error"
        },
        {
            "text": "Further, for any m\u00d7n matrix Q, let Q j \u2208 R m denote the j-th column of Q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E. Hard Singular Value Thresholding (HSVT) Estimation Error"
        },
        {
            "text": "Remark. We note that the following results immediately hold for M and M with (i) T 0 replaced with T \u2212T 0 ; and (ii) k replaced with k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E. Hard Singular Value Thresholding (HSVT) Estimation Error"
        },
        {
            "text": "Consider a matrix Q \u2208 R m\u00d7n with the following SVD",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "here, U k \u2208 R m\u00d7k and V k \u2208 R n\u00d7k denote the matrices consisting of the top k left and right singular vectors of Q, respectively, and S k = diag(s 1 ,...,s k ) \u2208 R k\u00d7k . We now show how P U k \u2208 R m\u00d7m relates to the HSVT operation that retains the top k singular components.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "Lemma 12 Let Q = HSVT(Q,k). Then for any j \u2208 [n],",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "Proof Let e j \u2208 R n denote the canonical basis vector in R n . Then using the orthonormality property of U , it follows that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "This completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "Remark 13 Suppose we have randomly missing data. By Lemma 12, and linearity of the projection operator, we note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.1. A Column Representation for the HSVT Operator"
        },
        {
            "text": "High-Probability Events. We define the following events: for any \u03b4 1 ,\u03b4 2 ,\u03b4 3 ,\u03b4 4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Finally, we denote",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "E 1 occurs with high probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Lemma 14 Assume Properties 2.2, 2.5, and 2.6 hold. Then for any \u03b4 1 > 0, it follows that P(E c 1 ) \u2264 \u03b4 1 . Proof The proof follows immediately from Lemma 9 for any s \u2265 C log(1/\u03b4 1 ). E 2 occurs with high probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Lemma 15 Assume Property 2.6 holds. Then for any \u03b4 2 > 0, it follows that P(E c 2 ) \u2264 \u03b4 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Proof By the Binomial Chernoff bound, for \u03b1 > 1,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "By the union bound, P(\u03c1/\u03b1 \u2264 \u03c1 \u2264 \u03b1\u03c1) \u2265 1\u2212P( \u03c1 > \u03b1\u03c1)\u2212P( \u03c1 < \u03c1/\u03b1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Noticing \u03b1 + 1 < 2\u03b1 < 2\u03b1 2 for all \u03b1 > 1, and setting the above probability to be at least 1 \u2212 \u03b4 2 and solving for \u03b4 2 completes the proof. Proof The proof follows immediately from Lemma 11 for any s \u2265 CK 2 T 0 log(N/\u03b4 3 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Lemma 17 Assume Properties 2.2, 2.5, and 2.6 hold. Then for any \u03b4 4 > 0, it follows that P(E c 4 ) \u2264 \u03b4 4 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "Proof Using the arguments made in the proof of Lemma 11, we see that the columns of Y \u2212\u03c1M are sub-gaussian random vectors with independent, mean zero, sub-gaussian coordinates. Additionally, its sub-gaussian norms are bounded by CK. Now, let us fix a column j \u2208 [N ], and let X = Y j \u2212 \u03c1M j such that X t = Y tj \u2212 \u03c1M tj . Then, we can express",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": ". Note that we have made use of the following facts: P U M = 1 and P U M 2 F = k. To bound the expected value, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "where u i denotes the i-th column of U M (the i-th left singular vector of M ). By the independence of the entries of X and the orthonormality of U M ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "We now unfix j by applying a union bound, which yields for any s \u2265 0, \u03c1) . Setting the above probability to be less than \u03b4 4 and solving for s, we establish that the probability is bounded above by \u03b4 4 if and only if s \u2265 CK 2 klog(N/\u03b4 4 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 70,
                    "end": 72,
                    "text": "\u03c1)",
                    "ref_id": "FIGREF25"
                }
            ],
            "section": "E.2. High Probability Bounds on Noise Deviation"
        },
        {
            "text": "We begin by stating the key lemma used to prove Lemma 19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Lemma 18 Suppose \u03c1/\u03b1 \u2264 \u03c1 \u2264 \u03b1\u03c1 for some \u03b1 \u2265 1. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Proof We prove our key lemma in three steps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Step 1. Fix a column index j \u2208 [N ]. Observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Since rank( M ) = k, it follows that P U M is an orthogonal projection operator onto the span of the top k left singular vectors of Y , namely, span{ u 1 ,..., u k }. Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Additionally, by (15) , we have that",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "(15)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "by the Pythagorean theorem. It remains to bound the terms on the right hand side of (17).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Step 2. We begin by bounding the first term on the right hand side of (17) . Again, applying Lemma 12, we can rewrite",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 74,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Using the Parallelogram Law (or, equivalently, combining Cauchy-Schwartz and AM-GM inequalities), we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "To arrive at the above inequality, note that Condition 2 implies 1/ \u03c1 \u2264 \u03b1/\u03c1 and (\u03c1\u2212 \u03c1)/ \u03c1 2 \u2264 (\u03b1\u22121) 2 . Further, using the Parallelogram Law, observe that the first term of (18) can be decomposed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "We now bound the first term on the right hand side of (19) separately. First, we apply Theorem B.3 to arrive at the following inequality:",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "text": "(19)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "where U M and U M denote the top k left singular vectors of Y and M , respectively. Then, it follows that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Combining the inequalities together, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Step 3. We now bound the second term of (17). Using (31), we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Inserting (21) and (22) back into (17) , and observing that this inequality holds for every column j \u2208 [N ] completes the proof.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Lemma 19 Suppose Properties 2.2, 2.3, 2.5, 2.6 hold. Consider rank( M ) = rank(M ) = k. For any \u03b4 > 0, if \u03c1 > Clog(1/\u03b4) N T 0 , then the following holds with probability at least 1\u2212\u03b4:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": ", and C 2 = C 1 K 2 (1+log 3/2 (1/\u03b4)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Proof From Lemma 18, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "In Lemmas 14, 15, 16 , and 17, set \u03b4 i = \u03b4/4 for any \u03b4 > 0. Then,",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 13,
                    "text": "Lemmas 14,",
                    "ref_id": null
                },
                {
                    "start": 14,
                    "end": 17,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 18,
                    "end": 20,
                    "text": "16",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "where E is given by (16) .",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Let C = C(1+\u03c3 2 )(1+\u03b3 2 )(1+K 2 ). We then have the following bounds:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Further since, \u03c1 > Clog(1/\u03b4) N T 0 , for sufficiently large absolute constant C, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Collecting and simplifying the above bounds, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": ". Letting C 2 = C 1 K 2 (1+log 3/2 (1/\u03b4)), we bound \u2206 as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Relabeling the above bound as \u2206 completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.3. Proof of Lemma 18"
        },
        {
            "text": "Corollary 20 Suppose the conditions of Lemma 19 hold. Then for any \u03b4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.4. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.4. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Proof The proof follows that of Lemma 19 under the event E, given by (16) .",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "E.4. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Notation. For all t \u2264 T 0 , we define our pre-intervention estimates of M t1 (latent potential outcome under the no-intervention state) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "where M t is the t-th row of M . We define the corresponding pre-intervention (training or in-sample) prediction error as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "We state the error bound in high-probability in Lemma 23. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "t1 ] t\u2264T 0 denote the pre-intervention target vector of observations. By (5), we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "On the other hand, the optimality of \u03b2 yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Combining (25) and (26), we have",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "(25)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "We now apply (generalized) H\u00f6lder's inequality with q 1 = 1 and q 2 = \u221e to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Normalizing by T 0 gives the desired result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Lemma 22 Suppose Property 2.4 holds. Consider rank( M ) = rank(M ) = k. Then for any \u03b4 > 0, the following holds w.p. at least 1\u2212\u03b4:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Since Q is an orthogonal projection operator, it follows that Q 2 F = k, Q = 1, and Qu 2 \u2264 u 2 for any u \u2208 R T 0 . Now, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "It remains to bound each term independently. To begin, for any s 1 \u2265 0, Lemma 37 gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Using the law of total expectations and the independence within the entries of \u03b5, we bound the expectation as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Further, for any s 2 \u2265 0, Lemma 36 gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "then for any s 3 \u2265 0, Lemma 36 yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "which implies that, with probability at least 1\u2212exp \u2212cs 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "By triangle inequality, it follows that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "where the final inequality follows from Property 2.2. To complete the proof, we fix any \u03b4 > 0 and set the above probabilities to be less than \u03b4/3 to solve for s 1 ,s 2 , and s 3 . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Proof Let us fix some \u03b4 > 0. We define the event",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "which occurs with probability at least 1\u2212\u03b4 by Lemma 22. Recall Lemma 21:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "We note the following simplification:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "where C 1 ,\u2206 are given by (23) , and C 3 = C 1 (1+log(1/\u03b4)). Additionally,",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "(23)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "where C 2 is given by (23) and C 4 = C 1 K 2 (1+log 7 4 (1/\u03b4)). Thus, under E pre , we use the above results to conclude",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "text": "(23)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Relabeling C 4 completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Interpretation of Bound in Lemma 23. The first term in the result of Lemma 23 above represents the minmax error rate from low-dimensional ordinary least squares regression with noiseless covariates; the second term corresponds to the HSVT estimation error as the pre-intervention donor (training covariate) matrix is noisily observed; the third term corresponds to the error due to providing a high-probability bound (which will be absent if we choose the expected error to be our metric of choice). Proof Let Q = M M \u2020 \u2208 R T 0 \u00d7T 0 . Using the arguments that led to (27) and linearity of expectations, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Under the independence assumptions, we have the following equalities:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Using the cyclic and linearity properties of the trace operator, we further have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "This completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Corollary 25 Suppose the conditions of Lemma 23 hold. Then for any \u03b4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Proof We follow the proof of Lemma 23. In particular, under the event E (given by (16)), we apply Lemma 21 to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "We complete the proof by applying Lemmas 24 and 19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix F. Pre-intervention (Train) Prediction Error"
        },
        {
            "text": "Proof Sketch. In order to provide a bound on the parameter estimation error (Theorem 3.1), we will first show that V M is a good approximation to the latent feature space spanned by the columns of V M , provided that Y is thresholded appropriately (Lemma 26). Next, we state Lemma 28, which bounds the error between \u03b2 and any \u03b2 * that is a solution to (5), when projected onto the subspace spanned by the columns of V M . Since our estimator \u03b2 lies within V M , which is shown to be close to V M as per Lemma 26, it follows that \u03b2 is a good approximation of the component of \u03b2 * that lives within V M . This is formalized in Lemma 29. For a geometric picture of the proof sketch, see Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 684,
                    "end": 692,
                    "text": "Figure 8",
                    "ref_id": "FIGREF38"
                }
            ],
            "section": "Appendix G. Parameter Estimation"
        },
        {
            "text": "Notation. Throughout, we will denote U M \u22a5 \u2208 R T 0 \u00d7(T 0 \u2212k) and V M \u22a5 \u2208 R N \u00d7(N \u2212k) as the orthogonal complements to U M and V M . We continue to define \u03bd 1 = Y \u2212\u03c1M , as in (14) , and also define",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "(14)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Appendix G. Parameter Estimation"
        },
        {
            "text": "We first state Lemma 26, which bounds the misalignment between the subspaces spanned by V M and V M . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.1. Learning Subspaces"
        },
        {
            "text": "Proof From Theorem B.3 we have:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.1. Learning Subspaces"
        },
        {
            "text": "where V M and V M denote the top k right singular vectors of Y and M , respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.1. Learning Subspaces"
        },
        {
            "text": "Having shown that V M is close to V M in Lemma 26, we now bound the gap between \u03b2 and any \u03b2 * that satisfies (5) in the subspace spanned by V M ; this is formalized in Lemma 28.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 112,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "G.2. Bounding the Projected Parameter Estimation Error"
        },
        {
            "text": "Recall that s i denotes the i-th singular value of Y . Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Proof Observing that Y = (Y \u2212\u03c1M )+\u03c1M and applying Weyl's Inequality (Lemma 5) completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Lemma 28 Suppose Property 2.4 holds. Consider rank( M ) = rank(M ) = k. Then, for any \u03b2 * that is a solution to (5),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Proof Recall that s i denotes the i-th singular value of Y . To achieve our desired result, we will upper and lower bound the 2 -norm of M ( \u03b2 \u2212\u03b2 * ). To begin, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Applying Lemma 27 and combining the above results completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Below, we state Lemma 29, which provides a deterministic bound between \u03b2 and the unique \u03b2 * satisfying (5) with minimum 2 -norm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Lemma 29 Suppose Property 2.4 holds. Consider rank( M ) = rank(M ) = k and the unique \u03b2 * that satisfies (5) with minimum 2 -norm. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "where \u039b M is given by (30).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Proof To begin, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "To bound the first term of the above inequality, we can appeal to Lemma 28. Thus, it remains to bound the second expression.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Observing that V T M \u22a5 \u03b2 = 0 yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Let V M \u22a5 \u2208 R N \u00d7(N \u2212k) denote the orthogonal complement of V M . Since V T M \u22a5 \u03b2 * = 0 by assumption, we apply Lemma 26 to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "where \u039b M is given by (32). Putting everything together and applying Lemma 28, we arrive at the following inequality:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "This completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 27"
        },
        {
            "text": "Proof From Lemma 29, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Now, suppose E (given by (16)) occurs. Then by Lemmas 19 and 23, we note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "where C 2 ,\u2206 1 are given by (28). Further, under E and Property 2.3,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "where C 3 = C(1+\u03c3 2 )(1+\u03b3 2 )(1+K 2 ). Our conditions on \u03c1 additionally yield",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Collecting terms and simplifying gives us the desired bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "The proof is complete after relabeling constants.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Model. We construct our underlying training covariates M \u2208 R T 0 \u00d7N via the probabilistic PCA model as described in Section 2.1. That is, we first generate M k \u2208 R T 0 \u00d7k by sampling each entry from a standard normal distribution, independently of other entries. Then, we sample a transformation matrix Q \u2208 R k\u00d7N , where each entry is uniformly and independently sampled from {\u22121/ \u221a k, 1/ \u221a k}. The final matrix then takes the form M = M k Q. We define rank(M ) = k = N 1 4 , where N \u2208 {128, 256, 512}. Next, we generate \u03b2 \u2208 R N by first sampling from a multivariate standard normal vector with independent entries and then arbitrarily scaling the resulting values by 5. The underlying response vector M 1 \u2208 R T 0 is then defined to be the product M 1 = M \u03b2. Finally, the model parameter of interest, \u03b2 * , is then computed as",
            "cite_spans": [
                {
                    "start": 470,
                    "end": 473,
                    "text": "1 4",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Observations. We consider an additive noise model. Specifically, the entries of \u03b5 \u2208 R T 0 are sampled i.i.d. from a normal distribution with mean zero and variance \u03c3 2 = 0.2. The entries of H \u2208 R T 0 \u00d7N are sampled in an identical fashion. We then define our observed response vector as Y 1 = M 1 +\u03b5 and corrupted covariate matrix as Y = M +H.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Results. Using (Y 1 ,Y ), we perform PCR to yield \u03b2. To show that PCR can accurately recover \u03b2 * , the minimum 2 -norm solution, we compute the 2 -norm parameter estimation error, or root-mean-squarederror (RMSE), with respect to \u03b2 * and \u03b2 in Figures 9a and 9b , respectively. As suggested by Figure 9a , the RMSE with respect to \u03b2 * roughly aligns for different values of N , after rescaling the sample size as T 0 /(k 2 \u221a logN ), and decays to zero as the sample size increases; this is predicted by Theorem 3.1. On the other hand, Figure 9b shows that the RMSE with respect to \u03b2 stays roughly constant across different values of N . Therefore, as established in (5), PCR performs implicit regularization by not only de-noising the observed covariates, but also finding the minimum-norm solution.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 243,
                    "end": 260,
                    "text": "Figures 9a and 9b",
                    "ref_id": "FIGREF39"
                },
                {
                    "start": 293,
                    "end": 302,
                    "text": "Figure 9a",
                    "ref_id": "FIGREF39"
                },
                {
                    "start": 534,
                    "end": 543,
                    "text": "Figure 9b",
                    "ref_id": "FIGREF39"
                }
            ],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "(a) 2 -norm error of \u03b2 with respect to the minimum 2 -norm solution of (5), i.e., \u03b2 * .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "(b) 2 -norm error of \u03b2 with respect to a random solution to (5). Remark. Recall the discussion of Theorem 3.1 in Section 3.1. That is, if we apply the inequality \u03b2 * 1 \u2264 \u221a N \u03b2 * 2 , then for any fixed \u03b4, the parameter estimation error scales as O(k 2 \u221a logN \u03b2 * 2 2 /(N \u2227T 0 )). Thus, we choose our rescaled sample size to be T 0 /(k 2 \u221a logN ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G.3. Proof of Theorem 3.1"
        },
        {
            "text": "Having shown that V M is close to V M (Lemma 26), and \u03b2 is close to the unique \u03b2 * that lives within V M (Theorem 3.1), we are now ready to complete the proof for our post-intervention (test) counterfactual prediction error.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix H. Post-intervention (Test) Prediction Error"
        },
        {
            "text": "Notation. As before, we define \u03bd 1 = Y \u2212\u03c1M and \u039b M = \u03bd 1 /(\u03c1s k ), given by (14) and (30), respectively. Additionally, we define \u03bd 1 = Y \u2212\u03c1M . We also define the following events: for any \u03b4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix H. Post-intervention (Test) Prediction Error"
        },
        {
            "text": "where E is given by (16) .",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Appendix H. Post-intervention (Test) Prediction Error"
        },
        {
            "text": "Lemma 30 Let Property 2.4 hold. Consider rank( M ) = rank(M ) = k, rank( M ) = rank(M ) = k , and the unique \u03b2 * that satisfies (5) with minimum 2 -norm. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Proof Let x = \u03b2 \u2212\u03b2 * . Further, it is convenient to express M as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "This yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 1. To bound the first term of (37), observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Lemma 32 Assume the conditions of Theorem 3.1 hold. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "C 3 , \u2206 1 are given by (28); C 4 = C(1 + \u03c3 2 )(1 + \u03b3 2 )(1 + K 2 ); C 5 = C 4 (1 + log(1/\u03b4)); and T = T 0 \u2227(T \u2212T 0 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Proof By Lemma 32, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "We will bound each term independently. However, we first use the arguments that led to (24) to establish P(E ) \u2265 1\u2212\u03b4, where E is given by (36). Throughout, we suppose E occurs. Importantly, we highlight that under E and Property 2.3,",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 91,
                    "text": "(24)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 1. Recall from (34),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 3 ,\u2206 1 are given by (28). Further, Property 2.3 and (35) yield",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 2. We follow the proof of term 1. By Property 2.3, we have \u03bd 1 \u2264 s 1 . As a result, we conclude that the second term is bounded above by the first term.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 3. We apply Lemma 19 to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 3 is given by (28).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 4. We use (44) to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Term 5. By (44), observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "This yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Combining the bounds for terms 4 and 5 gives us the following upper bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where T = T 0 \u2227(T \u2212T 0 ). Term 6. Using the arguments from above, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 5 = C 4 (1+log(1/\u03b4)). Further, we note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Combining the above yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Conclusion. Putting everything together, we conclude",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where \u2206 pre ,\u2206 HSVT ,\u2206 gen ,\u2206 model are given in (43).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Completing Proof of Theorem 3.2. Proof We will simplify the terms in Lemma 32. Throughout, let T = T 0 \u2227(T \u2212T 0 ). To begin, since span(V M ) \u2286 span(V M ), it follows that \u2206 model = 0, k \u2265 k , and k k \u2206 pre +\u2206 HSVT \u2264 k k",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 3 ,\u2206 1 are given by (28). Further,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 4 is given by (43). Collecting and simplifying the above results gives the following:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "where C 1 is given by (28) and C 6 = C 1 K 2 (1 + log 2 (1/\u03b4)). The proof is complete after relabeling constants and observing that, by definition,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.1. Helper Lemmas"
        },
        {
            "text": "Consider the following update to the post-intervention counterfactual estimates defined in (3):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "That is, since the underlying potential outcomes are assumed to be bounded under Property 2.2, we also restrict our target unit estimates to lie within the unit interval. Importantly, we note that our previous results (e.g., Theorems 3.1 and 3.2) continue to hold. However, we update our estimates in (47) in order to bound the post-intervention error in expectation, which we state in Corollaries 33 and 34; in particular, the update is used to control the error in the \"bad\" event case, where E (given by (36)) does not occur.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Notation. To that end, we define the corresponding post-intervention error as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where M (d) t1 , the thresholded estimate, is now given by (48). By construction (and Property 2.2), we highlight the following relation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where E post ( M \u03b2) is given by (6) .",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 35,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Corollary 33 Suppose the conditions of Theorem 3.2 hold. Then for any \u03b4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where C 2 is given by (29); and C 3 = C(1+\u03c3 2 )(1+\u03b3 2 )(1+K 2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Proof The proof follows that of Theorem 3.2. As shown in the proof of Lemma 32, P(E ) \u2265 1 \u2212 \u03b4, where E is given by (36). Now, observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "We proceed to bound each term separately. Term 1. Suppose E occurs. Using (49) and Lemma 32, we arrive at the following inequality:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "E post ( M \u03b2) \u2264 E post ( M \u03b2) \u2264 k k \u2206 pre + \u2206 HSVT + \u2206 gen + \u2206 model .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "First, note that our assumptions give \u2206 model = 0 and k \u2265 k . We then use (45), coupled with Corollaries 20 and 25, to establish k k \u2206 pre +\u2206 HSVT \u2264 k k 2\u03c3 2 k T 0 + C 1 k \u03c1 4 (N \u2227T ) \u03b2 * 2 1 +",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where C 1 ,C 2 are given by (29). At the same time, following the arguments that led to (46), we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where C 3 = C(1+\u03b3 2 )(1+K 2 ). Therefore, combining and simplifying the above results yield Conclusion. Collecting terms completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Corollary 34 Suppose the conditions of Theorem 3.2 hold with T 0 = \u0398(N ) = \u0398(T ). Then for any \u03b4 > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "where C 2 is given by (29) and C 3 = C(1+\u03c3 2 )(1+\u03b3 2 )(1+K 2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "Proof The result follows immediately after applying Corollary 33 with T 0 = \u0398(N ) = \u0398(T ). Proof Observe that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H.3. Corollaries: Bounds in Expectation"
        },
        {
            "text": "We will now bound each term independently. For the first term, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix I. Hypothesis Test"
        },
        {
            "text": "Further, under H 0 , recall that P V M \u22a5 V M = 0. Therefore, using the isometric property of V M , we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix I. Hypothesis Test"
        },
        {
            "text": "Plugging in (51) and (52) into (50) completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix I. Hypothesis Test"
        },
        {
            "text": "Proof Let us first fix some \u03b1 > 0. Let C = C(1+\u03c3 2 )(1+\u03b3 2 )(1+K 2 ). By Lemma 9, it follows that with probability at least 1\u2212\u03b1, Y \u2212\u03c1M 2 \u2264 C (T 0 +N +log(1/\u03b1)) Y \u2212\u03c1M 2 \u2264 C ((T \u2212T 0 )+N +log(1/\u03b1)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I.1. Proof of Theorem 3.3"
        },
        {
            "text": "Combining the above with Lemma 26 then yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I.1. Proof of Theorem 3.3"
        },
        {
            "text": "Plugging the above into Lemma 35 concludes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I.1. Proof of Theorem 3.3"
        },
        {
            "text": "Lemma 36 (Modified Hoeffding Inequality) Let X \u2208 R n be random vector with independent meanzero sub-Gaussian random coordinates with X i \u03c8 2 \u2264 K. Let a \u2208 R n be another random vector that satisfies a 2 \u2264 b almost surely for some constant b \u2265 0. Then for all t \u2265 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Proof Let S n = n i=1 a i X i . Then applying Markov's inequality for any \u03bb > 0, we obtain P(S n \u2265 t) = P(exp(\u03bbS n ) \u2265 exp(\u03bbt)) where the equality follows from conditional independence, the first inequality by Lemma 3, and the final inequality by assumption. Therefore, P(S n \u2265 t) \u2264 exp CK 2 \u03bb 2 b 2 \u2212\u03bbt .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Optimizing over \u03bb yields the desired result:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Applying the same arguments for \u2212 X,a gives a tail bound in the other direction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Lemma 37 (Modified Hanson-Wright Inequality) Let X \u2208 R n be a random vector with independent mean-zero sub-Gaussian coordinates with X i \u03c8 2 \u2264 K. Let A \u2208 R n\u00d7n be a random matrix satisfying A \u2264 a and A 2 F \u2264 b almost surely for some a,b \u2265 0. Then for any t \u2265 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "where |\u03bb|\u2264 c/(aK 2 ). Therefore, optimizing over \u03bb yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Step 2: off-diagonals. Let S = i =j A ij X i X j . Again, applying Markov's inequality for any \u03bb > 0, we have Let g be a standard multivariate gaussian random vector. Further, let X and g be independent copies of X and g, respectively. Conditioning on A yields E[exp(\u03bbS)] \u2264 E exp 4\u03bbX T AX (by Decoupling Remark 6.1.3 of (24))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "\u2264 E exp C 1 \u03bbg T Ag (by Lemma 6.2.3 of (24)) \u2264 exp C 2 \u03bb 2 A 2 F (by Lemma 6.2.2 of (24))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "where |\u03bb|\u2264 c/a. Optimizing over \u03bb then gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Step 3: combining. Putting everything together completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Lemma 38 (Independent Sub-Gaussian Rows) Let A be an m \u00d7 n matrix whose rows A i are independent, mean zero, sub-gaussian random vectors in R n with second moment matrix \u03a3 = (1/m)E[A T A]. Then for any t \u2265 0, the following inequality holds with probability at least 1 \u2212 exp \u2212t 2 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "1 m A T A\u2212\u03a3 \u2264 K 2 max(\u03b4,\u03b4 2 ), where \u03b4 = C n m + t \u221a m ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "here, K = max i A i \u03c8 2 and C > 0 is an absolute constant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        },
        {
            "text": "Proof The following proof extends the proof of Theorem 4.6.1 of (24) for the non-isotropic setting; we present it here for completeness. Recall that the operator norm of A can computed by maximizing the following quadratic form: A = max x\u2208S n\u22121 ,y\u2208S m\u22121 Ax,y , where S n\u22121 ,S m\u22121 denote the unit spheres in R n and R m , respectively. Rather than searching through the entire unit spheres, we will discretize the spheres using an -net argument to establish a tight control of the quadratic term Ax,y for any pair of fixed unit vectors x,y. Then, we will take a union bound over all x,y in the net.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix J. Helpful Concentration Inequalities"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Google LLC google covid-19 community mobility reports",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2020--2024",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "The economic costs of conflict: A case study of the basque country",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abadie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gardeazabal",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "American Economic Review",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Synthetic control methods for comparative case studies: Estimating the effect of california\u00e2s tobacco control program",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abadie",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Diamond",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hainmueller",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of the American Statistical Association",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Using synthetic controls: Feasibility, data requirements, and methodological aspects (working paper)",
            "authors": [
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Abadie",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "On robustness of principal component regression",
            "authors": [
                {
                    "first": "Anish",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                },
                {
                    "first": "Devavrat",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "Dennis",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Dogyoon",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Robust synthetic control",
            "authors": [
                {
                    "first": "Devavrat",
                    "middle": [],
                    "last": "Muhammad Jehangir Amjad",
                    "suffix": ""
                },
                {
                    "first": "Dennis",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Machine Learning Research",
            "volume": "19",
            "issn": "",
            "pages": "1--51",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "mrsc: Multi-dimensional robust synthetic control",
            "authors": [
                {
                    "first": "Muhummad",
                    "middle": [],
                    "last": "Amjad",
                    "suffix": ""
                },
                {
                    "first": "Vishal",
                    "middle": [],
                    "last": "Mishra",
                    "suffix": ""
                },
                {
                    "first": "Devavrat",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "Dennis",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
            "volume": "3",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "The state of applied econometrics -causality and policy evaluation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Athey",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Imbens",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The Journal of Economic Perspectives",
            "volume": "31",
            "issn": "2",
            "pages": "3--32",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Matrix completion methods for causal panel data models",
            "authors": [
                {
                    "first": "Susan",
                    "middle": [],
                    "last": "Athey",
                    "suffix": ""
                },
                {
                    "first": "Mohsen",
                    "middle": [],
                    "last": "Bayati",
                    "suffix": ""
                },
                {
                    "first": "Nikolay",
                    "middle": [],
                    "last": "Doudchenko",
                    "suffix": ""
                },
                {
                    "first": "Guido",
                    "middle": [],
                    "last": "Imbens",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Prediction by supervised principal components",
            "authors": [
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Bair",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "Debashis",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Journal of the American Statistical Association",
            "volume": "101",
            "issn": "473",
            "pages": "119--137",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Improving immunization coverage through incentives, reminders, and social networks in india",
            "authors": [
                {
                    "first": "Abhijit",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "Arun",
                    "middle": [],
                    "last": "Chandrasekhar",
                    "suffix": ""
                },
                {
                    "first": "Esther",
                    "middle": [],
                    "last": "Duflo",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Floretta",
                    "suffix": ""
                },
                {
                    "first": "Harini",
                    "middle": [],
                    "last": "Kannan",
                    "suffix": ""
                },
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Schrimpf",
                    "suffix": ""
                },
                {
                    "first": "Maheshwor",
                    "middle": [],
                    "last": "Shrestha",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Bayesian pca",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Christopher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bishop",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "382--388",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Recent advances in supervised dimension reduction: A survey",
            "authors": [
                {
                    "first": "Guoqing",
                    "middle": [],
                    "last": "Chao",
                    "suffix": ""
                },
                {
                    "first": "Yuan",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "Weiping",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Machine Learning and Knowledge Extraction",
            "volume": "1",
            "issn": "1",
            "pages": "341--358",
            "other_ids": {
                "DOI": [
                    "10.3390/make1010020"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Cocolasso for high-dimensional error-in-variables regression",
            "authors": [
                {
                    "first": "Abhirup",
                    "middle": [],
                    "last": "Datta",
                    "suffix": ""
                },
                {
                    "first": "Hui",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The Annals of Statistics",
            "volume": "45",
            "issn": "6",
            "pages": "2400--2426",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Causal inference: What if",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ma Hern\u00e1n",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Guido",
                    "suffix": ""
                },
                {
                    "first": "Donald",
                    "middle": [
                        "B"
                    ],
                    "last": "Imbens",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1017/CBO9781139025751"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A note on the use of principal components in regression",
            "authors": [
                {
                    "first": "Ian",
                    "middle": [
                        "T"
                    ],
                    "last": "Jolliffe",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Journal of the Royal Statistical Society",
            "volume": "31",
            "issn": "3",
            "pages": "300--303",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity",
            "authors": [
                {
                    "first": "Po-Ling",
                    "middle": [],
                    "last": "Loh",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [
                        "J"
                    ],
                    "last": "Wainwright",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The Annals of Statistics",
            "volume": "40",
            "issn": "3",
            "pages": "1637--1664",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "High dimensional statistics",
            "authors": [
                {
                    "first": "Philippe",
                    "middle": [],
                    "last": "Rigollet",
                    "suffix": ""
                },
                {
                    "first": "Jan-Christian",
                    "middle": [],
                    "last": "H\u00fctter",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Improved matrix uncertainty selector",
            "authors": [
                {
                    "first": "Mathieu",
                    "middle": [],
                    "last": "Rosenbaum",
                    "suffix": ""
                },
                {
                    "first": "Alexandre",
                    "middle": [
                        "B"
                    ],
                    "last": "Tsybakov",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "From Probability to Statistics and Back: High-Dimensional Models and Processes",
            "volume": "9",
            "issn": "",
            "pages": "276--290",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Learning mixture model with missing values and its application to rankings",
            "authors": [
                {
                    "first": "Devavrat",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "Dogyoon",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1812.11917"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Probabilistic principal component analysis",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tipping",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Christopher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bishop",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "volume": "61",
            "issn": "3",
            "pages": "611--622",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "High-dimensional probability: An introduction with applications in data science",
            "authors": [
                {
                    "first": "Roman",
                    "middle": [],
                    "last": "Vershynin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "47",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Case-fatality estimates for covid-19 calculated by using a lag time for fatality",
            "authors": [
                {
                    "first": "Telfar",
                    "middle": [],
                    "last": "Barnard",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Baker",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kvalsvig",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Emerg Infect Dis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3201/eid2606.200320"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Control (SC), A Partial Solution . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Synthetic Interventions (SI), A Complete Solution . . . . . . . . . . . . . . . . . . 5 1.3 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Existence of SyntheticInterventions . . . . . . . . . . . . . . . . . . . . . . . .. 11 2.3 Perturbed Observations: Error-In-Variable Regression Model . . . . . . . . . . . . 11 3 Main Results 12 3.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Post-Intervention (Test) Prediction Error . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 A Natural, Data-Driven Hypothesis Test . . . . . . . . . . . . . . . . . . . . . . . 14 4 Experiments: Three Case Studies 14 4.1 Experiments Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 Quantifying Counterfactual Prediction Accuracy . . . . . . . . . . . . . . . . . . . 14 4.3 COVID-19: What-if Policy Evaluation via SI . . . . . . . . . . . . . . . . . . . . . 15 4.4 Web A/B Testing: Towards Data Efficient RCTs via SI . . . . . . . . . . . . . . . . 19 4.5 Development Economics: Towards \"Personalized\" RCTs via SI . . . . . . . . . . -Gaussian Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.2 Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.3 Perturbation Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C Existence of an Invariant Linear Model 26 C.1 Proof of Proposition 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2 Proof of Proposition 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D Impact of Measurement Noise and Sparsity 27 D.1 Operator Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.2 2,\u221e -norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E Hard Singular Value Thresholding (HSVT) Estimation Error 32 E.1 A Column Representation for the HSVT Operator . . . . . . . . . . . . . . . . . . 33 E.2 High Probability Bounds on Noise Deviation . . . . . . . . . . . . . . . . . . . . . 34 E.3 Proof of Lemma 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.4 Corollaries: Bounds in Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 39",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Prein Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 42 G Parameter Estimation 43 G.1 Learning Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.2 Bounding the Projected Parameter Estimation Error . . . . . . . . . . . . . . . . . 44 G.3 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 G.4 Synthetic Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H Post-intervention (Test) Prediction Error 48 H.1 Helper Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 H.2 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 H.3 Corollaries: Bounds in Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 53 I Hypothesis Test 55 I.1 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "time periods and D \u2265 1 possible interventions (e.g., different mobility restriction interventions). Unless otherwise stated, we index units with n \u2208 [N ] 1 , time with t \u2208 [T ], and interventions with d \u2208 [D]. Let Y (d)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Observations. We encode our observations into a T \u00d7N matrix Y = [Y tn ] (see Figure 2e), where Y tn = Y (1) tn \u00b7\u03c0 tn , for all t \u2264 T 0 Y (d) tn \u00b7\u03c0 tn , for all t > T 0 , n \u2208 I (d) , d \u2208 [D];",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Algorithm. For any unit i \u2208 [N ] and intervention d \u2208 [D] of interest, let I",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Counterfactual predictions of COVID-19 death counts under different mobility restriction policies.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Pictorial depiction of potential and observed outcomes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": ", represents the potential outcome of unit n at time t under intervention d. Let M (d) \u2208 R T \u00d7N denote the d-th frontal slice of M , which represents the matrix of potential outcomes across all time and units under intervention d (see Figure 2d). Let M (d)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "denote the pre-and post-intervention sub-matrices of M (d) restricted to the donor units in I (d)3 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Low-rank) The canonical polyadic tensor rank of M is r. That is, there exists vectors",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Bounded) The entries of M are bounded by one in absolute value, i.e., M max \u2264 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Well-balanced spectra) For every intervention d, the non-zero singular values s i of M (d) pre are well-balanced, i.e., s 2 i = \u0398(N (d) . Similarly, the non-zero singular values \u03c4 i of M (d) post satisfy \u03c4 2 i = \u0398(N (d) (T \u2212T 0 s 2 i = \u0398(\u03b6) for some \u03b6. Then, Cr",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "= \u0398(N (d) T 0 ) for some constant C. An identical argument applies to M",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "for a canonical probabilistic generating process used to analyze probabilistic PCA in (12; 23). 3. For ease of notation (and wlog), if unit i is the target and receives d, then I (d) := I (d) \\{i}, N (d) := |I (d) \\{i}|.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Suppose Property 2.1 holds. Let the N units be re-indexed as per some permutation chosen uniformly at random. Then for any unit i, Property 2.4 holds w.p. at least 1\u2212r/N (d) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Assume the conditions of Theorem 3.1 hold, rank( M post ) = rank(M post ) = r post ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": ", rank( M pre ) = rank(M pre ) = r pre , and rank( M post ) = rank(M post ) = r post . For any \u03b1 \u2208 (0, 1) and some",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Average reduction in mobility and the assigned intervention group for the 27 countries.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "(a) U.S. under all interventions. (b) Top donor nations for the U.S. (c) U.K. under all interventions. (d) Top donor nations for the U.K.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Validating SI: countries with low mobility restricting interventions. (a) Brazil under all interventions. (b) Top donor nations for Brazil. (c) Turkey under all interventions. (d) Top donor nations for Turkey.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Validating SI: countries with moderate mobility restricting interventions. (a) India under all interventions. (b) Top donor nations for India. (c) Ireland under all interventions. (d) Top donor nations for Ireland.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "Validating SI: countries with severe mobility restricting interventions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "Experimental setup under SI. (b) Experimental setup of e-commerce company.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Experimental setups for A/B testing case study.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": ") as the number of donor units within I (d) .Potential Outcomes. We denote the latent pre-and post-intervention donor matrices as",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "], for all t \u2264 T 0 ,n \u2208 I (d)H = [\u03b5 (d)tn ], for all t > T 0 ,n \u2208 I (d) as the pre-and post-intervention donor noise, respectively.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "We denote the estimates of the latent pre-and post-intervention donor matrices as",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "Weyl's inequality) Let \u03c3 i and \u03c3 i denote the singular values of A and Z, respectively, in decreasing order and repeated by multiplicities. Suppose Z = A+H. Then for all i \u2208 [m\u2227n], |\u03c3 i \u2212 \u03c3 i | \u2264 H . Theorem B.3 (Wedin's generalized sin\u0398 theorem) Let A and Z = A+H be defined as in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF32": {
            "text": "this version of Wedin's theorem, please see Corollary 1.4.10 in (22). Appendix C. Existence of an Invariant Linear Model C.1. Proof of Proposition 2.For all (t,d ) \u2208 [T ]\u00d7[D], we have that",
            "latex": null,
            "type": "figure"
        },
        "FIGREF33": {
            "text": "\u2264 m(\u03c3 2 \u03c1+\u03c1(1\u2212\u03c1))+s with probability at least 1\u2212exp \u2212cmin s 2 K 4 m , s K 2",
            "latex": null,
            "type": "figure"
        },
        "FIGREF34": {
            "text": "3 and E 4 occur with high probability. Lemma 16 Assume Properties 2.2, 2.5, and 2.6 hold. Then for any \u03b4 3 > 0, it follows that P(E c 3 ) \u2264 \u03b4 3 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF35": {
            "text": "Suppose Property 2.4 holds. Consider rank( M ) = rank(M ) = k. Then,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF36": {
            "text": ", and rank( M ) = rank(M ) = k. For any \u03b4 > 0, if \u03c1 \u2265 Clog(1/\u03b4) N T 0 , then the following holds w.p. at least 1\u2212\u03b4:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF37": {
            "text": "Corollaries: Bounds in Expectation Lemma 24 Suppose Property 2.4 holds. Consider rank( M ) = rank(M ) = k. Then, E \u03b5, M ( \u03b2 \u2212\u03b2 * ) \u2264 \u03c3 2 k.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF38": {
            "text": "Interaction between the row and column space of M on any \u03b2 * , and the effect of misaligned subspaces between V M and V on the gap between \u03b2 (which lives in V M ) and P V M \u03b2 * . Lemma 26 Consider rank( M ) = rank(M ) = k. Then,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF39": {
            "text": "Plots of 2 -norm error, i.e., \u03b2 \u2212 \u03b2 * 2 in 9a and \u03b2 \u2212 \u03b2 2 in 9b, versus the rescaled sample size T 0 /(k 2 \u221a logN ) after running PCR with rank k = N1 4 . As predicted by Theorem 3.1, the curves for different values of N under 9a roughly align and decay to zero as T 0 increases.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF40": {
            "text": "By Property 2.2 and (47), it immediately follows that E post ( M \u03b2) \u2264 4.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF41": {
            "text": ". Consider rank( M ) = rank(M ) = k and rank( M ) = rank(M ) = k . Then under H 0 , V M \u2212P V M V",
            "latex": null,
            "type": "figure"
        },
        "FIGREF42": {
            "text": "E[exp(\u03bbS n )]\u00b7exp(\u2212\u03bbt) = E a [E[exp(\u03bbS n ) | a]]\u00b7exp(\u2212\u03bbt).Now, conditioned on the random vector a, observe thatE[exp(\u03bbS n )] = n i=1 E[exp(\u03bba i X i )] \u2264 exp CK 2 \u03bb 2 a 2 2 \u2264 exp CK 2 \u03bb 2 b 2 ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF43": {
            "text": "P(exp(\u03bbS) \u2265 exp(\u03bbt/2)) \u2264 E A [E[exp(\u03bbS) | A]]\u00b7exp(\u2212\u03bbt/2).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Table 1: Comparison with some recent works in the SC literature focused on finite-sample analysis.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison with some notable works in the high-dimensional error-in-variable regression literature.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Proposition 2.1 Suppose Property 2.1 holds. For a given intervention d and unit i, let Property 2.4 hold. Then for all (t,d ) \u2208 [T ]\u00d7[D], we have M",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Hypothesis test and prediction accuracy results for SI in the context of A/B testing.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Hypothesis test and prediction accuracy results for SI in the context of immunization case study for top 1-10 most frequent interventions.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Hypothesis test and prediction accuracy results for SI in the context of immunization case study for top 11-20 most frequent interventions.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "(38)Recalling (33) and applying Lemma 26 yieldsPlugging the above result into (38) and applying Lemma 29 givesTerm 2. Now, to bound the second term of (37), which yieldswhere the final inequality follows from Lemma 26. Conclusion. Collecting the above terms completes the proof. Proof By construction,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}