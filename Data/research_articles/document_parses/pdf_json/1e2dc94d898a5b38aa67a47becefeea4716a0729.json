{"paper_id": "1e2dc94d898a5b38aa67a47becefeea4716a0729", "metadata": {"title": "Different Strategies of Fitting Logistic Regression for Positive and Unlabelled Data", "authors": [{"first": "", "middle": [], "last": "Pawe L Teisseyre", "suffix": "", "affiliation": {"laboratory": "", "institution": "Polish Academy of Sciences", "location": {"settlement": "Warsaw", "country": "Poland"}}, "email": ""}, {"first": "Jan", "middle": [], "last": "Mielniczuk", "suffix": "", "affiliation": {"laboratory": "", "institution": "Polish Academy of Sciences", "location": {"settlement": "Warsaw", "country": "Poland"}}, "email": ""}, {"first": "Ma", "middle": [], "last": "Lgorzata Laz Ecka", "suffix": "", "affiliation": {"laboratory": "", "institution": "Polish Academy of Sciences", "location": {"settlement": "Warsaw", "country": "Poland"}}, "email": "malgorzata.lazecka@ipipan.waw.pl"}]}, "abstract": [{"text": "In the paper we revisit the problem of fitting logistic regression to positive and unlabelled data. There are two key contributions. First, a new light is shed on the properties of frequently used naive method (in which unlabelled examples are treated as negative). In particular we show that naive method is related to incorrect specification of the logistic model and consequently the parameters in naive method are shrunk towards zero. An interesting relationship between shrinkage parameter and label frequency is established. Second, we introduce a novel method of fitting logistic model based on simultaneous estimation of vector of coefficients and label frequency. Importantly, the proposed method does not require prior estimation, which is a major obstacle in positive unlabelled learning. The method is superior in predicting posterior probability to both naive method and weighted likelihood method for several benchmark data sets. Moreover, it yields consistently better estimator of label frequency than other two known methods. We also introduce simple but powerful representation of positive and unlabelled data under Selected Completely at Random assumption which yields straightforwardly most properties of such model.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Learning from positive and unlabelled data (PU learning) has attracted much interest within the machine learning literature as this type of data naturally arises in many applications (see e.g. [1] ). In the case of PU data, we have an access to positive examples and unlabeled examples. Unlabeled examples can be either positive or negative. In this setting the true class label Y \u2208 {0, 1} is not observed directly. We only observe surrogate variable S \u2208 {0, 1}, which indicates whether an example is labeled (and thus positive; S = 1) or unlabeled (S = 0). PU problem naturally occurs in under-reporting [2] which frequently happens in survey data, and it refers to situation when some respondents fail to the answer a question truthfully. For example, imagine that we are interested in predicting an occurrence of some disease (Y = 1 denotes presence of disease and Y = 0 its absence) using some feature vector X. In some cases we only have an access to self-reported data [3] , i.e. respondents answer to the question concerning the occurrence of the disease. Some of them admit to the disease truthfully (S = 1 =\u21d2 Y = 1) and the other group reports no disease (S = 0). The second group consists of respondents who suffer from disease but do not report it (Y = 1, S = 0) and those who really do not have a disease (Y = 0, S = 0). Under-reporting occurs due to a perceived social stigma concerning e.g. alcoholism, HIV disease or socially dangerous behaviours such as talking on the phone frequently while driving. PU data occur frequently in text classification problems [4] [5] [6] . When classifying user's web page preferences, some pages can be bookmarked as positive (S = 1) whereas all other pages are treated as unlabelled (S = 0). Among unlabelled pages, one can find pages that users visit (Y = 1, S = 0) as well as those which are avoided by users (Y = 0, S = 0). The third important example is a problem of disease gene identification which aims to find which genes from the human genome are causative for diseases [7, 8] . In this case all the known disease genes are positive examples (S = 1), while all other candidates, generated by traditional linkage analysis, are unlabelled (S = 0). Several approaches exist to learn with PU data. A simplest approach is to treat S as a class label (this approach is called naive method or non-traditional classification) [9] . To organize terminology, learning with true class label Y will be called oracle method. Although this approach cannot be used in practice, it may serve as a benchmark method with which all considered methods are compared.", "cite_spans": [{"start": 193, "end": 196, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 605, "end": 608, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 975, "end": 978, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 1574, "end": 1577, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 1578, "end": 1581, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 1582, "end": 1585, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 2029, "end": 2032, "text": "[7,", "ref_id": "BIBREF6"}, {"start": 2033, "end": 2035, "text": "8]", "ref_id": "BIBREF7"}, {"start": 2377, "end": 2380, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper we focus on logistic regression. Despite its popularity, there is a lack of thorough analysis of different learning methods based on logistic regression for PU data. We present the following novel contributions. First, we analyse theoretically the naive method and its relationship with oracle method. We show that naive method is related to incorrect specification of the logistic model and we establish the connection between risk minimizers corresponding to naive and oracle methods, for certain relatively large class of distributions. Moreover, we show that parameters in naive method are shrunk towards zero and the amount of shrinkage depends on label frequency c = P (S = 1|Y = 1). Secondly, we propose an intuitive method of parameter estimation in which we simultaneously estimate parameter vector and label frequency c (called joint method hereafter). The method does not require prior estimation which is a difficult task in PU learning [10, 11] . Finally, we compare empirically the proposed method with two existing methods (naive method and the method based on optimizing weighted empirical risk, called briefly weighted method) with respect to estimation errors.", "cite_spans": [{"start": 963, "end": 967, "text": "[10,", "ref_id": "BIBREF9"}, {"start": 968, "end": 971, "text": "11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Introduction"}, {"text": "Finally, the popular taxonomy used in PU learning [1] differentiates between three categories of methods. The first group are postprocessing methods which first use naive method and then modify output probabilities using label frequency [9] . The second group are preprocessing methods that weigh the examples using label frequency [12] [13] [14] . We refer to [1] (Sect. 5.3.2) for a description of general empirical risk minimization framework in which the weights of observations depending on label frequency c, for any loss function are determined. The last group are methods incorporating label frequency into learning algorithms. A representative algorithm from this group is POSC4.5 [15] , which is PU tree learning method. The three methods considered in this paper (naive, weighted and joint method) represent the above three categories, respectively. This paper is organized as follows. In Sect. 2, we state the problem and discuss its variants and assumptions. In Sect. 3, we analyse three learning methods based on logistic regression in detail. Section 4 discusses the relationship between naive and oracle methods. We report the results of experiments in Sect. 5 and conclude the paper in Sect. 6 . Technical details are stated in Sect. 7. Some additional experiments are described in Supplement 1 .", "cite_spans": [{"start": 50, "end": 53, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 237, "end": 240, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 332, "end": 336, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 337, "end": 341, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 342, "end": 346, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 361, "end": 364, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 690, "end": 694, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1209, "end": 1210, "text": "6", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this work we consider single training data (STD) scenario, which can be described as follows. Let X be feature vector, Y \u2208 {0, 1} be a true class label and S \u2208 {0, 1} an indicator of whether an example is labelled (S = 1) or not (S = 0). We assume that there is some unknown distribution P (Y, X, S) such that (y i , x i , s i ), i = 1, . . . , n is iid sample drawn from it and data (x i , s i ), i = 1, . . . , n, is observed. Thus, instead of a sample (x i , y i ) which corresponds to classical classification task, we observe only sample (x i , s i ), where s i depends on (x i , y i ). Note that this is equivalent to X and S being independent given Y (denoted X \u22a5 S|Y ) as P (S = 1|Y = 0, X) = P (S = 1|Y = 0) = 0. Parameter c := P (S = 1|Y = 1) is called label frequency and plays an important role in PU learning. In the paper we introduce a useful representation of variable (X, S) under SCAR assumption. Namely, we show that S can be represented as", "cite_spans": [], "ref_spans": [], "section": "Assumptions and Useful Representation for PU Data"}, {"text": "for a certain 0 < p < 1 and Bern(1, p) stands for Bernoulli distribution. Indeed,", "cite_spans": [], "ref_spans": [], "section": "Assumptions and Useful Representation for PU Data"}, {"text": "Thus probability of success P (\u03b5 = 1) coincides with c. Under SCAR assumption we have", "cite_spans": [], "ref_spans": [], "section": "Assumptions and Useful Representation for PU Data"}, {"text": "[9] and P (X = x|Y = 1) = P (X = x|S = 1). (4) [2] . Properties (2)-(4) are easily derivable when (1) is applied (see Sect. 7).", "cite_spans": [{"start": 47, "end": 50, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Assumptions and Useful Representation for PU Data"}, {"text": "We also note that the assumed STD scenario should be distinguished from case-control scenario when two independent samples are observed: labeled sample consisting of independent observations drawn from distribution of X given Y = 1 and the second drawn from distribution of X. This is carefully discussed in [1] . Both PU scenarios should be also distinguished from semi-supervised scenario when besides fully observable sample from distribution of (X, Y ) we also have at our disposal sample from distribution of X [16] or, in extreme case, we have full knowledge of distribution of X, see [17] and references therein. One of the main goals of PU learning is to estimate the posterior probability f (x) := P (Y = 1|X = x). The problem is discussed in the following sections.", "cite_spans": [{"start": 308, "end": 311, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 516, "end": 520, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 591, "end": 595, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Assumptions and Useful Representation for PU Data"}, {"text": "In this section we present three different methods of estimating f (x) := P (Y = 1|X = x) using logistic loss. When data is fully observed the natural way to learn a model is to consider risk for logistic loss", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "where \u03c3(s) = 1/(1 + exp(\u2212s)) and minimize its empirical version. This will be called oracle method. Note that using logistic loss function in the definition of R(b) above corresponds to fitting logistic regression using Maximum Likelihood (ML) method. Obviously, for PU data, this approach is not feasible as we do not observe Y and inferential procedures have to be based on (S, X). The simplest approach (called naive estimation or non-traditional estimation) is thus to consider risk", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "and the corresponding empirical risk", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "which can be directly optimized. In Sect. 4 we study the relationship between minimizers of R(b) and", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "It turns out that for certain, relatively large, class of distributions of X, b * 1 = \u03b7b * , for some \u03b7 \u2208 R (i.e. b * 1 and b * are collinear). Moreover, when predictors X are normal and when (Y, X) corresponds to logistic model, we establish the relationship between \u03b7 and label frequency c which shows that \u03b7 < 1 and thus naive approach leads to shrinking of vector b * . To estimate the posterior f (x) = P (Y = 1|X = x) using naive estimation, we perform a twostep procedure, i.e. we first estimateb naive = arg min bR1 (b) and then let", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "where unknown c has to be estimated using some external procedure. Note that even when (Y, X) corresponds to logistic regression model, b * and whence posterior probability is not consistently estimated by naive method.", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "The second approach is based on weighted empirical risk minimization. As mentioned before, the empirical counterpart of risk R(b) cannot be directly optimized as we do not observe Y . However it can be shown [1] that", "cite_spans": [{"start": 208, "end": 211, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "The risk above is approximated by", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "This means that all unlabelled examples are assigned weight 1, whereas each labelled example is treated as a combination of positive example with weight 1/c and negative example with weight (1\u22121/c). The posterior estimator is defined as", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "The above idea of weighted empirical risk minimization was used in case-control scenario for which the above formulas have slightly different forms, see [12, 13] .", "cite_spans": [{"start": 153, "end": 157, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 158, "end": 161, "text": "13]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "In the paper we propose a novel, intuitive approach, called joint method (name refers to joint estimation of b and c). In this method we avail ourselves of an important feature of logistic regression, namely that posterior probability is directly parametrized. This in turn allows to directly plug in the equation (2) into the risk function", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "The empirical counterpart of the above risk is", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "The empirical risk R 2 (b, c) can be optimized with respect to b if c is assumed to be known or can be optimized simultaneously with respect to both b and c.", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "In the latter case the posterior estimator isf joint (x) := \u03c3(x Tb joint ) where (b joint ,\u0109 joint ) = arg min b,cR2 (b, c). Note that when conditional distribution of Y given X is governed by logistic model i.e. P (Y = 1|X = x) = \u03c3(\u03b2 T x), for some unknown vector \u03b2, then in view of (2) P (S = 1|X = x) = c\u03c3(\u03b2 T x) and R 2 (b, c) is log-likelihood for observed sample (x i , s i ). Whence under regularity conditions, maximisation of R 2 (b, c) yields consistent estimator of (\u03b2, c) in view of known results in consistency of maximum likelihood method. To optimize function R 2 we use BFGS algorithm, which requires the knowledge of functional form of gradient. The partial derivatives of R 2 are given by", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": ".", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "For c = 1, the first equation above reduces to well-known formula for gradient of the maximum likelihood function for standard logistic regression. In general we observe quick convergence of BFGS algorithm. The proposed method is described by the following scheme.", "cite_spans": [], "ref_spans": [], "section": "Logistic Regression for PU Data"}, {"text": "Finally, we note that the joint method above is loosely related to nonlinear regression fit in dose-response analysis when generalized logistic curve is fitted [18] .", "cite_spans": [{"start": 160, "end": 164, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Algorithm 1. Joint method for posterior estimation"}, {"text": "In this Section we show that naive method is related to incorrect specification of the logistic model and that the corresponding parameter vector will be shrunk towards zero for relatively large class of distributions of X. Moreover, we establish the relationship between the amount of shrinkage and label frequency. Assume for simplicity of exposition that components of X are non-constant random variables (in the case when one of predictors is a dummy variable which allows for the intercept in the model, collinearity in (9) corresponds to vector of predictors with dummy variable omitted) and assume that regression function of Y given X has the following form", "cite_spans": [], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "for a certain response function q taking its values in (0, 1) and a certain \u03b2 \u2208 R p . We note that when oracle method (5) is correctly specified, i.e. q(\u00b7) = \u03c3(\u00b7), then \u03b2 = b * (cf [19] ). Here we consider more general situation in which we may have q(\u00b7) = \u03c3(\u00b7). Under SCAR assumption, P (S = 1|X = x) = cq(\u03b2 T X) and thus when cq(\u00b7) = \u03c3(\u00b7) then maximising R 1 (b) corresponds to fitting misspecified logistic model to (X, S). Importantly, this model is misspecified even if the oracle model is correctly specified. Observe that in this case shrinking of parameters is intuitive as they have to move towards 0 to account for diminished (c < 1) aposteriori probability. We explain in the following why misspecified fit, which occurs frequently in practice may still lead to reasonable results. Assume namely that distribution of X satisfies linear regression condition (LRC)", "cite_spans": [{"start": 181, "end": 185, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "for a certain w 0 , w \u2208 R p . Note that (8) has to be satisfied for a true \u03b2 only. LRC is fulfilled (for all \u03b2) by normal distribution, and more generally, by a larger class of elliptically contoured distributions (multivariate t-Student distribution is a representative example). Then it follows (see e.g. [20] )", "cite_spans": [{"start": 307, "end": 311, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "and \u03b7 = 0 provided Cov(Y, X) = 0. In this case true vector \u03b2 and its projection on a logistic model are collinear which partly explains why logistic classification works even when data does not follow logistic model. When oracle method (5) is correctly specified, i.e. q(\u00b7) = \u03c3(\u00b7), then (9) can be written as", "cite_spans": [], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "i.e. risk minimizers corresponding to naive and oracle methods are collinear. In the following we investigate the relationship between label frequency c and collinearity factor \u03b7. Intuition suggests that small c should result in shrinking of estimators towards zero. First, we have a general formula (see [19] for derivation) describing the relationship between c and \u03b7 when (7) holds", "cite_spans": [{"start": 305, "end": 309, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "for any j, where X j is j-th coordinate of X = (X 1 , . . . , X p ). Unfortunately, the above formula does not yield simple relationship between c and \u03b7. Some additional assumptions are needed to find more revealing one. In the case when X has normal distribution N (0, \u03a3) it follows from [20] together with (2) that the following equality holds", "cite_spans": [{"start": 289, "end": 293, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "where \u03c3 (s) denotes derivative of \u03c3(s) wrt to s. This is easily seen to be a corollary of Stein's lemma stating that Cov(h(Z 1 ), Z 2 ) = Cov(Z 1 , Z 2 )Eh (Z 1 ) for bivariate normal (Z 1 , Z 2 ). Equation (11) can be used to find upper and lower bounds for \u03b7. Namely, we prove the following Theorem.", "cite_spans": [{"start": 207, "end": 211, "text": "(11)", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Naive Method as an Incorrect Specification of Logistic Regression"}, {"text": "Assume that X follows normal distribution N (0, \u03a3) and that linear regression condition holds (8) . Then", "cite_spans": [{"start": 94, "end": 97, "text": "(8)", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Theorem 1."}, {"text": "Note that RHS inequality in (1) yields the lower bound on the amount of shrinkage of true vector \u03b2 * whereas LHS gives a lower bound on this amount.", "cite_spans": [], "ref_spans": [], "section": "Theorem 1."}, {"text": "Proof. Let Z = \u03b2 T X and note that Z has normal distribution N (0, a 2 ) with a 2 = \u03b2 T \u03a3\u03b2. It follows from the fact that \u03c3 (s) = \u03c3(s)(1 \u2212 \u03c3(s)) is nonincreasing for s > 0 that function h(\u03bb) = E\u03c3 (\u03bbZ) is non-increasing. This justifies the last equality on the right as c \u2264 1. Define g(\u03bb) = h(1) \u2212 (\u03bb/c)h(\u03bb) and note that g(0) = h(1) > 0, g(c) \u2264 0 and g is continuous. Thus for a certain \u03bb 0 \u2208 [0, c] it holds that g(\u03bb 0 ) = 0 and it follows from (11) and uniqueness of projection that \u03b7 = \u03bb 0 . In order to prove the RHS inequality it is enough to prove that g(\u03bb) is convex as then \u03bb 0 \u2264 \u03bb * , where \u03bb * is a point at which a line h(1) \u2212 \u03bbh(c)/c joining points (0, g(0)) and (c, g(c)) crosses x-axis. As \u03bb * = (h(1)/h(c))c the inequality follows. Convexity of g follows from concavity of \u03bbh(\u03bb) which is proved in Supplement. In order to prove the left inequality it is enough to observe that \u03c3 (x) \u2264 1/4 and use (11) again.", "cite_spans": [], "ref_spans": [], "section": "Theorem 1."}, {"text": "Note for c \u2192 0 the ratio of the lower and upper bound tends to 1 as E\u03c3 (c\u03b2 T X) \u2192 1/4. To illustrate the above theoretical result we performed simulation experiment in which we artificially generated a sample of size n = 10 6 in such a way that X followed 3-dimensional standard normal distribution and Y was generated from (7) with q(\u00b7) = \u03c3(\u00b7), with known \u03b2. Then Z = \u03b2 T X has N (0, ||\u03b2|| 2 ) distribution and the bounds in (12) depend only on c and ||\u03b2||. Figure 1 shows how collinearity parameter \u03b7 and the corresponding bounds depend on c, for three different norms ||\u03b2||. Note that the bounds become tighter for smaller ||\u03b2|| and smaller c. Secondly, for small c, the lower bound is nearly optimal.", "cite_spans": [], "ref_spans": [{"start": 459, "end": 467, "text": "Figure 1", "ref_id": "FIGREF1"}], "section": "Theorem 1."}, {"text": "We use 9 popular benchmark datasets from UCI repository 2 . To create PU datasets from the completely labelled datasets, we generated 100 PU datasets labelling randomly elements having Y = 1 with probability c and then averaged the results over 100 repetitions. In addition, we consider one artificial dataset having n observations, generated as follows. Feature vector X was drawn from 3-dimensional standard normal distribution and Y was simulated from (7) with q(\u00b7) = \u03c3(\u00b7), with known \u03b2 = (1, 1, 1). This corresponds to correct specification of the oracle method. The observed variable S was labelled as 1 for elements having Y = 1 with probability c. Note however, that in view of discussion in Sect. 4, the naive model is incorrectly specified. Moreover, recall that in this case \u03b2 = b * = arg min R(b). The main advantage of using artificial data is that \u03b2 (and thus also b * ) is known and thus we can analyse the estimation error for the considered methods. For artificial dataset, we experimented with different values of c and n.", "cite_spans": [], "ref_spans": [], "section": "Datasets"}, {"text": "The aim of the experiments is to compare the three methods of learning parameters in logistic regression: naive, weighted and joint. Our implementation of the discussed methods is available at https://github.com/teisseyrep/PUlogistic. Our main goal is to investigate how the considered methods relate to the oracle method, corresponding to idealized situation in which we have an access to Y . In view of this, as an evaluation measure we use approximation error for posterior defined as", "cite_spans": [], "ref_spans": [], "section": "Methods and Evaluation Measures"}, {"text": "where 'method' corresponds to one of the considered methods (naive, weighted or joint), i.e.f naive (x) :=", "cite_spans": [], "ref_spans": [], "section": "Methods and Evaluation Measures"}, {"text": ", whereb oracle is minimizer of empirical version of (5). Estimation error for posterior, defined above, measures how accurate we can approximate the oracle classifier when using S instead of true class label Y . We consider two scenarios. In the first one we assume that c is known and we only estimate parameters corresponding to vector X. This setting corresponds to known prior probability P (Y = 1) (c can be estimated accurately when prior is known via equation c = P (S = 1)/P (Y = 1) by plugging-in corresponding fraction for P (S = 1)). In the second more realistic scenario, c is unknown and is estimated from data. For joint method we jointly minimize empirical risk R 2 (b, c) with respect to b and c. For two remaining methods (naive and weighted) we use external methods of estimation of c. We employ two methods; the first one was proposed by Elkan and Noto [9] (called EN) is based on averaging predictions of naive classifier over labeled examples for validation data. The second method, described in recent paper [11] , is based on optimizing a lower bound of c via top-down decision tree induction (this method will be called TI). In order to analyse prediction performance of the proposed methods, we calculate AUC (Area Under ROC curve) of classifiers based on f method on independent test set.", "cite_spans": [{"start": 873, "end": 876, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 1031, "end": 1035, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Methods and Evaluation Measures"}, {"text": "For artificial datasets, the true parameter \u03b2 is known so we can analyse mean estimation error defined as EE = p \u22121 p j=1 |b j \u2212 \u03b2 j |, whereb corresponds to one of the considered methods. Moreover, we consider an angle between \u03b2 andb. In view of property (9) the angle should be small, for sufficiently large sample size. Finally, let us note, that some real datasets may contain large number of features, so to make the estimation procedures more stable, we first performed feature selection. We used filter method recommended in [21] based on mutual information and select top t = 3, 5, 10 features for each dataset (we present the results for t = 5, the results for other t are similar and are presented in Supplement). This step is common for all considered methods.", "cite_spans": [{"start": 532, "end": 536, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Methods and Evaluation Measures"}, {"text": "First, we analyse how the approximation errors for posterior depend on c, for real datasets (Fig. 2) . We show the results for unknown c, the results for known c are presented in Supplement https://github.com/teisseyrep/PUlogistic. For unknown c, estimation of label frequency plays an important role. We observe that the performance curves vary depending on the method used. For most datasets, TI method outperforms EN, which is consistent with experiments described in [11] , an exception is spambase for which TI works poorly. Importantly, joint method is a clear winner for most of the datasets, what suggests that simultaneous estimation of c and b is more effective than performing these two steps separately. Its superiority is frequently quite dramatic (see diabetes, creditg and spambase). For most datasets, we observe the deterioration in posterior approximation when c becomes smaller. This is concordant with expectations, as for small c, the level of noise in observed variable S increases (cf Eq. (1)) and thus the gap between oracle and naive methods increases. Tables 1 and 2 show values of AUC, for cases of known and unknown c, respectively. The results are averaged over 100 repetitions. In each repetition, we randomly chose c \u2208 (0, 1), then generate PU dataset and finally split it into training and testing subsets. For naive and weighted methods, c is estimated using TI algorithm (the performance for EN algorithm is generally worse and thus not presented in the Table) . The last row contains averaged ranks, the larger the rank for AUC the better. The best method from three (naive, weighted and joint method) is in bold. As expected, the oracle method is an overall winner. The differences between the remaining methods are not very pronounced. Surprisingly, naive and joint methods work in most cases on par, whereas weighted method performs slightly worse. The advantage of joint method is the most pronounced for spambase, for which we also observed superior performance of the joint method wrt approximation error (Fig. 2, bottom panel) . Finally, joint method turns out to be effective for estimating c (Table 3 )-the estimation errors for joint method are smaller than for TI and EN, for almost all datasets. Figures 3 and 4 show results for artificial data, for c = 0.3, 0.6, 0.9, respectively. Mean estimation error converges to zero with sample size for weighted and joint methods (Fig. 3 ) and the convergence for joint method is faster. As expected, the estimation error for naive method is much larger than for joint and weighted methods, which is due to incorrect specification of the logistic regression. Note that weighted and joint methods account for wrong specification and therefore both methods perform better. Next we analysed an angle between true \u03b2 (or equivalently b * ) andb. Although the naive method does not recover the true signal \u03b2, it is able to consistently estimate the direction of \u03b2. Indeed the angle for naive method converges to zero with sample size (Fig. 4) , which is in line with property (9) . Interestingly the speed of converge for weighted method is nearly the same as for naive method, whereas the convergence for joint method is a bit faster.", "cite_spans": [{"start": 471, "end": 475, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 3058, "end": 3061, "text": "(9)", "ref_id": "BIBREF8"}], "ref_spans": [{"start": 92, "end": 100, "text": "(Fig. 2)", "ref_id": "FIGREF2"}, {"start": 1078, "end": 1092, "text": "Tables 1 and 2", "ref_id": "TABREF0"}, {"start": 1488, "end": 1494, "text": "Table)", "ref_id": null}, {"start": 2046, "end": 2068, "text": "(Fig. 2, bottom panel)", "ref_id": "FIGREF2"}, {"start": 2136, "end": 2144, "text": "(Table 3", "ref_id": "TABREF2"}, {"start": 2243, "end": 2258, "text": "Figures 3 and 4", "ref_id": "FIGREF3"}, {"start": 2418, "end": 2425, "text": "(Fig. 3", "ref_id": "FIGREF3"}, {"start": 3016, "end": 3024, "text": "(Fig. 4)", "ref_id": "FIGREF4"}], "section": "Results"}, {"text": "We analysed three different approaches to fitting logistic regression model for PU data. We study theoretically the naive method. Although it does not estimate the true signal \u03b2 consistently, it is able to consistently estimate the direction of \u03b2. This property can be particularly useful in the context of feature selection, where consistent estimation of the direction allows to discover the true significant features -this issue is left for future research. We have shown that under mild assumptions, risk minimizers corresponding to naive and oracle methods are collinear and the collinearity factor \u03b7 is related to label frequency c. Moreover, we proposed novel method that allows to estimate parameter vector and label frequency c simultaneously. The proposed joint method achieves the smallest approximation error, which indicates that it is the closest to the oracle method among considered methods. Secondly, the joint method, unlike weighted and naive methods, does not require using external procedures to estimate c. Importantly, it outperforms the two existing methods (EN and TI) wrt to estimation error for c. In view of above, joint method can be recommended in practice, especially for estimating posterior probability and c; the differences in AUC for classifiers between the considered methods are not very pronounced.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}, {"text": "Equation (2) where the second to last equality follows from P (\u03b5 = 0)/P (\u03b5 = 1) = (1 \u2212 c)/c. To prove (4) we write P (X = x|S = 1) = P (X = x|Y = 1, \u03b5 = 1) = P (X = x|Y = 1).", "cite_spans": [], "ref_spans": [], "section": "Proofs"}, {"text": "The third equality follows from conditional independence of X and \u03b5 given Y .", "cite_spans": [], "ref_spans": [], "section": "Proofs"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Learning from positive and unlabeled data: a survey", "authors": [{"first": "J", "middle": [], "last": "Bekker", "suffix": ""}, {"first": "J", "middle": [], "last": "Davis", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Dealing with under-reported variables: an information theoretic solution", "authors": [{"first": "K", "middle": [], "last": "Sechidis", "suffix": ""}, {"first": "M", "middle": [], "last": "Sperrin", "suffix": ""}, {"first": "E", "middle": ["S"], "last": "Petherick", "suffix": ""}, {"first": "M", "middle": [], "last": "Luj An", "suffix": ""}, {"first": "G", "middle": [], "last": "Brown", "suffix": ""}], "year": 2017, "venue": "Int. J. Approx. Reason", "volume": "85", "issn": "", "pages": "159--177", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "The gap between self-reported and objective measures of disease status in India", "authors": [{"first": "I", "middle": [], "last": "Onur", "suffix": ""}, {"first": "M", "middle": [], "last": "Velamuri", "suffix": ""}], "year": 2018, "venue": "PLOS ONE", "volume": "13", "issn": "8", "pages": "1--18", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Building text classifiers using positive and unlabeled examples", "authors": [{"first": "B", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Dai", "suffix": ""}, {"first": "X", "middle": [], "last": "Li", "suffix": ""}, {"first": "W", "middle": ["S"], "last": "Lee", "suffix": ""}, {"first": "P", "middle": ["S"], "last": "Yu", "suffix": ""}], "year": 2003, "venue": "Proceedings of the Third IEEE International Conference on Data Mining, ICDM 2003", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Text classification without negative examples revisit", "authors": [{"first": "G", "middle": ["P C"], "last": "Fung", "suffix": ""}, {"first": "J", "middle": ["X"], "last": "Yu", "suffix": ""}, {"first": "H", "middle": [], "last": "Lu", "suffix": ""}, {"first": "P", "middle": ["S"], "last": "Yu", "suffix": ""}], "year": 2006, "venue": "IEEE Trans. Knowl. Data Eng", "volume": "18", "issn": "1", "pages": "6--20", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Learning to classify texts using positive and unlabeled data", "authors": [{"first": "X", "middle": [], "last": "Li", "suffix": ""}, {"first": "B", "middle": [], "last": "Liu", "suffix": ""}], "year": 2003, "venue": "Proceedings of the 18th International Joint Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "587--592", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "ProDiGe: prioritization of disease genes with multitask machine learning from positive and unlabeled examples", "authors": [{"first": "F", "middle": [], "last": "Mordelet", "suffix": ""}, {"first": "J.-P", "middle": [], "last": "Vert", "suffix": ""}], "year": 2011, "venue": "BMC Bioinformatics", "volume": "12", "issn": "1", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Learning gene regulatory networks from only positive and unlabeled data", "authors": [{"first": "L", "middle": [], "last": "Cerulo", "suffix": ""}, {"first": "C", "middle": [], "last": "Elkan", "suffix": ""}, {"first": "M", "middle": [], "last": "Ceccarelli", "suffix": ""}], "year": 2010, "venue": "BMC Bioinformatics", "volume": "11", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Learning classifiers from only positive and unlabeled data", "authors": [{"first": "C", "middle": [], "last": "Elkan", "suffix": ""}, {"first": "K", "middle": [], "last": "Noto", "suffix": ""}], "year": 2008, "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "volume": "", "issn": "", "pages": "213--220", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Class-prior estimation for learning from positive and unlabeled data", "authors": [{"first": "M", "middle": ["C"], "last": "Du Plessis", "suffix": ""}, {"first": "G", "middle": [], "last": "Niu", "suffix": ""}, {"first": "M", "middle": [], "last": "Sugiyama", "suffix": ""}], "year": 2016, "venue": "Mach. Learn", "volume": "106", "issn": "4", "pages": "463--492", "other_ids": {"DOI": ["10.1007/s10994-016-5604-6"]}}, "BIBREF10": {"ref_id": "b10", "title": "Estimating the class prior in positive and unlabeled data through decision tree induction", "authors": [{"first": "J", "middle": [], "last": "Bekker", "suffix": ""}, {"first": "J", "middle": [], "last": "Davis", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 32th AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Estimating logistic regression models when the dependent variable has no variance", "authors": [{"first": "D", "middle": [], "last": "Steinberg", "suffix": ""}, {"first": "N", "middle": ["S"], "last": "Cardell", "suffix": ""}], "year": 1992, "venue": "Commun. Stat. Theory Methods", "volume": "21", "issn": "2", "pages": "423--450", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Case-control studies with contaminated controls", "authors": [{"first": "T", "middle": [], "last": "Lancaster", "suffix": ""}, {"first": "G", "middle": [], "last": "Imbens", "suffix": ""}], "year": 1996, "venue": "J. Econom", "volume": "71", "issn": "1", "pages": "145--160", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Positive-unlabeled learning with non-negative risk estimator", "authors": [{"first": "R", "middle": [], "last": "Kiryo", "suffix": ""}, {"first": "G", "middle": [], "last": "Niu", "suffix": ""}, {"first": "M", "middle": ["C"], "last": "Du Plessis", "suffix": ""}, {"first": "M", "middle": [], "last": "Sugiyama", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1674--1684", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Learning from positive and unlabeled examples", "authors": [{"first": "F", "middle": [], "last": "Denis", "suffix": ""}, {"first": "R", "middle": [], "last": "Gilleron", "suffix": ""}, {"first": "F", "middle": [], "last": "Letouzey", "suffix": ""}], "year": 2005, "venue": "Theoret. Comput. Sci", "volume": "348", "issn": "1", "pages": "70--83", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Semi-Supervised Learning", "authors": [{"first": "O", "middle": [], "last": "Chapelle", "suffix": ""}, {"first": "B", "middle": [], "last": "Sch\u00f6lkopf", "suffix": ""}, {"first": "A", "middle": [], "last": "Zien", "suffix": ""}], "year": 2010, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Panning for gold: model-x knockoffs for high-dimensional controlled variable selection", "authors": [{"first": "E", "middle": [], "last": "Cand\u00e8s", "suffix": ""}, {"first": "Y", "middle": [], "last": "Fan", "suffix": ""}, {"first": "L", "middle": [], "last": "Janson", "suffix": ""}, {"first": "J", "middle": [], "last": "Lv", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "The five-parameter logistic: a characterization and comparison with the four-parameter logistic", "authors": [{"first": "P", "middle": ["G"], "last": "Gottschalk", "suffix": ""}, {"first": "J", "middle": ["R"], "last": "Dunn", "suffix": ""}], "year": 2005, "venue": "Anal. Biochem", "volume": "343", "issn": "1", "pages": "54--65", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "What do we choose when we err? Model selection and testing for misspecified logistic regression revisited", "authors": [{"first": "J", "middle": [], "last": "Mielniczuk", "suffix": ""}, {"first": "P", "middle": [], "last": "Teisseyre", "suffix": ""}], "year": 2016, "venue": "Challenges in Computational Statistics and Data Mining. SCI", "volume": "605", "issn": "", "pages": "271--296", "other_ids": {"DOI": ["10.1007/978-3-319-18781-5_15"]}}, "BIBREF19": {"ref_id": "b19", "title": "Active set of predictors for misspecified logistic regression", "authors": [{"first": "M", "middle": [], "last": "Kubkowski", "suffix": ""}, {"first": "J", "middle": [], "last": "Mielniczuk", "suffix": ""}], "year": 2017, "venue": "Statistics", "volume": "51", "issn": "", "pages": "1023--1045", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Simple strategies for semi-supervised feature selection", "authors": [{"first": "K", "middle": [], "last": "Sechidis", "suffix": ""}, {"first": "G", "middle": [], "last": "Brown", "suffix": ""}], "year": 2017, "venue": "Mach. Learn", "volume": "107", "issn": "2", "pages": "357--395", "other_ids": {"DOI": ["10.1007/s10994-017-5648-2"]}}}, "ref_entries": {"FIGREF0": {"text": "Only positive examples (Y = 1) can be labelled, i.e. P (S = 1|X, Y = 0) = 0. The true class label is observed only partially, i.e. when S = 1 we know that Y = 1, but when S = 0, then Y can be either 1 or 0. A commonly used assumption is SCAR (Selected Completely At Random) assumption which states that labelled examples are selected randomly from a set of positives examples, independently from X, i.e. P (S = 1|Y = 1, X) = P (S = 1|Y = 1).", "latex": null, "type": "figure"}, "FIGREF1": {"text": "the positive examples are selected to be labelled with label frequencies c = 0.1, 0.2, . . . , 0.9. For each label frequency c Shrinkage parameter \u03b7 wrt c for simulated dataset for n = 10 6 .", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Approximation error for posterior wrt to c, for estimated c.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Mean absolute error p \u22121 p j=1 |bj \u2212\u03b2| wrt to sample size n, whereb corresponds to one of the methods: naive, weighted and joint method.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Degree of angle between \u03b2 andb wrt to sample size n, whereb corresponds to one of the methods: naive, weighted and joint.", "latex": null, "type": "figure"}, "FIGREF5": {"text": "follows fromP (S = 1|X = x) = P (Y \u03b5 = 1|X = x) = P (Y = 1, \u03b5 = 1|X = x) = P (Y = 1|X = x)P (\u03b5 = 1|X = x) = P (Y = 1|X = x)P (\u03b5 = 1) = P (Y = 1|X = x)P (S = 1|Y = 1).The third equality follows from conditional independence of Y and \u03b5 given X. To prove (3), note that P (Y = 1|S = 0, X) can be written as P (Y = 1, \u03b5 = 0, X) P (S = 0, X)", "latex": null, "type": "figure"}, "TABREF0": {"text": "AUC, known c", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Oracle Joint </td><td>Naive Weighted\n</td></tr><tr><td>Breastc </td><td>0.993 </td><td>0.981 </td><td>0.987 </td><td>0.974\n</td></tr><tr><td>Diabetes </td><td>0.821 </td><td>0.805 </td><td>0.808 </td><td>0.805\n</td></tr><tr><td>Heart-c </td><td>0.879 </td><td>0.847 </td><td>0.849 </td><td>0.850\n</td></tr><tr><td>Credit-a </td><td>0.914 </td><td>0.875 </td><td>0.899 </td><td>0.891\n</td></tr><tr><td>Credit-g </td><td>0.740 </td><td>0.726 </td><td>0.727 </td><td>0.725\n</td></tr><tr><td>Adult </td><td>0.874 </td><td>0.874 </td><td>0.869 </td><td>0.874\n</td></tr><tr><td>Vote </td><td>0.973 </td><td>0.974 </td><td>0.968 </td><td>0.970\n</td></tr><tr><td>Wdbc </td><td>0.987 </td><td>0.981 </td><td>0.971 </td><td>0.970\n</td></tr><tr><td>Spambase </td><td>0.911 </td><td>0.914 </td><td>0.892 </td><td>0.899\n</td></tr><tr><td>Rank </td><td>3.8 </td><td>2.4 </td><td>2.1 </td><td>1.7\n</td></tr></table></body></html>"}, "TABREF1": {"text": "AUC (est. c)", "latex": null, "type": "table"}, "TABREF2": {"text": "|c \u2212\u0109|", "latex": null, "type": "table"}}, "back_matter": []}