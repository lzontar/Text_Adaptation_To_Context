{"paper_id": "01f3ed13bbbfbedcde039fb2b187d6760fa2b16e", "metadata": {"title": "Adaptive Forgetting Curves for Spaced Repetition Language Learning", "authors": [{"first": "Ahmed", "middle": [], "last": "Zaidi", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Cambridge", "location": {"addrLine": "15 JJ Thomson Avenue", "postCode": "CB3 0FD", "settlement": "Cambridge", "country": "UK"}}, "email": ""}, {"first": "Andrew", "middle": [], "last": "Caines", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Cambridge", "location": {"addrLine": "15 JJ Thomson Avenue", "postCode": "CB3 0FD", "settlement": "Cambridge", "country": "UK"}}, "email": ""}, {"first": "Russell", "middle": [], "last": "Moore", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Cambridge", "location": {"addrLine": "15 JJ Thomson Avenue", "postCode": "CB3 0FD", "settlement": "Cambridge", "country": "UK"}}, "email": ""}, {"first": "Paula", "middle": [], "last": "Buttery", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Cambridge", "location": {"addrLine": "15 JJ Thomson Avenue", "postCode": "CB3 0FD", "settlement": "Cambridge", "country": "UK"}}, "email": ""}, {"first": "Andrew", "middle": [], "last": "Rice", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Cambridge", "location": {"addrLine": "15 JJ Thomson Avenue", "postCode": "CB3 0FD", "settlement": "Cambridge", "country": "UK"}}, "email": ""}]}, "abstract": [{"text": "The forgetting curve has been extensively explored by psychologists, educationalists and cognitive scientists alike. In the context of Intelligent Tutoring Systems, modelling the forgetting curve for each user and knowledge component (e.g. vocabulary word) should enable us to develop optimal revision strategies that counteract memory decay and ensure long-term retention. In this study we explore a variety of forgetting curve models incorporating psychological and linguistic features, and we use these models to predict the probability of word recall by learners of English as a second language. We evaluate the impact of the models and their features using data from an online vocabulary teaching platform and find that word complexity is a highly informative feature which may be successfully learned by a neural network model.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Optimal human learning techniques have been extensively studied by researchers in psychology [4] and computer science [8, 16, 19, 20] . The impact of learning techniques can be measured by how they affect the long-term retention of the learning materials. Measuring retention requires a model of the human forgetting curve, which plots the probability of recall over time. The first version of the forgetting curve was defined by Ebbinghaus [5] but has since been developed further by many researchers who have incorporated additional psychologically grounded variations to the model [3, 9, 13, 14, 17] . The ideal forgetting curve should adapt to learning materials as well as user meta-features (including current ability). In this study we examine the task of vocabulary learning. We investigate a range of linguistically motivated features, meta-features, and a variety of models in order to predict the probability a given learner will correctly recall a particular word. ", "cite_spans": [{"start": 93, "end": 96, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 118, "end": 121, "text": "[8,", "ref_id": "BIBREF7"}, {"start": 122, "end": 125, "text": "16,", "ref_id": "BIBREF15"}, {"start": 126, "end": 129, "text": "19,", "ref_id": "BIBREF18"}, {"start": 130, "end": 133, "text": "20]", "ref_id": "BIBREF19"}, {"start": 441, "end": 444, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 584, "end": 587, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 588, "end": 590, "text": "9,", "ref_id": "BIBREF8"}, {"start": 591, "end": 594, "text": "13,", "ref_id": "BIBREF12"}, {"start": 595, "end": 598, "text": "14,", "ref_id": "BIBREF13"}, {"start": 599, "end": 602, "text": "17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Introduction"}, {"text": "We use the Duolingo spaced repetition dataset [15] in order to train and evaluate our features and variety of models. The dataset is filtered for English language learners which results in approximately 4.28 million learner-word datapoints. Our models are a modification of the half-life regression model proposed by Settles and Meeder [16] .", "cite_spans": [{"start": 46, "end": 50, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 336, "end": 340, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Method"}, {"text": "The half-life regression model is defined as follows:", "cite_spans": [], "ref_spans": [], "section": "Half-Life Regression (HLR)"}, {"text": "where p is the probability of recall, \u0394 is the time since last seen (days) and h is the half-life or strength of the learner's memory. We denote the estimated half-life by\u0125 \u0398 , and it is defined as:", "cite_spans": [], "ref_spans": [], "section": "Half-Life Regression (HLR)"}, {"text": "where \u0398 is a vector of weights for the features x. The features of the model are made up of lexeme tags, one tag for each word in the vocabulary (e.g. the lexeme tag for word camera is camera.N.SG). The aim of these features is to capture the inherent difficulty of the word. The HLR model is trained using the following loss function:", "cite_spans": [], "ref_spans": [], "section": "Half-Life Regression (HLR)"}, {"text": "In practice, it was found that optimising for both p and h in the loss function improved the model. The true value of h is defined as h = \u2212\u0394 log(p) . p andp \u0398 are the true probability and model estimated probability of recall, respectively.", "cite_spans": [], "ref_spans": [], "section": "Half-Life Regression (HLR)"}, {"text": "We now expand on the HLR model by adding additional linguistic, psychological and meta-features to x. We refer to this model as HLR+. The features include word complexity scores estimated by a pre-trained model [6] , mean concreteness scores and percent known based on human judgements [2] , SUBTLEX word frequencies [18] and user ids.", "cite_spans": [{"start": 211, "end": 214, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 286, "end": 289, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 317, "end": 321, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "HLR with Linguistic/Psychological Features (HLR+)"}, {"text": "The motivation for including complexity as a feature is based on the intuition that the more complex the word, the harder it is to remember. Concreteness is included based on previous work showing that concrete words are easier to remember than abstract words because they activate perceptual memory codes in addition to verbal codes [10] . SUBTLEX is the relative frequency of an English word based on a corpus of 201.3 million words: we hypothesise that more frequent words are more likely to be encountered and reinforced during the time since last seen \u0394. Similarly, we expect that 'percent known' (the proportion of respondents familiar with each word based on survey data) will correlate with probability of recall. Lastly, we include user id to capture latent behavioural aspects about the learners.", "cite_spans": [{"start": 334, "end": 338, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "HLR with Linguistic/Psychological Features (HLR+)"}, {"text": "In addition to adding new features, we now describe a new model that modifies the p such that it directly incorporates word complexity. Gooding et al. [6] derived word complexity to express perceived difficulty. We hypothesise that this will correlate with probability of recall. As the complexity of the word rises, the forgetting curve will become steeper. Therefore, the new model is as follows:", "cite_spans": [{"start": 151, "end": 154, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Complexity-Based Half-Life Regression (C-HLR+)"}, {"text": "where C is the mean complexity for word i. We define estimated half-life\u0125 \u0398 as 2 \u0398\u00b7x where x is a vector composed of all of the features described in Sect. 2.2.", "cite_spans": [], "ref_spans": [], "section": "Complexity-Based Half-Life Regression (C-HLR+)"}, {"text": "Motivated by the recent success of neural networks, we now describe the N-HLR+ model which replaces\u0125 \u0398 = 2 \u0398\u00b7x with a neural network. The network can be described as follows:\u0125", "cite_spans": [], "ref_spans": [], "section": "Neural Half-Life Regression (N-HLR+)"}, {"text": "where the network contains a single hidden layer. x is a vector of input features, w 1 is the weight matrix between the inputs and the hidden layer and w 2 is the weight matrix between the hidden layer and the output. We use the same loss function as HLR which optimises for both p and h.", "cite_spans": [], "ref_spans": [], "section": "Neural Half-Life Regression (N-HLR+)"}, {"text": "We use mean absolute error (MAE) of probability of recall for a lexical item as our evaluation metric which, despite some known problems [11] , is in line with previous work [16] . MAE is defined as: 1", "cite_spans": [{"start": 137, "end": 141, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 174, "end": 178, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Evaluation and Implementation"}, {"text": "We divided the Duolingo English data into 90% training and 10% test. We trained all non-neural models (e.g. HLR, HLR+, C-HLR) using the following parameters which were tuned on the first 500k data points-learning rate: 0.001, alpha \u03b1: 0.01, \u03bb: 0.1. For all neural models (e.g. N-HLR), we used-learning rate: 0.001, epochs: 200, hidden dim: 4. ", "cite_spans": [], "ref_spans": [], "section": "Evaluation and Implementation"}, {"text": "Pimsleur [12] 0.396 Leitner [7] 0.214 Logistic Regression 0.196 HLR [16] 0.195 HLR-lex [16] 0.130", "cite_spans": [{"start": 9, "end": 13, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 28, "end": 31, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 68, "end": 72, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 87, "end": 91, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Model MAE\u2193"}, {"text": "Model MAE\u2193 HLR+ 0.129 C-HLR+ 0.109 N-HLR+ 0.105 CN-HLR+ 0.105", "cite_spans": [], "ref_spans": [], "section": "Model MAE\u2193"}, {"text": "We can see in Table 1 that HLR+ did not perform much better than HLR. By modifying the loss function to include complexity as a parameter in the C-HLR+ model, we considerably improved the performance of our model. This was in line with our hypothesis that more complex words are forgotten faster and thus are an important feature in modelling the forgetting curve.", "cite_spans": [], "ref_spans": [{"start": 14, "end": 21, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Results and Discussion"}, {"text": "The N-HLR+ model provided additional improvements to the C-HLR+ model. This is due to the fact that neural models are better at capturing nonlinearities between the features and the expected output. Furthermore, when compared to the N-HLR+ model we can see that including complexity into the loss function (CN-HLR+) provides no clear improvements in performance. This is because the model learns to place more importance on the complexity feature. We confirm this by analysing the average weights in the hidden layer of the model. The model learns to give greater importance to word complexity, percent known, and concreteness respectively. It does not however, learn much from the user id and SUBTLEX. This is probably due to the fact that a single dimension for capturing user behaviour is not sufficient and that SUBTLEX does not adequately represent learners' experience with English as a second language.", "cite_spans": [], "ref_spans": [], "section": "Results and Discussion"}, {"text": "We present a new model for adaptively learning a forgetting curve for language learning using a modified HLR loss function and a neural network. We incorporate linguistically and psychologically motivated features and show that word complexity is an important feature in predicting probability of recall for a vocabulary item. Furthermore, we illustrate that neural networks can capture the importance of word complexity while a simple HLR fails to take advantage of that signal. This work lays the foundation for work in neural approaches to understanding language learning over time. Future work in this area includes incorporating high-dimensional user embeddings to capture user specific signals that might influence the forgetting curve, and also different models such as Pareto and power functions which have been proposed in prior work [1] .", "cite_spans": [{"start": 843, "end": 846, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "The form of the forgetting curve and the fate of memories", "authors": [{"first": "L", "middle": [], "last": "Averell", "suffix": ""}, {"first": "A", "middle": [], "last": "Heathcote", "suffix": ""}], "year": 2011, "venue": "J. Math. Psychol", "volume": "55", "issn": "", "pages": "25--35", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Concreteness ratings for 40 thousand generally known English word lemmas", "authors": [{"first": "M", "middle": [], "last": "Brysbaert", "suffix": ""}, {"first": "A", "middle": ["B"], "last": "Warriner", "suffix": ""}, {"first": "V", "middle": [], "last": "Kuperman", "suffix": ""}], "year": 2013, "venue": "Behav. Res. Methods", "volume": "46", "issn": "3", "pages": "904--911", "other_ids": {"DOI": ["10.3758/s13428-013-0403-5"]}}, "BIBREF2": {"ref_id": "b2", "title": "DAS3H: modeling student learning and forgetting for optimally scheduling distributed practice of skills", "authors": [{"first": "B", "middle": [], "last": "Choffin", "suffix": ""}, {"first": "F", "middle": [], "last": "Popineau", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bourda", "suffix": ""}, {"first": "J", "middle": [], "last": "Vie", "suffix": ""}], "year": 2019, "venue": "Proceedings of The 12th International Conference on Educational Data Mining (EDM)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Improving students' learning with effective learning techniques: promising directions from cognitive and educational psychology", "authors": [{"first": "J", "middle": [], "last": "Dunlosky", "suffix": ""}, {"first": "K", "middle": ["A"], "last": "Rawson", "suffix": ""}, {"first": "E", "middle": ["J"], "last": "Marsh", "suffix": ""}, {"first": "M", "middle": ["J"], "last": "Nathan", "suffix": ""}, {"first": "D", "middle": ["T"], "last": "Willingham", "suffix": ""}], "year": 2013, "venue": "Psychol. Sci. Public Interest", "volume": "14", "issn": "1", "pages": "4--58", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Ueber das ged\u00e4chtnis", "authors": [{"first": "H", "middle": [], "last": "Ebbinghaus", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Complex word identification as a sequence labelling task", "authors": [{"first": "S", "middle": [], "last": "Gooding", "suffix": ""}, {"first": "E", "middle": [], "last": "Kochmar", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "1148--1153", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "So lernt man lernen: angewandte Lernpsychologie-ein Weg zum Erfolg", "authors": [{"first": "S", "middle": [], "last": "Leitner", "suffix": ""}], "year": 1972, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Skills embeddings: a neural approach to multicomponent representations of students and tasks", "authors": [{"first": "R", "middle": [], "last": "Moore", "suffix": ""}, {"first": "A", "middle": [], "last": "Caines", "suffix": ""}, {"first": "M", "middle": [], "last": "Elliott", "suffix": ""}, {"first": "A", "middle": [], "last": "Zaidi", "suffix": ""}, {"first": "A", "middle": [], "last": "Rice", "suffix": ""}, {"first": "P", "middle": [], "last": "Buttery", "suffix": ""}], "year": 2019, "venue": "Proceedings of The 12th International Conference on Educational Data Mining (EDM)", "volume": "360", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Artificial intelligence to support human instruction", "authors": [{"first": "M", "middle": ["C"], "last": "Mozer", "suffix": ""}, {"first": "M", "middle": [], "last": "Wiseheart", "suffix": ""}, {"first": "T", "middle": ["P"], "last": "Novikoff", "suffix": ""}], "year": 2019, "venue": "Proc. Natl. Acad. Sci", "volume": "116", "issn": "", "pages": "3953--3955", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Imagery and Verbal Processes", "authors": [{"first": "A", "middle": [], "last": "Paivio", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Metrics for evaluation of student models", "authors": [{"first": "R", "middle": [], "last": "Pel\u00e1nek", "suffix": ""}], "year": 2015, "venue": "J. Educ. Data Min", "volume": "7", "issn": "2", "pages": "1--19", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "A memory schedule. Modern Lang", "authors": [{"first": "P", "middle": [], "last": "Pimsleur", "suffix": ""}], "year": 1967, "venue": "J", "volume": "51", "issn": "2", "pages": "73--75", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Accelerating human learning with deep reinforcement learning", "authors": [{"first": "S", "middle": [], "last": "Reddy", "suffix": ""}, {"first": "S", "middle": [], "last": "Levine", "suffix": ""}, {"first": "A", "middle": [], "last": "Dragan", "suffix": ""}], "year": 2017, "venue": "NeurIPS Workshop: Teaching Machines, Robots, and Humans", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "One hundred years of forgetting: a quantitative description of retention", "authors": [{"first": "D", "middle": ["C"], "last": "Rubin", "suffix": ""}, {"first": "A", "middle": ["E"], "last": "Wenzel", "suffix": ""}], "year": 1996, "venue": "Psychol. Rev", "volume": "103", "issn": "4", "pages": "", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Replication data for: a trainable spaced repetition model for language learning", "authors": [{"first": "B", "middle": [], "last": "Settles", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.7910/DVN/N8XJME"]}}, "BIBREF15": {"ref_id": "b15", "title": "A trainable spaced repetition model for language learning", "authors": [{"first": "B", "middle": [], "last": "Settles", "suffix": ""}, {"first": "B", "middle": [], "last": "Meeder", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "1", "issn": "", "pages": "1848--1858", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Enhancing human learning via spaced repetition optimization", "authors": [{"first": "B", "middle": [], "last": "Tabibian", "suffix": ""}, {"first": "U", "middle": [], "last": "Upadhyay", "suffix": ""}, {"first": "A", "middle": [], "last": "De", "suffix": ""}, {"first": "A", "middle": [], "last": "Zarezade", "suffix": ""}, {"first": "B", "middle": [], "last": "Sch\u00f6lkopf", "suffix": ""}, {"first": "M", "middle": [], "last": "Gomez-Rodriguez", "suffix": ""}], "year": 2019, "venue": "Proc. Natl. Acad. Sci", "volume": "116", "issn": "10", "pages": "3988--3993", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "SUBTLEX-UK: a new and improved word frequency database for British English", "authors": [{"first": "W", "middle": ["J"], "last": "Van Heuven", "suffix": ""}, {"first": "P", "middle": [], "last": "Mandera", "suffix": ""}, {"first": "E", "middle": [], "last": "Keuleers", "suffix": ""}, {"first": "M", "middle": [], "last": "Brysbaert", "suffix": ""}], "year": 2014, "venue": "Q. J. Exp. Psychol", "volume": "67", "issn": "6", "pages": "1176--1190", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Accurate modelling of language learning tasks and students using representations of grammatical proficiency", "authors": [{"first": "A", "middle": ["H"], "last": "Zaidi", "suffix": ""}, {"first": "A", "middle": [], "last": "Caines", "suffix": ""}, {"first": "C", "middle": [], "last": "Davis", "suffix": ""}, {"first": "R", "middle": [], "last": "Moore", "suffix": ""}, {"first": "P", "middle": [], "last": "Buttery", "suffix": ""}, {"first": "A", "middle": [], "last": "Rice", "suffix": ""}], "year": 2019, "venue": "Proceedings of The 12th International Conference on Educational Data Mining (EDM)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Curriculum Q-learning for visual vocabulary acquisition", "authors": [{"first": "A", "middle": ["H"], "last": "Zaidi", "suffix": ""}, {"first": "R", "middle": [], "last": "Moore", "suffix": ""}, {"first": "T", "middle": [], "last": "Briscoe", "suffix": ""}], "year": 2017, "venue": "Proceedings of Visually-Grounded Interaction and Language (ViGIL). NeurIPS", "volume": "", "issn": "", "pages": "", "other_ids": {}}}, "ref_entries": {"TABREF0": {"text": "c Springer Nature Switzerland AG 2020 I. I. Bittencourt et al. (Eds.): AIED 2020, LNAI 12164, pp. 358-363, 2020. https://doi.org/10.1007/978-3-030-52240-7_65", "latex": null, "type": "table"}, "TABREF1": {"text": "Evaluation of forgetting curve models. Pimsleur and Leitner are previous methods of modelling the forgetting curve.", "latex": null, "type": "table"}}, "back_matter": [{"text": "Acknowledgements. This paper reports on research supported by Cambridge Assessment, University of Cambridge.", "cite_spans": [], "ref_spans": [], "section": "acknowledgement"}]}