{"paper_id": "2b98c25235193717866dcb7aad2a401a7a943283", "metadata": {"title": "Joint Relational Dependency Learning for Sequential Recommendation", "authors": [{"first": "Xiangmeng", "middle": [], "last": "Wang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Shanghai University", "location": {"postCode": "200444", "settlement": "Shanghai", "country": "China"}}, "email": ""}, {"first": "Qian", "middle": [], "last": "Li", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Technology Sydney", "location": {"settlement": "Sydney", "country": "Australia"}}, "email": "qian.li@uts.edu.au"}, {"first": "Wu", "middle": [], "last": "Zhang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Shanghai University", "location": {"postCode": "200444", "settlement": "Shanghai", "country": "China"}}, "email": "wzhang@shu.edu.cn"}, {"first": "Guandong", "middle": [], "last": "Xu", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Technology Sydney", "location": {"settlement": "Sydney", "country": "Australia"}}, "email": "guandong.xu@uts.edu.au"}, {"first": "Shaowu", "middle": [], "last": "Liu", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of Technology Sydney", "location": {"settlement": "Sydney", "country": "Australia"}}, "email": "shaowu.liu@uts.edu.au"}, {"first": "Wenhao", "middle": [], "last": "Zhu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Shanghai University", "location": {"postCode": "200444", "settlement": "Shanghai", "country": "China"}}, "email": "whzhu@shu.edu.cn"}]}, "abstract": [{"text": "Sequential recommendation leverages the temporal information of users' transactions as transition dependencies for better inferring user preference, which has become increasingly popular in academic research and practical applications. Short-term transition dependencies contain the information of partial item orders, while long-term transition dependencies infer long-range user preference, the two dependencies are mutually restrictive and complementary. Although some work investigates unifying both long-term and short-term dependencies for better performance, they still neglect the fact that short-term interactions are multi-folds, which are either individual-level interactions or unionlevel interactions. Existing sequential recommendations mainly focus on user's individual (i.e., individual-level) interactions but ignore the important collective influence at union-level. Since union-level interactions can reflect that human decisions are made based on multiple items he/she has already interacted, ignoring such interactions can result in the disability of capturing the collective influence between items. To alleviate this issue, we proposed a Joint Relational Dependency learning (JRD-L) for sequential recommendation that exploits both long-term and short-term preferences at individual-level and union-level. Specifically, JRD-L combines long-term user preferences with short-term interests by measuring shortterm pair relations at individual-level and union-level. Moreover, JRD-L can alleviate the sparsity problem of union-level interactions by adding more descriptive details to each item, which is carried by individual-level relations. Extensive numerical experiments demonstrate JRD-L outperforms state-of-the-art baselines for the sequential recommendation.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Nowadays, abundant user-item interactions in recommender system (RS) are recorded over time, which can be further used to discover the patterns of users' behaviors [3, 12] . Therefore, sequential recommendation is becoming a new trend in academic research and practical applications, because it is capable of leveraging temporal information among users' transactions for better inferring the user preference.", "cite_spans": [{"start": 164, "end": 167, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 168, "end": 171, "text": "12]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Introduction"}, {"text": "Dominant approaches aim to modeling long-term temporal information, capturing holistic dependencies of user-item sequence, while short-term temporal information which are essential in capturing partial dependencies are also significant. The long-term interaction is depicted in Fig. 1(a) where arrows indicate the dependency among a user-item interaction sequence. As a representative in long-term dependency modeling for general RS, factorization-based methods plays an important role in long-term dependency sequential recommendation for its remarkable efficiency [12] . Factorization-based methods model the entire user-item interaction matrix into two low-rank matrices. Such measure that aims to deal with the entire user-item interaction matrix is well-suited to train models that capture longer-term user preference profiles, however has limitations on capturing short-term user interests. Two main drawbacks exist in factorizationbased methods for sequential recommendation: 1) they failed to fully exploit the rich information of transition dependencies of multiple items; 2) modeling the entire user-item dependencies causes enormous computing cost of growing size of user-item interaction matrix when user has new interactions [8, 9] .", "cite_spans": [{"start": 566, "end": 570, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 1238, "end": 1241, "text": "[8,", "ref_id": "BIBREF7"}, {"start": 1242, "end": 1244, "text": "9]", "ref_id": "BIBREF8"}], "ref_spans": [{"start": 278, "end": 287, "text": "Fig. 1(a)", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "As for modeling users' short-term interests, mainstream methods such as Markov chain-based approaches [3] leverage transition dependency of items from the individual-level. The short-term interaction at individual-level is shown as Fig. 1(b) . Therefore, individual-level dependencies can capture individual influence between a pair of single item, but may neglect the collective influence [19] among three or more items denoted by union-level dependencies, as shown in Fig. 1(c) . Namely, the collective influence is caused by the dependency of a group of items on a single item. To alleviate this issue, Yu et al. [19] leverages both individual and collective influence for better sequential recommendation performance. However, two main drawbacks exist in this methods: 1) the information of individual and collective influence is simply added to the output proximity score of a factorization-based model, leveraging none of the long-term information; 2) The union-level interaction requires a group of items to be joint modeled within a limit length of sequence, which may lead to sparsity problem.", "cite_spans": [{"start": 102, "end": 105, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 390, "end": 394, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 616, "end": 620, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [{"start": 232, "end": 241, "text": "Fig. 1(b)", "ref_id": "FIGREF0"}, {"start": 470, "end": 479, "text": "Fig. 1(c)", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "In this paper, we propose a unified framework joint relational dependency learning(JRD-L), which exploits long-term temporal information and shortterm temporal information from individual-level and union-level for improving sequential recommendation. In particular, a Long Short-Term Memory (LSTM) model [5] is used to encode long-term preferences, while short-term dependencies existing in pair relations among items are computed based on the intermedia hidden states of LSTM on both individual-level and union-level. LSTM hidden states can carry the long-term dependencies information and transmit them to short-term item pairs. Meanwhile, the individual-level relation and union-level relation are modeled together to fully exploit the collective influence among union-level pair relation and to address the sparsity problem. The framework of JRD-L is described in Fig. 3 . Experiments on large-scale dataset demonstrate the effectiveness of the proposed JRD-L. The main contributions of our paper can be summarized as -JRD-L considers user's long-term preferences along with short-term pair-wise item relations from multiple perspectives of individual-level and union-level. Specifically, JRD-L involves a novel multi-pair relational LSTM model that can capture both long-term dependency and multi-level temporal correlations for better inferring user preferences. -A novel attention model is also combined with JRD-L that can augment individual-level and union-level pair relation by learning the contributions to the subsequent interactions between users and items. Meanwhile, the weighted outputs of attention model are fused together, contributing more individuallevel information to alleviates the sparse problem in the union-level dependency. ", "cite_spans": [{"start": 304, "end": 307, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [{"start": 868, "end": 874, "text": "Fig. 3", "ref_id": "FIGREF1"}], "section": "Introduction"}, {"text": "Many methods consider long-term temporal information to mining the sequential patterns of the users' behaviors, including factorziation-based approaches [12, 14] and Markov chains based approaches [2] . Recently, Deep learning (DL)-based models have achieved significant effectiveness in long-term temporal information modeling, including multi-layer perceptron-based (MLP-based) models [16, 17] , Convolutional neural network-based (CNN-based) models [6, 15] and Recurrent neural network-based (RNN-based) models [1] . RNN-based models stand out among these models for its capacity of modeling sequential dependencies by transmiting long-term sequential information from the first hidden state to the last one. However, RNN can be difficult to trained due to the vanishing gradient problem [7] , but advances such as Long Short-Term Memory (LSTM) [5] has enabled RNN to be successful. LSTM is considered one of the most successful variant of RNN, with the capability of capturing long-term relationships in a sequence and suffering from the vanishing gradient problem. So far, LSTM models have achieved tremendous success in sequence modelling tasks [20, 21] .", "cite_spans": [{"start": 153, "end": 157, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 158, "end": 161, "text": "14]", "ref_id": "BIBREF13"}, {"start": 197, "end": 200, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 387, "end": 391, "text": "[16,", "ref_id": "BIBREF15"}, {"start": 392, "end": 395, "text": "17]", "ref_id": "BIBREF16"}, {"start": 452, "end": 455, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 456, "end": 459, "text": "15]", "ref_id": "BIBREF14"}, {"start": 514, "end": 517, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 791, "end": 794, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 848, "end": 851, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 1151, "end": 1155, "text": "[20,", "ref_id": "BIBREF19"}, {"start": 1156, "end": 1159, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Related Works"}, {"text": "With respect to short-term temporal information modeling, existing works on modeling short-term temporal information mainly model pair relations between items. The representative work is Markov Chain (MC)-based models [3] . The objective of such model is to measure the average or weighted relevance values between a given item and its next-interaction item, this only captures dependencies between two single items. Tang et al. [15] propose a method capturing collective dependencies among three or more items. However, the model in [15] suffers from data sparsity problems. Therefore, in order to solve the sparsity problem when merely modeling collective dependencies, Yu et al. [19] add individual (i.e. individual-level) dependencies into collective (i.e. union-level) dependencies, but their work is still insufficient for it does not leverage long-term temporal information.", "cite_spans": [{"start": 218, "end": 221, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 429, "end": 433, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 534, "end": 538, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 682, "end": 686, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Related Works"}, {"text": "Before introducing the proposed method, we provide some useful notations as follows. Let U and I be the user and item set, as shown in Fig. 2 ", "cite_spans": [], "ref_spans": [{"start": 135, "end": 141, "text": "Fig. 2", "ref_id": null}], "section": "Joint Relational Dependency Learning"}, {"text": ". The goal of JRD-L method is to predict the likelihood of the user preferred item e ui c , based on the user's behavior sequences S ui j .", "cite_spans": [], "ref_spans": [], "section": ". A sequence of interactions between U and I can be represented as"}, {"text": "denotes a sequence of interactions between a user ui and a given item set I. (b) Next-item recommendation aims to generate a ranking list exposed to users by modeling user-item interaction sequence.", "cite_spans": [], "ref_spans": [], "section": ". A sequence of interactions between U and I can be represented as"}, {"text": "The overall architecture of JRD-L is shown in Fig. 3 . Generally, JRD-L first models long-term dependency over the whole user-item interaction data S = {S ui j : u i \u2208 U } in a LSTM layer. JRD-L takes the most recent n items before time point t of the whole sequence as the short-term interaction sequence. Then, JRD-L computes individual-level and union-level pair relations on the taken short sequence as shortterm dependencies modeling. Specifically, with the input of u i and S ui j , JRD-L composes u i and S ui j into a single user-item vector via an Embedding layer, and output e ui t as the user-item interaction embedding. A LSTM layer is then used to map ", "cite_spans": [], "ref_spans": [{"start": 46, "end": 52, "text": "Fig. 3", "ref_id": "FIGREF1"}], "section": ". A sequence of interactions between U and I can be represented as"}, {"text": "individually. JRD-L then passes the corresponding hidden status pairs of the most recent items to an attention layer, output the correlation likelihood S individual and S union , from which the short-term individual-level and union-level pair relation is modeled, respectively. At Last, S individual is concatenate with S union to obtain the correlation of e ui c with the existing items for the next-item prediction task.", "cite_spans": [], "ref_spans": [], "section": ". A sequence of interactions between U and I can be represented as"}, {"text": "By learning the item similarities from a large number of sequential behaviors over items, we apply skip-gram with negative sampling (SGNS) [10] to generate a unified representation for each item in an given user-item interaction sequence", "cite_spans": [{"start": 139, "end": 143, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Skip-Gram Based Item Representation"}, {"text": ". Before exploiting users' sequences dependencies, our prior problem is to represent items via embedding layer in a numerical way for subsequent calculations. In the embedding layer, the skip-gram with negative sampling is applied to directly learn high-quality item vectors from users' interaction sequences. The SGNS [10] generate item representations by exploiting the sequence of interactions between users and items. Specifically, given an item interaction sequence S ui j = (S ui 1 , S ui 2 , ..., S ui |S u i j | ) of user u i from the user-item interaction sequence S, SGNS aims to solve the following objective function arg max vj ,wi", "cite_spans": [{"start": 319, "end": 323, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Skip-Gram Based Item Representation"}, {"text": "are the latent vectors that correspond to the target and context representation for items in S ui j , respectively. The parameter m is the dimension parameter that is defined empirically according to dataset size. E is the number of negative samples per a positive sample. Finally, matrices U and V are computed to generate representation of interaction sequences.", "cite_spans": [], "ref_spans": [], "section": "Skip-Gram Based Item Representation"}, {"text": "To model the long-term temporal information in users' behaviors, we apply a standard LSTM [5] as in Fig. 3 to model the temporal information over the whole user-item interaction sequence. For each user u i , we first generate an interaction sequence S ui with embedding items x j \u2208 I based on U and V calculated by Eq. (1) from embedding layer in Fig. 3 ", "cite_spans": [{"start": 90, "end": 93, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [{"start": 100, "end": 106, "text": "Fig. 3", "ref_id": "FIGREF1"}, {"start": 347, "end": 353, "text": "Fig. 3", "ref_id": "FIGREF1"}], "section": "User Preference Modeling for Long-Term Pattern"}, {"text": "Through LSTM long-term information modeling in Fig. 3 , h ui c1 , h ui c2 , ..., h ui |c l | is output by Eq. (3) and l is the total number of candidate next items. The sequence", "cite_spans": [], "ref_spans": [{"start": 47, "end": 53, "text": "Fig. 3", "ref_id": "FIGREF1"}], "section": "User Preference Modeling for Long-Term Pattern"}, {"text": "(2) for the following multi-relational dependency modeling stage.", "cite_spans": [], "ref_spans": [], "section": "User Preference Modeling for Long-Term Pattern"}, {"text": "Long-term dependency models long-range user preferences but neglect important pairwise relations between items, which is insufficient in capturing pairwise relation from multiple level. Therefore our proposed method should unify both short-term sequential dependency (at both individual-level and union-level) and long-term sequential dependency. Inspired by [18] ", "cite_spans": [{"start": 359, "end": 363, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Multi-relational Dependency Modeling for Short-Term Pattern"}, {"text": "The task is then to learn the correlation between the items in interaction sequence and candidate items. Rather than directly applying the work [18] for modeling the short-term dependency, we introduce an attention mechanism to calculate pair relations from individual-level and union-level to fully modeling the user preferences to different items. This is mainly because the work [18] implies that all vectors share the same weight, discarding an important fact that human naturally have different opinions on items. By introducing attention mechanism, our work can distribute high weights on these items user like more, thus improving recommendation performance. (1) . After obtaining the weight \u03b1 i of each existing item h u t\u2212j , the likelihood S ci , which describe how likely the exiting items in user-item interaction sequence will interact with e ui ci in candidate next-interact items set, can be calculated by", "cite_spans": [{"start": 144, "end": 148, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 382, "end": 386, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 666, "end": 669, "text": "(1)", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Multi-relational Dependency Modeling for Short-Term Pattern"}, {"text": "where s k is the correlation score of the pair of item h u t\u2212j \u2208 h (1) with h u ci \u2208 h (2) , S ci \u2208 S individual is the output of attention network for individual-level relation measuring layer. \u03b2 1 , \u03b2 2 and b are LSTM parameters.", "cite_spans": [{"start": 67, "end": 70, "text": "(1)", "ref_id": "BIBREF0"}, {"start": 87, "end": 90, "text": "(2)", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Individual-Level Pairwise"}, {"text": "In order to model short-term union-level pair relation, we predefine a sliding window to determine the length of collective items set in existing user-item sequences. Based on the defined items set, collaborate influence in union-level pair relation can be learned in attention network for union-level relation measuring layer. Union-level pairwise relations learned by our method can capture collective dependencies among three or more items, which complements to the individual-level relation for improving recommendation performance. In the union-level pairwise relation modeling stage, the candidate length of collective items set is defined from \u03b8 = {2, 4, 6, 8}. To learn the collaborate influence in union-level pair relation, we define a sequence Eq. (3) . Then union-level pairs pass through the attention network for union-level relation measuring layer to obtain the weight \u03b1 i of each existing item h u t\u2212j , and output the correlation likelihood S union by", "cite_spans": [], "ref_spans": [{"start": 755, "end": 762, "text": "Eq. (3)", "ref_id": "FIGREF1"}], "section": "Union-Level Pairwise Relations."}, {"text": "h u ci \u2208 h (2) , \u03b2 3 , \u03b2 4 and b are model parameters. Then, S union is output by attention network for union-level pair relation measuring layer. Finally, S union is concatenated with S individual from attention network for individual-level pair relation measuring layer to calculate the correlation of e ui c with the existing items for the next-item prediction task.", "cite_spans": [{"start": 11, "end": 14, "text": "(2)", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Union-Level Pairwise Relations."}, {"text": "To effectively learn the parameters of the proposed JDR-L model, our training objective is to minimize the loss between the predicted labels and the true labels of candidate items. The optimization setup is, firstly, we define the item that has the latest timestamp among the user-item interaction sequence as the standard subsequent item, and define the rest of items as the non-subsequent items. Secondly, the loss function is therefore based on the assumption that an item (positive samples, i.e. standard subsequent item) this user liked will have a relative larger value than other items (negative samples) that he/she has no interest in. The loss function is then formulated as arg min", "cite_spans": [], "ref_spans": [], "section": "Optimization"}, {"text": "where the parameter \u0398 = {W LST M , \u03c9, \u03b2 1 , \u03b2 2 , \u03b2 3 , \u03b2 4 , b}. S union in Eq. (5) represents the correlation likelihood output by attention network for union-level relation measuring layer. y i is the label of the candidate item and \u03bb is a parameter for l 2 regularization. Adaptive moment estimation (Adam) [11] is used to optimize parameters during the training process.", "cite_spans": [{"start": 311, "end": 315, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Optimization"}, {"text": "We conduct experiments to validate JDR-L for Top-N sequential recommendation task on the real-world dataset, i.e., Movie&TV dataset [19] , that belongs to Amazon data 1 . Since the original datasets are sparse, we firstly filter out users with fewer than 10 interactions as in [19] . The statistical information of the before-processing and after-processing of Movie&TV dataset is shown in Table 1 . Following the evaluation settings in [19] , we set train/test with ratios 80/20. We compare JRD-L with three baselines: BPR-MF [12] is a widely used matrix factorization method for sequential RS; TranRec [4] models users as translation vectors operating on item sequences for sequential RS); RNN-based model (i.e., GRU4Rec [6] uses basic Gated Recurrent Unit for sequential RS); FPMC [13] is a typical Markov chain method modeling individiual item interactions; Multi-level item temporal dependency model (MARank) [19] models both individual-level and union-level interactions with factorization model.", "cite_spans": [{"start": 132, "end": 136, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 277, "end": 281, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 437, "end": 441, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 527, "end": 531, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 604, "end": 607, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 723, "end": 726, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 784, "end": 788, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 914, "end": 918, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [{"start": 390, "end": 397, "text": "Table 1", "ref_id": "TABREF5"}], "section": "Evaluation Setup"}, {"text": "For fair comparisons, we set the dropout percentage as 0.5 [19] . The embedding size d of Embedding layer is chosen from {32, 64, 128, 256}, which should be equal to the hidden size h of LSTM. The regularization hyper-parameter \u03bb is selected from {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}. We set the learning rate of Aadm as the default number 0.001 [11] . As n is the most recent items for short-term dependency, we choose n from {10, 20, 40, 60}. The length l of the sliding window of union-level interaction is chosen from {2, 4, 6, 8}. We define the length N of ranked list as 20. For the hardware settings, JRD-L model is trained on a Linux server with Tesla P100-PCIE GPU.", "cite_spans": [{"start": 59, "end": 63, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 352, "end": 356, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Evaluation Setup"}, {"text": "This section will discuss how the parameters influence the JRD-L model performance. We first explore the impact of n on the performance of JDR-L, the comparison is set on different n chosen from {10, 20, 40, 60}. Secondly, we evaluate the influence of the length l, l is chosen from {2, 4, 6, 8}. We use two metrics to evaluate the model performance, which are MRR (Mean Reciprocal Rank) -the average of reciprocal ranks of the predicted candidate items, and NDCG (Normalized Discounted Cumulative Gain) -a normalized average of reciprocal ranks of the predicted candidate items with a discounting factor, the comparison results of different setups are shown in Fig. 4 . Figure 4 show that when other hyperparameters are set equal, n = 10 achieves the best performance. These observations, presumably, because sequential pattern does not involve a very long sequence. Besides, l = 4 achieves the best performance, indicating that the collective influence of 4 items is informative for the Movie&TV dataset. ", "cite_spans": [], "ref_spans": [{"start": 662, "end": 668, "text": "Fig. 4", "ref_id": "FIGREF4"}, {"start": 671, "end": 679, "text": "Figure 4", "ref_id": "FIGREF4"}], "section": "Effect of Parameter Selection for JDR-L"}, {"text": "Ranking performance evaluates how the predicted Top-N lists act on the recommendation system. Table 2 shows the comparison results of JDR-L with baselines. Encouragingly, we can find that JDR-L performs best with the highest MRR and NDCG scores. Besides, baselines may not perform well as JDR-L. Firstly, BPR-MF as matrix factorization-based method obtains less competitive performance when compared with GRU4Rec. This is mainly because BPR-MF considers user intrinsic preference over item while GRU4Rec models union-level item interaction along with users' overall preferences. Secondly, TranRec and FPMC are two state-of-the-art methods exploiting individual-level item temporal dependency. Both of them outperform the other baselines, since they consider individual-level item temporal dependency. This indicates that keeping directed interaction between a pair of items is essential for sequential recommendation. Thirdly, MARank considering individual-level and union-level interactions but neglecting long-term dependencies performs worse than JDR-L. Above all, BPR-MF performs the worst, this is mainly because BPR-MF models only intrinsic preferences within short sequences of user-item interactions, neglecting long-term user preferences and item interactions at individual-level and union-level. ", "cite_spans": [], "ref_spans": [{"start": 94, "end": 101, "text": "Table 2", "ref_id": "TABREF6"}], "section": "Ranking Performance Comparison"}, {"text": "JDR-L contains three components as indicated by Fig. 3 , i.e. Long-term user-item interaction modelling, individual-level item interaction modeling and union-level item interaction modelling. To analyze the influence of different components to the overall recommendation performance, we set different combinations of components for evaluation, with the results been shown in Table 3 . JDR-L with three components performs best compared with other combinations as shown in Table 3 , verifying that our proposed JDR-L is optimal. As for other combinations, LSTM-only obtains the lower MRR and NDCG scores compared with JDR-L, this is because LSTM-only models long-term dependencies. LSTM+individuallevel item interaction outperforms LSTM+union-level item interaction, the main reason is that union-level item interaction suffers from a sparsity problem as the length of item set increases. Besides, both of LSTM+individual-level item interaction and LSTM+union-level item interaction obtain lower scores compared with JDR-L model. This further indicates that the information in individuallevel item interaction should be combined into union-level interaction modeling stage to solve the sparsity problem. ", "cite_spans": [], "ref_spans": [{"start": 48, "end": 54, "text": "Fig. 3", "ref_id": "FIGREF1"}, {"start": 375, "end": 382, "text": "Table 3", "ref_id": "TABREF7"}, {"start": 472, "end": 479, "text": "Table 3", "ref_id": "TABREF7"}], "section": "Components Influence of JDR-L"}, {"text": "In this paper, we design a Joint Relational Dependency learning (JRD-L) for sequential recommendation. JDR-L builds a novel model to unify both longterm dependencies and short-term dependencies from individual-level and unionlevel. Moreover, JDR-L can handle the sparsity problem when exploiting the individual-level relation information from the sequential behaviors. Extensive experiments on the benchmark dataset demonstrate the effectiveness of JRD-L.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Incorporating dwell time in session-based recommendations with recurrent neural networks", "authors": [{"first": "V", "middle": [], "last": "Bogina", "suffix": ""}, {"first": "T", "middle": [], "last": "Kuflik", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "57--59", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "The YouTube video recommendation system", "authors": [{"first": "J", "middle": [], "last": "Davidson", "suffix": ""}, {"first": "B", "middle": [], "last": "Liebald", "suffix": ""}, {"first": "J", "middle": [], "last": "Liu", "suffix": ""}, {"first": "P", "middle": [], "last": "Nandy", "suffix": ""}, {"first": "T", "middle": [], "last": "Van Vleet", "suffix": ""}], "year": 2010, "venue": "RecSys 2010 -Proceedings of the 4th ACM Conference on Recommender Systems", "volume": "", "issn": "", "pages": "293--296", "other_ids": {"DOI": ["10.1145/1864708.1864770"]}}, "BIBREF2": {"ref_id": "b2", "title": "Translation-based recommendation", "authors": [{"first": "R", "middle": [], "last": "He", "suffix": ""}, {"first": "W", "middle": ["C"], "last": "Kang", "suffix": ""}, {"first": "J", "middle": [], "last": "Mcauley", "suffix": ""}], "year": 2017, "venue": "RecSys 2017 -Proceedings of the 11th ACM Conference on Recommender Systems", "volume": "", "issn": "", "pages": "161--169", "other_ids": {"DOI": ["10.1145/3109859.3109882"]}}, "BIBREF3": {"ref_id": "b3", "title": "Translation-based recommendation", "authors": [{"first": "R", "middle": [], "last": "He", "suffix": ""}, {"first": "W", "middle": ["C"], "last": "Kang", "suffix": ""}, {"first": "J", "middle": [], "last": "Mcauley", "suffix": ""}], "year": 2017, "venue": "Eleventh ACM Conference on Recommender Systems", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Long short-term memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {"DOI": ["10.1162/neco.1997.9.8.1735"]}}, "BIBREF5": {"ref_id": "b5", "title": "Neural network based next-song recommendation. arXiv: Information Retrieval", "authors": [{"first": "K", "middle": [], "last": "Hsu", "suffix": ""}, {"first": "S", "middle": [], "last": "Chou", "suffix": ""}, {"first": "Y", "middle": [], "last": "Yang", "suffix": ""}, {"first": "T", "middle": [], "last": "Chi", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Multiplicative LSTM for sequence modelling", "authors": [{"first": "B", "middle": [], "last": "Krause", "suffix": ""}, {"first": "L", "middle": [], "last": "Lu", "suffix": ""}, {"first": "I", "middle": [], "last": "Murray", "suffix": ""}, {"first": "S", "middle": [], "last": "Renals", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1609.07959"]}}, "BIBREF7": {"ref_id": "b7", "title": "Lingo: linearized grassmannian optimization for nuclear norm minimization", "authors": [{"first": "Q", "middle": [], "last": "Li", "suffix": ""}, {"first": "W", "middle": [], "last": "Niu", "suffix": ""}, {"first": "G", "middle": [], "last": "Li", "suffix": ""}, {"first": "Y", "middle": [], "last": "Cao", "suffix": ""}, {"first": "J", "middle": [], "last": "Tan", "suffix": ""}, {"first": "L", "middle": [], "last": "Guo", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Riemannian submanifold tracking on low-rank algebraic variety", "authors": [{"first": "Q", "middle": [], "last": "Li", "suffix": ""}, {"first": "Z", "middle": [], "last": "Wang", "suffix": ""}], "year": 2017, "venue": "Thirty-First AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Distributed representations of words and phrases and their compositionality. arXiv: Computation and Language", "authors": [{"first": "T", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "K", "middle": [], "last": "Chen", "suffix": ""}, {"first": "G", "middle": ["S"], "last": "Corrado", "suffix": ""}, {"first": "J", "middle": [], "last": "Dean", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Adaptive estimation of regression models via moment restrictions", "authors": [{"first": "W", "middle": ["K"], "last": "Newey", "suffix": ""}], "year": 1988, "venue": "J. Econ", "volume": "38", "issn": "3", "pages": "301--339", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "BPR: Bayesian personalized ranking from implicit Feedback", "authors": [{"first": "S", "middle": [], "last": "Rendle", "suffix": ""}, {"first": "C", "middle": [], "last": "Freudenthaler", "suffix": ""}, {"first": "Z", "middle": [], "last": "Gantner", "suffix": ""}, {"first": "L", "middle": [], "last": "Schmidt-Thieme", "suffix": ""}], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Factorizing personalized Markov chains for next-basket recommendation", "authors": [{"first": "S", "middle": [], "last": "Rendle", "suffix": ""}, {"first": "C", "middle": [], "last": "Freudenthaler", "suffix": ""}, {"first": "L", "middle": [], "last": "Schmidtthieme", "suffix": ""}], "year": 2010, "venue": "The Web Conference", "volume": "", "issn": "", "pages": "811--820", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Fast context-aware recommendations with factorization machines", "authors": [{"first": "S", "middle": [], "last": "Rendle", "suffix": ""}, {"first": "Z", "middle": [], "last": "Gantner", "suffix": ""}, {"first": "C", "middle": [], "last": "Freudenthaler", "suffix": ""}, {"first": "L", "middle": [], "last": "Schmidt-Thieme", "suffix": ""}], "year": 2011, "venue": "SIGIR 2011 -Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval", "volume": "", "issn": "", "pages": "635--644", "other_ids": {"DOI": ["10.1145/2009916.2010002"]}}, "BIBREF14": {"ref_id": "b14", "title": "Personalized top-n sequential recommendation via convolutional sequence embedding", "authors": [{"first": "J", "middle": [], "last": "Tang", "suffix": ""}, {"first": "K", "middle": [], "last": "Wang", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "565--573", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Next basket recommendation with neural networks", "authors": [{"first": "S", "middle": [], "last": "Wan", "suffix": ""}, {"first": "Y", "middle": [], "last": "Lan", "suffix": ""}, {"first": "P", "middle": [], "last": "Wang", "suffix": ""}, {"first": "J", "middle": [], "last": "Guo", "suffix": ""}, {"first": "J", "middle": [], "last": "Xu", "suffix": ""}, {"first": "X", "middle": [], "last": "Cheng", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Learning hierarchical representation model for nextbasket recommendation", "authors": [{"first": "P", "middle": [], "last": "Wang", "suffix": ""}, {"first": "J", "middle": [], "last": "Guo", "suffix": ""}, {"first": "Y", "middle": [], "last": "Lan", "suffix": ""}, {"first": "J", "middle": [], "last": "Xu", "suffix": ""}, {"first": "S", "middle": [], "last": "Wan", "suffix": ""}, {"first": "X", "middle": [], "last": "Cheng", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "403--412", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Integrating order information and event relation for script event prediction", "authors": [{"first": "Z", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "C", "middle": ["Y"], "last": "Chang", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "57--67", "other_ids": {"DOI": ["10.18653/v1/d17-1006"]}}, "BIBREF18": {"ref_id": "b18", "title": "Multi-order attentive ranking model for sequential recommendation", "authors": [{"first": "L", "middle": [], "last": "Yu", "suffix": ""}, {"first": "C", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "S", "middle": [], "last": "Liang", "suffix": ""}, {"first": "X", "middle": [], "last": "Zhang", "suffix": ""}], "year": 2019, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issn": "", "pages": "5709--5716", "other_ids": {"DOI": ["10.1609/aaai.v33i01.33015709"]}}, "BIBREF19": {"ref_id": "b19", "title": "Where to go next: A Spatiotemporal LSTM model for next poi recommendation", "authors": [{"first": "P", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "H", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Z", "middle": [], "last": "Li", "suffix": ""}, {"first": "J", "middle": [], "last": "Xu", "suffix": ""}, {"first": "V", "middle": ["S"], "last": "Sheng", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Personalized learning full-path recommendation model based on LSTM neural networks", "authors": [{"first": "Y", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "C", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Q", "middle": [], "last": "Hu", "suffix": ""}, {"first": "J", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Tang", "suffix": ""}], "year": 2018, "venue": "Inf. Sci", "volume": "444", "issn": "", "pages": "135--152", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "(a) Long-term user-item interaction; (b) Individual-level item relevance; (3) Union-level item relevance. The dependencies of an item on its' subsequent item is represented as the transition arrows.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "The overall framework of Joint Relational Dependency Learning (JRD-L).", "latex": null, "type": "figure"}, "FIGREF2": {"text": "matrix consisting of output vectors of last LSTM layer, and n is thesize of h (1) = (h ui t\u22121 , h ui t\u22122 , ..., h ui t\u2212n ) in Eq. (2) and l is the size of h (2) = (h ui c1 , h ui c2 , ..., h ui |c l | ) in Eq. (3). The attentive weights \u03b1 = (\u03b1 1 , \u03b1 2 , ..., \u03b1 t\u2212n ) of the items in interaction sequence are defined by a weighted sum of these output vectors as \u03b1 = softmax(\u03c9 T M ) and M = tanh(H). We obtain M by a fully connection layer activated by tanh activation function. \u03c9 T is a transpose vector of attention network's parameters. \u03b1 i \u2208 [0, 1] is the weight of h ui t\u2212j and h ui t\u2212j \u2208 h", "latex": null, "type": "figure"}, "FIGREF3": {"text": "in Eq. (4) represents the correlation likelihood output by attention network for individual-level relation measuring layer. S (i)", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Results of JDR-L under different settings.", "latex": null, "type": "figure"}, "TABREF0": {"text": "j | to derive hidden vector h ui ci encoding e ui c to model long-term sequential information. Based on this, h ui ci is paired with the most recent n items before time point t", "latex": null, "type": "table"}, "TABREF1": {"text": "| as the d-dimensions latent vector of item x j . Given the embedding of the user-item interaction sequence e ui | as inputs to the LSTM. The inner hidden states in LSTM hidden layer are updated at each time step, which can carry the long-term dependencies information and transmit them to item pairs. At each time step, the next output of computing last hidden status h u i is computed by", "latex": null, "type": "table"}, "TABREF2": {"text": ") to obtain the long-term-dependency-sensitive hidden states h ui ci .", "latex": null, "type": "table"}, "TABREF4": {"text": "Relations. To capture the individual-level pairwise relations, the input of attention network for individual-level relation measuring layer is h ui | calculated by Eq. (2). An attention network is used for pairwise relation measuring. Let H \u2208 R n\u00d7l , H ij = h", "latex": null, "type": "table"}, "TABREF5": {"text": "Statistical information of dataset.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Movies&amp;TV </td><td>Users </td><td>Items </td><td>Interactions\n</td></tr><tr><td>Before-processing </td><td>40929 </td><td>51510 </td><td>1163413\n</td></tr><tr><td>After-processing </td><td>35168 </td><td>51227 </td><td>1070645\n</td></tr></table></body></html>"}, "TABREF6": {"text": "Ranking performance.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Methods </td><td>Movie&amp;TV\n</td></tr><tr><td>Measures@20 </td><td>MRR </td><td>NDCG\n</td></tr><tr><td>BPR-MF </td><td>0.0089 </td><td>0.0248\n</td></tr><tr><td>TranRec </td><td>0.0155 </td><td>0.0392\n</td></tr><tr><td>GRU4Rec </td><td>0.0124 </td><td>0.0344\n</td></tr><tr><td>FPMC </td><td>0.0162 </td><td>0.0406\n</td></tr><tr><td>MARank </td><td>0.0170 </td><td>0.0444\n</td></tr><tr><td>JDR-L </td><td>0.0179 </td><td>0.0518\n</td></tr><tr><td>Improvement </td><td>5.2% </td><td>16.7%\n</td></tr></table></body></html>"}, "TABREF7": {"text": "Ranking performance on different components in JDR-L.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Methods </td><td>Movie&amp;TV\n</td></tr><tr><td>Measures@20 </td><td>MRR </td><td>NDCG\n</td></tr><tr><td>LSTM-only </td><td>0.0154 </td><td>0.0447\n</td></tr><tr><td>LSTM+ individual-level item interaction </td><td>0.0147 </td><td>0.0442\n</td></tr><tr><td>LSTM+ union-level item interaction </td><td>0.0142 </td><td>0.0423\n</td></tr><tr><td>JDR-L </td><td>0.0178 </td><td>0.0518\n</td></tr></table></body></html>"}}, "back_matter": [{"text": "Acknowledge. This work is supported by the National Key R&D Program of China (Nos: 2017YFB0701501) and Australian Research Council Linkage Projects under LP170100891.", "cite_spans": [], "ref_spans": [], "section": "acknowledgement"}]}